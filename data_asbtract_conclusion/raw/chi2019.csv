title,link,abstract
A Translational Science Model for HCI,https://dl.acm.org/doi/10.1145/3290605.3300231,"
Using scientific discoveries to inform design practice is an important, but difficult, objective in HCI. In this paper, we provide an overview of Translational Science in HCI by triangulating literature related to the research-practice gap with interview data from many parties engaged (or not) in translating HCI knowledge. We propose a model for Translational Science in HCI based on the concept of a continuum to describe how knowledge progresses (or stalls) through multiple steps and translations until it can influence design practice. The model offers a conceptual framework that can be used by researchers and practitioners to visualize and describe the progression of HCI knowledge through a sequence of translations. Additionally, the model may facilitate a precise identification of translational barriers, which allows devising more effective strategies to increase the use of scientific findings in design practice. The presence of barriers that hamper the progression of knowledge into design practice is a significant issue within HCI. Therefore, it is necessary to understand how knowledge progresses, or fails to progress, from research to practice. To design the model for Translational Science in HCI, we drew on past work and interview data with researchers, practitioners, and multiple parties who are engaged – or not — in translating HCI knowledge. In our continuum, we describe multiple steps and gaps between basic and applied research, and design practice. We also identify multiple translators and the translational work they do. This model offers insights on how to bridge translational gaps and how to work with and train translators effectively. It also acts as a foundation for future research on Translational Science in HCI.
"
"""They Don't Leave Us Alone Anywhere We Go"": Gender and Digital Abuse in South Asia",https://dl.acm.org/doi/10.1145/3290605.3300232,"
South Asia faces one of the largest gender gaps online globally, and online safety is one of the main barriers to gender-equitable Internet access [GSMA, 2015]. To better understand the gendered risks and coping practices online in South Asia, we present a qualitative study of the online abuse experiences and coping practices of 199 people who identified as women and 6 NGO staff from India, Pakistan, and Bangladesh, using a feminist analysis. We found that a majority of our participants regularly contended with online abuse, experiencing three major abuse types: cyberstalking, impersonation, and personal content leakages. Consequences of abuse included emotional harm, reputation damage, and physical and sexual violence. Participants coped through informal channels rather than through technological protections or law enforcement. Altogether, our findings point to opportunities for designs, policies, and algorithms to improve women's safety online in South Asia. We presented a qualitative study of online safety among 199 cisgender and non-cisgender people who identified as women and 6 NGO staff members, across a diverse socioeconomic spectrum in India, Pakistan, and Bangladesh. We described the types of online abuse that South Asian women encountered and coped with, analyzing their unique sociocultural contexts and technology use-cases, using a feminist lens. We presented three major abuse types experienced by our participants, primarily on social media platforms: (i) cyberstalking, (ii) impersonation, and (iii) personal content leakages. Our results show that online abuse was commonly experienced by our participants (72% experienced at least one abuse type) and created severe consequences such as reputation harm, emotional harm, coercive relations, and physical harm. Our participants had developed informal coping mechanisms to resolve abuse, relying on family and NGOs, rather than seeking formal support from law enforcement or technology platforms. To prevent abuse, participants proactively limited technology use and used creative workarounds, leading to even lower participation online by women in a region with the highest gender gaps online [41]. Given these results, we discussed opportunities, open questions, and challenges for technologists and policy makers to consider in advancing a gender-equitable Internet .
"
Guidelines for Human-AI Interaction,https://dl.acm.org/doi/10.1145/3290605.3300233,"
Advances in artificial intelligence (AI) frame opportunities and challenges for user interface design. Principles for human-AI interaction have been discussed in the human-computer interaction community for over two decades, but more study and innovation are needed in light of advances in AI and the growing uses of AI technologies in human-facing applications. We propose 18 generally applicable design guidelines for human-AI interaction. These guidelines are validated through multiple rounds of evaluation including a user study with 49 design practitioners who tested the guidelines against 20 popular AI-infused products. The results verify the relevance of the guidelines over a spectrum of interaction scenarios and reveal gaps in our knowledge, highlighting opportunities for further research. Based on the evaluations, we believe the set of design guidelines can serve as a resource to practitioners working on the design of applications and features that harness AI technologies, and to researchers interested in the further development of human-AI interaction design principles. We proposed and evaluated 18 generally applicable design guidelines for human-AI interaction. We distilled the guidelines from over 150 AI-related design recommendations and validated them through three rounds of evaluation. We are hopeful that application of these guidelines will result in better, more human-centric AI-infused systems, and that our synthesis can facilitate further research. As the current technology landscape is shifting towards the increasing inclusion of AI in computing applications, we see signifcant value in working to further develop and refne design guidelines for human-AI interaction.
"
Human-Centered Tools for Coping with Imperfect Algorithms During Medical Decision-Making,https://dl.acm.org/doi/10.1145/3290605.3300234,"
Machine learning (ML) is increasingly being used in image retrieval systems for medical decision making. One application of ML is to retrieve visually similar medical images from past patients (e.g. tissue from biopsies) to reference when making a medical decision with a new patient. However, no algorithm can perfectly capture an expert's ideal notion of similarity for every case: an image that is algorithmically determined to be similar may not be medically relevant to a doctor's specific diagnostic needs. In this paper, we identified the needs of pathologists when searching for similar images retrieved using a deep learning algorithm, and developed tools that empower users to cope with the search algorithm on-the-fly, communicating what types of similarity are most important at different moments in time. In two evaluations with pathologists, we found that these tools increased the diagnostic utility of images found and increased user trust in the algorithm. The tools were preferred over a traditional interface, without a loss in diagnostic accuracy. We also observed that users adopted new strategies when using refinement tools, re-purposing them to test and understand the underlying algorithm and to disambiguate ML errors from their own errors. Taken together, these findings inform future human-ML collaborative systems for expert decision-making. In this paper, we found that refinement tools not only increased trust and utility, but were also used for critical decisionmaking purposes beyond guiding an algorithm. Our work brings to light the dual challenges and opportunities of ML: although black-box ML algorithms can be difficult to understand, off-the-shelf image embeddings from DNNs could enable new, lightweight ways of creating interactive refinement and exploration mechanisms. Ultimately, refinement tools gave doctors the agency to hypothesis-test and apply their domain knowledge, while simultaneously leveraging the benefits of automation. Taken together, this work provides implications for how ML-based systems can augment, rather than replace, expert intelligence during critical decision-making, an area that will likely continue to rise in importance in the coming years.
"
Seeing with New Eyes: Designing for In-the-Wild Museum Gifting,https://dl.acm.org/doi/10.1145/3290605.3300235,"
This paper presents the GIFT smartphone app, an artist-led Research through Design project benefitting from a three-day in-the-wild deployment. The app takes as its premise the generative potential of combining the contexts of gifting and museum visits. Visitors explore the museum, searching for objects that would most appeal to the gift-receiver they have in mind, then photographing those objects and adding audio messages for their receivers describing the motivation for their choices. This paper charts the designers' key aim of creating a new frame of mind using voice, and the most striking findings discovered during in-the-wild deployment in a museum -- 'seeing with new eyes' and fostering personal connections. We discuss empathy, motivation, and bottom-up personalisation in the productive space revealed by this combination of contexts. We suggest that this work reveals opportunities for designers of gifting services as well as those working in cultural heritage. Blast Theory’s arts-based app for gifting experiences within museums juxtaposed two very different sets of practices and expectations. Personal memories and connections were evoked in a public space. Participants created and/or received digital gifts made of photos of museum objects taken from anywhere within the physical collection, combined with spoken explanations of their choices, clues of where to find the gifts, messages, and a song. Blast Theory’s design decisions resulted in some confirmations of their hunches and some surprises. However, many participants shared the sentiments of P47: ‘I think it was definitely an interesting way to look at the museum through fresh eyes. And I have been to this museum a lot, over, yes, 27 years, so having a new way to look at it and interact with it was really interesting’. Coming from a Research through Design perspective and benefitting from qualitative feedback from 114 participants, this work can be the starting point for coming to understand arts-based design for gifting activities in mixed contexts: in this case, museum-situated digital gifting practices. We have drawn implications around empathy, motivation, and bottom-up personalisation for design researchers, makers of gifting services, and museum professionals, respectively. The GIFT app raises ideas for future work. Perhaps most pressing for museum professionals is concern over the need for front of house staff to support visitors in overcoming the nervousness that some felt in participating in an unknown type of experience in a public place. Blast Theory knew that this would be a challenge, and the deployment highlighted the extent and specifics of that challenge. We acknowledge these difficulties, but we also see potential advantages in building relationships with visitors seen as relational beings who bring their most treasured memories and relationships with them wherever they go. Museums can build on these powerful relationships and possibly discover ways unique to each institution of fostering personal engagement. We also see potential in experimenting with new means of encouraging receivers to come to the museum to receive their gifts, or of using gifting as a way to frame engagement with a museum’s digital collection or virtual presence, even for visitors who may never see the museum in person. If this app is scaled up as Blast Theory hopes it may be, the GIFT app and other arts-led interventions may ‘change the context in which they operate’ [26, p. 943] and help shape the continuing re-evaluation of the purpose of museums in societies where digital interactions are increasingly the norm and where museums are expected to do ever more outreach with ever fewer resources, year upon year.

"
Design and Plural Heritages: Composing Critical Futures,https://dl.acm.org/doi/10.1145/3290605.3300236,"
We make theoretical and methodological contributions to the CHI community by introducing comparisons between contemporary Critical Heritage research and some forms of experimental design practice. Beginning by identifying three key approaches in contemporary heritage research: Critical Heritage, Plural Heritages and Future Heritage we introduce these in turn, while exploring their significance for thinking about design, knowledge and diversity. We discuss our efforts to apply ideas integrating Critical Heritage and design through the adoption of known Research through Design techniques in a research project in Istanbul, Turkey describing the design of our study and how this was productive of sensory and speculative reflection on the past. Finally, we reflect on the usefulness of such methods in developing new interactive technologies in heritage contexts and go on to propose a series of recommendations for a future Critical Heritage Design practice. We have shown that there is significant shared theoretical background, clearly identifiable common terminology and points of shared theoretical interest, contexts for application and some promising related work between areas of design/HCI practice and contemporary research around Critical Heritage. Our observations from our short cultural probes study underline the applicability of some RtD techniques in exploring a design space for the development of new interactive technologies. Finally, we believe that a much broader range of existing techniques might be productively repurposed to beter contribute to a Critical Heritage design practice.
"
"Connect-to-Connected Worlds: Piloting a Mobile, Data-Driven Reflection Tool for an Open-Ended Simulation at a Museum",https://dl.acm.org/doi/10.1145/3290605.3300237,"
Immersive open-ended museum exhibits promote ludic engagement and can be a powerful draw for visitors, but these qualities may also make learning more challenging. We describe our efforts to help visitors engage more deeply with an interactive exhibit's content by giving them access to visualizations of data skimmed from their use of the exhibit. We report on the motivations and challenges in designing this reflective tool, which positions visitors as a ""human in the loop"" to understand and manage their engagement with the exhibit. We used an iterative design process and qualitative methods to explore how and if visitors could (1) access and (2) comprehend the data visualizations, (3) reflect on their prior engagement with the exhibit, (4)plan their future engagement with the exhibit, and (5) act on their plans. We further discuss the essential design challenges and the opportunities made possible for visitors through data-driven reflection tools. Recorded data of visitor actions could, possibly, serve as a “prosthetic memory” for activities in which visitors were deeply and ludically engaged, helping visitors meld their experiential memories with extra information provided by the logs to support a deeper understanding. In museum contexts, there is an increasing number of digital exhibits which are generating interesting usage data which could promote reflection, but giving visitors feedback in this way is in its infancy. This work shows that data-driven feedback can influence the “stories visitors tell themselves” about an exhibit experience, which is hugely important: the way visitors construct narratives about their museum experiences is known to be a powerful learning mechanism in museums [15, 38], as both conversation and narrative strategies are fundamentally sense-making behaviors [14]. Data-driven feedback helped participants in the Feedback session develop a ""To Each According to Their Needs"" story about their use of Connected Worlds, a story that made direct use of the variables we provided them, and a story that required the consideration of more information than the simpler ""Share and Share Alike"" story participants generated in the Non-Feedback session. Expanding beyond our context, the story-framing consequences of data selection and presentation are a big design consideration for formative feedback in open-ended learning settings, and thus could benefit from a more formal consideration of narrative and narrative structures and how they could impact data-driven feedback design. For example, different data subsets or visualizations may or may not support different “plot” structures (mysteries, tales of redemption, etc.). HCI researchers have begun codifying narrative design patterns for presenting data in graphical stories [2], and how interactive technology can advance narratives in the form of interactive theater [4, 24, 43]; such work may have useful suggestions for engineering narratives with data-driven feedback. A key feature of narratives is that they are inherently sequential - although a story may be told nonlinearly, or begin in media res, the order in which the components of the narrative are revealed is a major part of how it is received by the audience. This concept resonates with the design tension we uncovered concerning when to deliver data-driven feedback to visitors. In prior work, feedback delivered too early could derail visitor engagement [21], the equivalent of reading the last page of a mystery story first. In another exhibit, despite being given “just-in-time” automated alerts that suggested that the moment was right to deliver content, mediators often found the visitors unreceptive [45], demonstrating that a “teachable moment” may not be the same as a “learnable moment” [23]. If just-in-time feedback can be accessed on-demand, when both needed and desired by learners, it might ultimately be better for supporting learning than retrospective reflection - Schön points out that in contrast to “reflection-on-action,” “reflection-in-action” yields widerranging learning benefits, which more directly improve people’s ability to perform situated tasks [40]. Baumer [5] likens “retrospective reflection” to “slow technology,” adopting the analogy from the distinction between gourmet and fast food. We argue that while some forms of just-in-time feedback may be as “unhealthy” for learning as a carton of fries, the use of a narrative framing to select which data to present, and when, may be the equivalent of “chewing slowly.”
"
Anchored Audio Sampling: A Seamless Method for Exploring Children's Thoughts During Deployment Studies,https://dl.acm.org/doi/10.1145/3290605.3300238,"
Many traditional HCI methods, such as surveys and interviews, are of limited value when working with preschoolers. In this paper, we present anchored audio sampling (AAS), a remote data collection technique for extracting qualitative audio samples during field deployments with young children. AAS offers a developmentally sensitive way of understanding how children make sense of technology and situates their use in the larger context of daily life. AAS is defined by an anchor event, around which audio is collected. A sliding window surrounding this anchor captures both antecedent and ensuing recording, providing the researcher insight into the activities that led up to the event of interest as well as those that followed. We present themes from three deployments that leverage this technique. Based on our experiences using AAS, we have also developed a reusable open-source library for embedding AAS into any Android application. Very young children are avid technology users who deserve to be frst-class citizens in the design of systems and experiences that afect their lives. AAS is a data collection technique for feld deployments that captures audio samples in response to anchor events of interest. This allows researchers to passively capture rich, qualitative feedback about moments of interest and the events that precede them, and to do so at scale with data that comes directly from children. By formalizing the principles of the approach and supporting these with an open-source library, we seek to empower the research community to confront the ethical tensions inherent in this work and to resolve them within the context of their own feld deployments.
"
"To Asymmetry and Beyond!: Improving Social Connectedness by Increasing Designed Interdependence in Cooperative
                                 Play",https://dl.acm.org/doi/10.1145/3290605.3300239,"
Social play can have numerous health benefits but research has shown that not all multiplayer games are effective at promoting social engagement. Asymmetric cooperative games have shown promise in this regard but the design and dynamics of this unique style of play is not yet well understood. To address this, we present the results of two player experience studies using our custom prototype game Beam Me 'Round, Scotty! 2: the first comparing symmetric cooperative play (e.g., where players have the same interface, goals, mechanics, etc.) to asymmetric cooperative play (e.g., where players have differing roles, abilities, interfaces, etc.) and the second comparing the effect of increasing degrees of interdependence between play partners. Our results not only indicate that asymmetric cooperative games may enhance players' perceptions of connectedness, social engagement, immersion, and comfort with a game's controls, but also demonstrate how to further improve these outcomes via deliberate mechanical design changes, such as changes in cooperative action timing and direction of dependence. In this paper, we present the findings of a pair of player experience studies that compared asymmetric play to symmetric play, and within asymmetric play, the degrees of designed interdependence.We found that social presence and perceptions of connectedness were higher in asymmetric play than symmetric play, and higher in tightly-coupled asymmetric play than loosely-coupled asymmetric play. These same trends were also found for immersion, behavioural engagement, and even understanding and comfort with the game’s controls. We also reflect on several themes that emerged, including the need for designers to consider both emergent cooperation vs. designed interdependence as well as whether, how, and when to provide flexibility in the degree of interdependence.

"
"DesignABILITY: Framework for the Design of Accessible Interactive Tools to Support Teaching to Children
                                 with Disabilities",https://dl.acm.org/doi/10.1145/3290605.3300240,"
Developing educational tools aimed at children with disabilities is a challenging process for designers and developers because existing methodologies or frameworks do not provide any pedagogical information and/or do not take into account the particular needs of users with some type of impairment. In this study, we propose a framework for the design of tools to support teaching to children with disabilities. The framework provides the necessary stages for the development of tools (hardware-based or software-based) and must be adapted for a specific disability and educational goal. For this study, the framework was adapted to support literacy teaching and contributes to the design of educational/interactive technology for deaf people while making them part of the design process and taking into account their particular needs. The experts' evaluation of the framework shows that it is well structured and may be adapted for other types of disabilities. The proposed framework makes it easy to break down the activities of every stage and adapt it to other types of disabilities and learning goals/strategies. The adaptation made in this study to support literacy teaching to deaf children shows that the DesignABILITY framework is not a general-purpose framework, instead, it is a modular approach that can be transformed according to the final users’ needs. A first evaluation of the framework is shown in this paper, 26 researchers from different fields (HCI, design, software development) took the survey and the results demonstrate how promising this proposal is for addressing accessibility in the development of educational tools. All the recommendations given by the researchers will be taken into account for the improvement of the framework. For the adaptation of the framework (Deaf+literacy), the “Design for Engaged Learning” stage proposes a storytelling and collaborative learning approach which could support literacy teaching to Deaf children by engaging children into learning through stories, teamwork and technology. The words storytelling and collaboration, when used in the same context, promise to provide social, creative and fun aspects of learning [51]. During the review of the literature on storytelling [13] and collaborative learning (CL) [8] to support literacy teaching, we found that there is a lack of research regarding the use of these two strategies for the education of Deaf children. Fortunately, the results of the studies that made use of any of these strategies on developing reading and writing skills, show the effectiveness of using technology with one or both approaches [10][11][12] [14] [41][52]. Two prototypes are under development following the stages of the DesignABILITY framework adapted for literacy teaching to deaf children. These prototypes will be tested by deaf children in institutions from Colombia and Scotland. The process is being supported by teachers of deaf pupils in both countries with high expectations from educators. The new framework proposed in this study, and its adaptation for literacy, contributes to the design of educational/interactive technology for deaf people while making them part of the design process and taking into account their particular needs. This enables a better application of technology to education and consequently a better learning experience. The adaptation of the framework also gives specific details on how to structure collaborative learning and storytelling activities with/for deaf children during the design of an educational tool, which is not found in current HCI literature. Finally, our proposal suggests principles for experts’ reviews to evaluate CL and also the tool aimed at deaf children (some of these principles are the result of our previous work with deaf children). Specific evaluation methods that can be used with deaf children are suggested to evaluate the UX of the designed tool. For future work, the DesignABILITY framework will be adapted to support teaching to children with other disabilities like blindness, autism or with cognitive impairments.
"
"Transcalibur: A Weight Shifting Virtual Reality Controller for 2D Shape Rendering based on Computational
                                 Perception Model",https://dl.acm.org/doi/10.1145/3290605.3300241,"
Humans can estimate the shape of a wielded object through the illusory feeling of the mass properties of the object obtained using their hands. Even though the shape of hand-held objects influences immersion and realism in virtual reality (VR), it is difficult to design VR controllers for rendering desired shapes according to the perceptions derived from the illusory effects of mass properties and shape perception. We propose Transcalibur, which is a hand-held VR controller that can render a 2D shape by changing its mass properties on a 2D planar area. We built a computational perception model using a data-driven approach from the collected data pairs of mass properties and perceived shapes. This enables Transcalibur to easily and effectively provide convincing shape perception based on complex illusory effects. Our user study showed that the system succeeded in providing the perception of various desired shapes in a virtual environment. In this paper, we introduced Transcalibur: the weight moving VR controller for 2D haptic shape illusion. We implemented a hardware prototype, which can change its mass property in 2D planar space, and applied data-driven methods to obtain maps between mass property and perceived shape. Based on the demonstration and experiment, we succeeded in rendering various shape perceptions through the controller based on pre-computed perception model. As a future work, we further investigate details on time factor of shape changing in VR, and we aim to develop a simpler design and yet maximizes range of rendering shape.

"
LightBee: A Self-Levitating Light Field Display for Hologrammatic Telepresence,https://dl.acm.org/doi/10.1145/3290605.3300242,"
LightBee is a novel ""hologrammatic"" telepresence system featuring a self-levitating light field display. It consists of a drone that flies a projection of a remote user's head through 3D space. The movements of the drone are controlled by the remote user's head movements, offering unique support for non-verbal cues, especially physical proxemics. The light field display is created by a retro-reflective sheet that is mounted on the cylindrical quadcopter. 45 smart projectors, one per 1.3 degrees, are mounted in a ring, each projecting a video stream rendered from a unique perspective onto the retroreflector. This creates a light field that naturally provides motion parallax and stereoscopy without requiring any headset nor stereo glasses. LightBee allows multiple local users to experience their own unique and correct perspective of the remote user's head. The system is currently one-directional: 2 small cameras mounted on the drone allow the remote user to observe the local scene. We presented LightBee, a hologrammatic self-levitating telepresence drone that conveys a 3D image of the remote participant’s head via a cylindrical light field display. Te light field display consists of a retroreflective screen mounted on a cylindrical quadcopter. Tis screen is projected on by a circular array of 45 projectors mounted above the heads of local interlocutors. Each projector has its own renderer that, in parallel, calculates a viewport from a 3D relief map captured at the remote location. Tis capture system comprises an array of visible light stereo cameras connected to a PC. Te remote user’s head movement is tracked and transmited to control the location of the drone. Our light field display provides continuous motion parallax and stereoscopy to multiple local interlocutors without any need for head-worn apparatus. It allows correct horizontal gaze awareness between local and remote participants. Moreover, the movement of the display provides the remote user with the ability to explore the local environment, even when not projected upon. We discussed results from initial evaluations with users indicating that the telepresence system could support interactions between remote users, and concluded with initial insights into areas of improvement.
"
TabletInVR: Exploring the Design Space for Using a Multi-Touch Tablet in Virtual Reality,https://dl.acm.org/doi/10.1145/3290605.3300243,"
Complex virtual reality (VR) tasks, like 3D solid modelling, are challenging with standard input controllers. We propose exploiting the affordances and input capabilities when using a 3D-tracked multi-touch tablet in an immersive VR environment. Observations gained during semi-structured interviews with general users, and those experienced with 3D software, are used to define a set of design dimensions and guidelines. These are used to develop a vocabulary of interaction techniques to demonstrate how a tablet's precise touch input capability, physical shape, metaphorical associations, and natural compatibility with barehand mid-air input can be used in VR. For example, transforming objects with touch input, ""cutting"" objects by using the tablet as a physical ""knife"", navigating in 3D by using the tablet as a viewport, and triggering commands by interleaving bare-hand input around the tablet. Key aspects of the vocabulary are evaluated with users, with results validating the approach. We are the first to investigate the design of an example interaction vocabulary for using a multi-touch tablet in VR for 3D solid modeling. We approach the design methodically and propose design dimensions that inform the design of our vocabulary, but can also inform the design of alternate vocabularies. We validate this interaction vocabulary with a proof of concept system that addresses the core components of 3D modeling and a user study that shows that the interface is useful in replicating and creating original designs. Our study also identified some limitations which we discuss with possible solutions, but it also hints at future possibilities in this largely unexplored design space. While our focus was on 3D solid modeling, the design dimensions can also inform vocabularies for other applications like gaming, data visualization, and simulation control. Our work can guide future researchers and designers by extending the VR interaction space beyond traditional input devices.
"
RotoSwype: Word-Gesture Typing using a Ring,https://dl.acm.org/doi/10.1145/3290605.3300244,"
We propose RotoSwype, a technique for word-gesture typing using the orientation of a ring worn on the index finger. RotoSwype enables one-handed text-input without encumbering the hand with a device, a desirable quality in many scenarios, including virtual or augmented reality. The method is evaluated using two arm positions: with the hand raised up with the palm parallel to the ground; and with the hand resting at the side with the palm facing the body. A five-day study finds both hand positions achieved speeds of at least 14 words-per-minute (WPM) with uncorrected error rates near 1%, outperforming previous comparable techniques. In this paper we introduced RotoSwype, a ring-tilt based textentry technique that enables unencumbered, one-handed, self-contained, and eyes-away typing that enables wordgesture typing on any qwerty keyboard on any device. We design and build the technique for the Hand-Up and HandDown postures. For Hand-Up, we further compare two baseposes of the hand, PTS and PTG, and choose PTS based on a preliminary study. In a 5-day study, we evaluate RotoSwype for text-entry for AR/VR HMDs. The results show that with <60 mins of typing, participants achieve speeds of >14 WPM with a near 1% uncorrected error rate for both HU and HD postures, outperforming existing unencumbered techniques that use ring-input or swiping on glasses. We further discuss subjective feedback, design outcomes, improvements, and wider applicability for future work.

"
BeamBand: Hand Gesture Sensing with Ultrasonic Beamforming,https://dl.acm.org/doi/10.1145/3290605.3300245,"
BeamBand is a wrist-worn system that uses ultrasonic beamforming for hand gesture sensing. Using an array of small transducers, arranged on the wrist, we can ensem-ble acoustic wavefronts to project acoustic energy at spec-ified angles and focal lengths. This allows us to interro-gate the surface geometry of the hand with inaudible sound in a raster-scan-like manner, from multiple view-points. We use the resulting, characteristic reflections to recognize hand pose at 8 FPS. In our user study, we found that BeamBand supports a six-class hand gesture set at 94.6% accuracy. Even across sessions, when the sensor is removed and reworn later, accuracy remains high: 89.4%. We describe our software and hardware, and future ave-nues for integration into devices such as smartwatches and VR controllers. We have presented BeamBand, a novel worn sensing method that uses ultrasonic beamforming for on-body hand gesture recognition. BeamBand projects ultrasonic wavefronts at different angles and focal points on the user’s hand, and measures waves reflected back to the band. We evaluated two gesture sets sourced from the literature and our user study reveals promising accuracies, both withinsession and across-session. We hope our effort will act as a catalyst for deeper investigation into ultrasonic beamforming for enabling novel interactions.

"
Airport Accessibility and Navigation Assistance for People with Visual Impairments,https://dl.acm.org/doi/10.1145/3290605.3300246,"
People with visual impairments often have to rely on the assistance of sighted guides in airports, which prevents them from having an independent travel experience. In order to learn about their perspectives on current airport accessibility, we conducted two focus groups that discussed their needs and experiences in-depth, as well as the potential role of assistive technologies. We found that independent navigation is a main challenge and severely impacts their overall experience. As a result, we equipped an airport with a Bluetooth Low Energy (BLE) beacon-based navigation system and performed a real-world study where users navigated routes relevant for their travel experience. We found that despite the challenging environment participants were able to complete their itinerary independently, presenting none to few navigation errors and reasonable timings. This study presents the first systematic evaluation posing BLE technology as a strong approach to increase the independence of visually impaired people in airports. We presented the fndings of two focus groups that illustrate the perceptions that people with visual impairments have regarding the accessibility of airports, which have been neglected in the literature [38]. These fndings suggest that the main constraints experienced by visually impaired people in airports occur after being escorted to their gate. At this point, their lack of knowledge of the environment and their fear of getting lost leads to a single option: sitting and waiting. In order to empower them with more independence, we installed a BLE beacon-based navigation system (NavCog) at the Pittsburgh International Airport and analyzed its efect. Results of a study with ten visually impaired people showed that the system was able to cope with many navigation challenges of airports, such as users frequent veering in wide open areas. The low number of navigation errors and reasonable route completion times pose indoor navigation assistance as a promising tool to support independent mobility and to enhance the experience of visually impaired people in airports. However, a few navigation errors (or researcher’s occasional need to intervene) also show that greater localization accuracy may be required when traversing areasthat may present greater risks to the user’s (or environment) safety. To the best of our knowledge, this is the frst formal evaluation of a working navigation system for visually impaired people at an airport. We note that most airports are already equipped or plan to equip their sites with BLE beacons. In addition, other localization methods (e.g., based on Wi-Fi) may become viable alternatives in the near future, easing the deployment and maintenance of apps like NavCog. Despite the availability of the required infrastructure, what is currently lacking is both the awareness and the support of the navigation needs of visually impaired travelers. We believe this study can be a valuable benchmark for new installations and for formal evaluation of navigation systems that are already in place [7, 33, 37].

"
"Effects of Moderation and Opinion Heterogeneity on Attitude towards the Online Deliberation
                                 Experience",https://dl.acm.org/doi/10.1145/3290605.3300247,"
Online deliberation offers a way for citizens to collectively discuss an issue and provide input for policymakers. The overall experience of online deliberation can be affected by multiple factors. We decided to investigate the effects of moderation and opinion heterogeneity on the perceived deliberation experience, by running the first online deliberation experiment in Singapore. Our study took place in three months with three phases. In phase 1, our 2,006 participants answered a survey, that we used to create groups of different opinion heterogeneity. During the second phase, 510 participants discussed about the population issue on the online platform we developed. We gathered data on their online deliberation experience during phase 3. We found out that higher levels of moderation negatively impact the experience of deliberation on perceived procedural fairness, validity claim and policy legitimacy; and that high opinion heterogeneity is important in order to get a fair assessment of the deliberation experience. We introduced our own open-source platform, which we built based on design recommendations from previous works. During the deliberation phase, we evaluated how moderation and opinion heterogeneity may impact users’ overall experience of deliberation. We found that higher levels of moderation may degrade the overall experience. We also found that opinion heterogeneity seems to be strongly linked to the CHI 2019 Paper CHI 2019, May 4–9, 2019, Glasgow, Scotland, UK Paper 17 Page 10 perceptions of the participants, and therefore suggest to use high heterogeneity with mixed opinion in order to avoid any perception bias, positive or negative, based on one’s existing opinions on the policies. In terms of the Singapore context, our results suggest that despite of the unique political setting, participants were overall satisfied with their experience, and would be likely to support future deliberation practices like such.
"
MilliSonic: Pushing the Limits of Acoustic Motion Tracking,https://dl.acm.org/doi/10.1145/3290605.3300248,"
Recent years have seen interest in device tracking and localization using acoustic signals. State-of-the-art acoustic motion tracking systems however do not achieve millimeter accuracy and require large separation between microphones and speakers, and as a result, do not meet the requirements for many VR/AR applications. Further, tracking multiple concurrent acoustic transmissions from VR devices today requires sacrificing accuracy or frame rate. We present MilliSonic, a novel system that pushes the limits of acoustic based motion tracking. Our core contribution is a novel localization algorithm that can provably achieve sub-millimeter 1D tracking accuracy in the presence of multipath, while using only a single beacon with a small 4-microphone array.Further, MilliSonic enables concurrent tracking of up to four smartphones without reducing frame rate or accuracy. Our evaluation shows that MilliSonic achieves 0.7mm median 1D accuracy and a 2.6mm median 3D accuracy for smartphones, which is 5x more accurate than state-of-the-art systems. MilliSonic enables two previously infeasible interaction applications: a) 3D tracking of VR headsets using the smartphone as a beacon and b) fine-grained 3D tracking for the Google Cardboard VR system using a small microphone array. We present MilliSonic, a novel system that pushes the limits of acoustic based motion tracking and localization. We show for the first time how to achieve sub-mm 1D tracking and localization accuracies using acoustic signals on smartphones, in the presence of multipath. To achieve this, we introduce algorithms that use the phase of FMCW signals to disambiguate between multiple paths. We also enable multiple smartphones to transmit concurrently using time-shifted FMCW acoustic signals and enable concurrent tracking without sacrificing accuracy or frame rate. While this paper presents multiple benchmarks, user studies and evaluation in indoor and outdoor environments, more extensive evaluation is required to understand its behavior in various edge cases as well as in rooms with significant multipath that can adversely affect accuracy. Here, we discuss the limitations of our current system design. First, we support simple occlusions such as fabric and paper, but do not support human limbs or the device itself. Additional algorithmic development is required to support these practical occlusion scenarios. Second, while our design has better drift characteristics than prior work on acoustic tracking, further work is required to make it comparable to optical based systems. One approach is to perform sensor fusion with IMU data and achieve better accuracy, lower latency and more resilience to clock drifts. This could also enable VR headset tracking while using a mobile beacon (i.e., smartphone) in the hand instead of placing it on a table. Our current range is limited to 2 m. This is because the microphones in our array prototype are not optimized for performance and are not designed to have optimal response in the 17.5–23.5 kHz frequencies. Finally, we support upto 4–5 concurrent smartphone acoustic transmissions without affecting the frame rate per device. One way to increase the number of concurrent devices is to use longer chirps so as to support more time-shifted FMCW chirps that can be allocated to different smartphones. This however comes at the expense of the frame rate per device.
"
Casual Microtasking: Embedding Microtasks in Facebook,https://dl.acm.org/doi/10.1145/3290605.3300249,"
Microtasks enable people with limited time and context to contribute to a larger task. In this paper we explore casual microtasking, where microtasks are embedded into other primary activities so that they are available to be completed when convenient. We present a casual microtasking experience that inserts writing microtasks from an existing microwriting tool into the user's Facebook feed. From a two-week deployment of the system with nine people, we observe that casual microtasking enabled participants to get things done during their breaks, and that they tended to do so only after first engaging with Facebook's social content. Participants were most likely to complete the writing microtasks during periods of the day associated with low focus, and would occasionally use them as a springboard to open the original document in Word. These findings suggest casual microtasking can help people leverage spare micromoments to achieve meaningful micro-goals, and even encourage them to return to work. We studied casual microtasking by inserting writing microtasks into people’s Facebook feeds. By analyzing this experience with nine people over two weeks, we find that casual microtasking enabled participants to make writing contributions in a lightweight way during their Facebook breaks while still allowing them to ignore the microtasks when they did not feel like engaging with work. Casual microtasking was particularly useful for completing microtasks associated with low-priority documents that required limited context because it helped them stay engaged with those documents without committing to larger edits. Casual microtasking offers a new way for individuals to complete work in alternative contexts, acting an avenue for reminding, completing secondary tasks, and returning to productivity.
"
Making Sense of Art: Access for Gallery Visitors with Vision Impairments,https://dl.acm.org/doi/10.1145/3290605.3300250,"
While there is widespread recognition of the need to provide people with vision impairments (PVI) equitable access to cultural institutions such as art galleries, this is not easy. We present the results of a collaboration with a regional art gallery who wished to open their collection to PVIs in the local community. We describe a novel model that provides three different ways of accessing the gallery, depending upon visual acuity and mobility: virtual tours, self-guided tours and guided tours. As far as possible the model supports autonomous exploration by PVIs. It was informed by a value sensitive design exploration of the values and value conflicts of the primary stakeholders. In this paper we have presented a values-based model for providing people with vision impairments equitable access to art galleries. The model was developed in collaboration with a regional gallery using value sensitive design. We conducted a formative study comparing different tactile presentation methods, then our main study which explored stakeholder values and initial ideas for the model. Finally, we conducted a pilot evaluation. User evaluation was extremely positive, and BAG is now planning to deploy the model more widely. Our research provides the first values-based framework and model for accessible access to cultural institutions. In particular, it highlights the importance of supporting independence and autonomy of visitors with visual impairments, and involving the artists when developing audio descriptions and translating their work into other sensory modalities. By clarifying stakeholder values and conflicts, our research helps ground future research into the use of IT technologies for this purpose. For example, it demonstrates the importance of providing online access both for PVIs with limited mobility and to prepare more mobile PVIs for a physical visit. Further, it motivates the use of digital design and manufacturing techniques such as 3D printing and laser cutting in the production of tactile presentations as well as research into wayfinding technologies for use in the gallery and supporting access with emerging virtual and augmented reality systems.

"
Co-Design Beyond Words: 'Moments of Interaction' with Minimally-Verbal Children on the Autism Spectrum,https://dl.acm.org/doi/10.1145/3290605.3300251,"
Existing co-design methods support verbal children on the autism spectrum in the design process, while their minimally-verbal peers are overlooked. We describe Co-Design Beyond Words (CDBW), an approach which merges existing co-design methods with practice-based methods from Speech and Language Therapy which are child-led and interests-based. These emphasise the rich detail that can be conveyed in the moment, through recognising occurrences of, for example, Joint Attention, Turn Taking and Imitation. We worked in an autism-specific primary school over 20 weeks with ten children, aged 5 to 8. We co-designed a playful prototype, the TangiBall, using the three iterative phases of CDBW; the Foundation Phase (preparation for interaction), the Interaction Phase (designing-and-reflecting in the moment) and the Reflection Phase (reflection-on-action). We contribute a novel co-design approach and present moments of interaction, the micro instances in design in which minimally-verbal children on the spectrum can convey meaning beyond words, through their actions, interactions, and attentional foci. These moments of interaction provide design insight, shape design direction, and reveal unique strengths, interests, and abilities. This work contributes Co-Design Beyond Words, a childled approach to co-design with minimally-verbal children on the autism spectrum. This approach values moments of interaction – social instances of connection, communication, or understanding between individuals (such as children and facilitating adults). We suggest that moments of interaction are important during the co-design process, as current codesign approaches operate at high cognitive and social levels, often misaligned with the existing skills and motivations of minimally-verbal children on the autism spectrum. This paper identifies an opportunity, and perhaps an obligation, for HCI researchers to address this imbalance. We highlight the importance of working towards methods, designs, and mind-sets that are inclusive, supportive, and empowering of minimally-verbal children on the autism spectrum and other communities who communicate differently, as we work towards answering the question: how does one have a voice in design if one is not verbal?
"
Emotional Utility and Recall of the Facebook News Feed,https://dl.acm.org/doi/10.1145/3290605.3300252,"
We report a laboratory study (N=53) in which participants browsed their own Facebook news feeds for 10-15 minutes, choosing exactly when to quit, and later rated the overall emotional utility of the episode before attempting to recall threads. Finally, the emotional utility of each encountered thread was rated while looking over a recording of the interaction. We report that Facebook browsing was, overall, an emotionally positive experience; that recall of threads exhibited classic primacy and recency serial order effects; that recalled threads were both more positive and more valenced (less neutral) on average, than forgotten threads; and that overall emotional valence judgments were predicted, statistically, by the peak and end thread judgments. We find no evidence that local quit decisions were driven by the emotional utility of threads. In the light of these findings, we discuss the suggestion that emotional utility might partly explain the attractiveness of reading the news feed, and that an emotional memory bias might further increase the attractiveness of the newsfeed in prospect. Our main, overarching aim in this study is to introduce the idea of emotional utility, and how this interacts with memory as part of the explanation for the attractiveness of the Facebook news feed. Our experiment has demonstrated the potential of using simple emotional utility ratings to throw light on aspects of Facebook use, and, in particular, on how memory for the experience of using Facebook might contribute to its attractiveness in prospect.
"
"The Breaking Hand: Skills, Care, and Sufferings of the Hands of an Electronic Waste Worker in Bangladesh",https://dl.acm.org/doi/10.1145/3290605.3300253,"
While repair work has recently been getting increasing attention in HCI, recycling practices have still remained relatively understudied, especially in the context of the Global South. To this end, building on our eight-month-long ethnography, this paper reports the electronic waste (`e-waste', henceforth) recycling practices among the e-waste recycler (`bhangari') communities in Dhaka, Bangladesh. In doing so, this paper offers the work of the bhangaris through an articulation of their hands and their uses. Drawing from a rich body of scholarly work on social science, we define and contextualize three characteristics of the hand of a bhangari: knowledge, care, and skills and collaboration. Our study also highlights the pains and sufferings involved in this profession. By explaining bhangari work through the hand, we also discuss its implications for design, and its connection to HCI's broader interest in sustainability. In the sections above, we have presented the electronic recycling practices in the main four e-waste markets in Dhaka, Bangladesh. We have shown how bhangaris, the e-waste workers, collect, sort, dismantle, and dispose broken electronics step-by-step. We have further focused on the role of the hand of bhangaris to deepen our understanding of their work. We have demonstrated how the hand of a bhangari is used to know, extend care, and perform collaborative and skillful work in this profession. Finally, we have also reported the pains and suffering of their hand due to various injuries and exposure to harmful chemicals. This study thus provides a deep understanding of bhangaris’ interaction with broken computing devices, and open up new scopes for HCI in both design and theory fronts. First, our work documents the informal process of e-waste management, an area that has been under-explored in HCI. This paper joins the growing body of work on the afteruse phase of digital technologies in informal markets in the Global South, and demonstrates the important human factors involved in it. Through an articulation of handworks, our study shows how bhangaris’ skills, experience, art, craft, knowledge, and suffering are involved in the e-waste management in Bangladesh. We note that bhangari works are often undervalued (our own study shows bhangaris are often neglected in the society), but they contribute significantly to sustainable digital consumption. Our study also shows how bhangari markets are also a place of innovation, repurposing, and improvisation, which supports similar observations in other places in the global south [11, 12, 65]. While such innovation in the repair market is often confined within producing novel electronics, bhangaris innovate all kinds of artifacts - starting from looking glass to the toothpick, and from paperweight to hair comb. Hence, we posit that bhangari work should be considered as an important site for innovation and sustainability studies in HCI. Second, our work demonstrates an unsafe working condition for the bhangaris. We have also discussed the sociotechnical complexities around this problem of occupational hazards in bhangari work. We call for HCI research to take a deeper look into this issue, along with the ongoing policy initiatives coming from other disciplines [55, 78, 90]. HCI research can focus on designing safe tools, techniques, and workplaces that will protect the bhangaris from the physical harm associated with recycling broken electronics in different phases. Considering the social and economic conditions of the bhangaris, design of such artifacts should be inexpensive and culturally appropriate. Besides such direct design interventions to bhangari work, we can also design technologies to create mass awareness, and a nation-wide movement to persuade the government to make policy and enforce laws that are required to ensure a safe working environment for this vulnerable and marginalized population in Bangladesh. Third, our study brings to the fore the tactile experiences of touching the broken electronics - a barely explored area in HCI. HCI’s central focus on ‘design’ and ‘use’ of technology has mostly confined the experience of touch within these two phases of the life-cycle of a technology. As a result, a rich set of experiences that take place when people touch a nonfunctioning, malfunctioning, or broken technologies have not gotten enough attention in HCI. This paper intends to draw the attention of HCI researchers of tangible interfaces and ergonomics to the tactile experiences of the bhangaris while interacting with the broken electronics. We argue that a research that may stem from this work can benefit HCI research on tangible interfaces in various ways. For example, by extending the designer’s vision from use to after-use phase will allow them to take into account the tactile experiences of the electronic waste workers in their design, as also indicated by Maestri and Wakkary [51]. Novel technologies can be built that are equally friendly for the users, repairers, and recyclers. Furthermore, many of the tactile experiences and hand activities of bhangari work can introduce many novel interactions in HCI. For example, twisting, pinching, rubbing, or bending an electronic device may introduce new modes of interactions with computing devices that are not much prevalent today. Fourth, in this study, we have used hand as the center of our analysis of bhangari work. There are many professions, especially art and craft works, that are mostly done by hand. Hence, the hand has long been a center of interest for social scientists and anthropologists [27, 53, 54, 93]. While many works require our active and skillful interactions with computing devices through hands, it is still a relatively an underused method of understanding human-computer interaction in HCI research. It should be noted that a long-standing HCI work on ergonomics looks at hand in a very functional dimension (and measure comfort, ease, efficiency through performances). However, the hand that we propose to study goes far beyond that and encompasses the emotional, artistic, skillful, emotional, and political dimensions of work. In this paper, by putting the hand at the center, we have shown how different modes of a hand constitute the work of breaking computers and how a rich set of human emotions and social contexts are attached to them. Studying hands will provide a detailed explanation of the skills involved in a work, and the politics and emotions associated with them. We argue that such findings are valuable in designing better technologies that embody tangible experiences of users, by leveraging the functional and emotional dimensions of work.

"
EarTouch: Facilitating Smartphone Use for Visually Impaired People in Mobile and Public Scenarios,https://dl.acm.org/doi/10.1145/3290605.3300254,"
Interacting with a smartphone using touch input and speech output is challenging for visually impaired people in mobile and public scenarios, where only one hand may be available for input (e.g., while holding a cane) and using the loudspeaker for speech output is constrained by environmental noise, privacy, and social concerns. To address these issues, we propose EarTouch, a one-handed interaction technique that allows the users to interact with a smartphone using the ear to perform gestures on the touchscreen. Users hold the phone to their ears and listen to speech output from the ear speaker privately. We report how the technique was designed, implemented, and evaluated through a series of studies. Results show that EarTouch is easy, efficient, fun and socially acceptable to use. We present EarTouch, a novel interaction technique that facilitates smartphone use for BVI people in mobile and public scenarios. EarTouch not only improves one-handed use, but also provides an alternative to protect privacy and avoid social awkwardness. We actively engaged 36 BVI smartphone users in the design and evaluation of EarTouch based on formative interviews and user-participatory design activities. In a broader sense, EarTouch brings us an important step closer to accessible smartphones for all users of all abilities.

"
Diagnosing and Coping with Mode Errors in Korean-English Dual-language Keyboard,https://dl.acm.org/doi/10.1145/3290605.3300255,"
In countries where languages with non-Latin characters are prevalent, people use a keyboard with two language modes namely, the native language and English, and often experience mode errors. To diagnose the mode error problem, we conducted a field study and observed that 78% of the mode errors occurred immediately after application switching. We implemented four methods (Auto-switch, Preview, Smart-toggle, and Preview & Smart-toggle) based on three strategies to deal with the mode error problem and conducted field studies to verify their effectiveness. In the studies considering Korean-English dual input, Auto-switch was ineffective. On the contrary, Preview significantly reduced the mode errors from 75.1% to 41.3%, and Smart-toggle saved typing cost for recovering from mode errors. In Preview & Smart-toggle, Preview reduced mode errors and Smart-toggle handled 86.2% of the mode errors that slipped past Preview. These results suggest that Preview & Smart-toggle is a promising method for preventing mode errors for the Korean-English dual-input environment. In this study, we observed that a majority (78.3%) of the mode errors occurred immediately after an application switch. We considered four methods to help users deal with mode errors that follow an application switch. Through a series of field studies in the Korean-English dual-input environment, we observed that Auto-switch was not accepted well, Preview could reduce the mode errors significantly (from 75.1% to 41.3%), and Smart-toggle was adopted well by users (71.4% of the time). We also observed that Preview & Smart-toggle was an effective complementary combination of Preview and Smart-toggle. We conclude that Preview & Smart-toggle is the best among the four methods for preventing mode errors for the Korean-English dual-input environment.

"
Impact of Contextual Factors on Snapchat Public Sharing,https://dl.acm.org/doi/10.1145/3290605.3300256,"
Public sharing is integral to online platforms. This includes the popular multimedia messaging application Snapchat, on which public sharing is relatively new and unexplored in prior research. In mobile-first applications, sharing contexts are dynamic. However, it is unclear how context impacts users' sharing decisions. As platforms increasingly rely on user-generated content, it is important to also broadly understand user motivations and considerations in public sharing. We explored these aspects of content sharing through a survey of 1,515 Snapchat users. Our results indicate that users primarily have intrinsic motivations for publicly sharing Snaps, such as to share an experience with the world, but also have considerations related to audience and sensitivity of content. Additionally, we found that Snaps shared publicly were contextually different from those privately shared. Our findings suggest that content sharing systems can be designed to support sharing motivations, yet also be sensitive to private contexts. We conducted a survey to explore the role of context in sharing decisions on Snapchat, as well as users’ general motivations and considerations when publicly sharing content. We observed that some contexts, such as Snaps of people or Snaps taken inside a home, were more likely to be privately shared to My Story, and were associated with less comfort with publicly sharing to Our Story. Additionally, participants’ motivations for public sharing were largely intrinsic, while considerations centered around the audience and content of the Snap. As such, our indings can inform the design of context-aware applications to better support user decisionmaking and preferences.

"
Cluster Touch: Improving Touch Accuracy on Smartphones for People with Motor and Situational Impairments,https://dl.acm.org/doi/10.1145/3290605.3300257,"
We present Cluster Touch, a combined user-independent and user-specific touch offset model that improves the accuracy of touch input on smartphones for people with motor impairments, and for people experiencing situational impairments while walking. Cluster Touch combines touch examples from multiple users to create a shared user-independent touch model, which is then updated with touch examples provided by an individual user to make it user-specific. Owing to this combination, Cluster Touch allows people to quickly improve the accuracy of their smartphones by providing only 20 touch examples. In a user study with 12 people with motor impairments and 12 people without motor impairments, but who were walking, Cluster Touch improved touch accuracy by 14.65% for the former group and 6.81% for the latter group over the native touch sensor. Furthermore, in an offline analysis of existing mobile interfaces, Cluster Touch improved touch accuracy by 8.21% and 4.84% over the native touch sensor for the two user groups, respectively. We have investigated touch offsets created by people with motor impairments, and by people without impairments while walking and standing. Our analysis found that users across groups exhibited similar touch behaviors regarding where touches occur relative to their intended targets, but that the magnitude of these errors was more pronounced for people with motor impairments and for walking people than for people who were standing. To improve touch screen accuracy for people with motor impairments, and for people in motor-impairing situations, we created Cluster Touch, a combined user-independent and user-specific touch model that can improve touch accuracy by collecting only 20 touch examples from the target user. In an evaluation of Cluster Touch, we found that it was significantly more accurate in predicting intended target locations for people with motor impairments than the touch sensors found in the Google Nexus 6 smartphone. Cluster Touch also improved accuracy over two prior statistical machine learning models, Gaussian Process [55] and Linear Offset [9]. We also found that Cluster Touch was able to improve touch accuracy for people without impairments who were walking, demonstrating that Cluster Touch has the potential to provide accuracy improvements for a range of users in different situations.
"
MindDot: Supporting Effective Cognitive Behaviors in Concept Map-Based Learning Environments,https://dl.acm.org/doi/10.1145/3290605.3300258,"
While prior research has revealed the promising impact of concept mapping on learning, few have comprehensively modeled different cognitive behaviors during concept mapping. In addition, existing concept mapping tools lack effective feedback to support better learning behaviors. This work presents MindDot, a concept map-based learning environment that facilitates the cognitive process of comparing and integrating related concepts via two forms of support. A hyperlink support and an expert template. Study results suggested that both types of support had positive impact on the development of comparative strategies and that hyperlink support enhanced learning. We further evaluated the cognitive learning progress at a fine-grained level with two forms of visualizations. We then extracted several behavioral patterns that provided insights about the cognitive progress in learning. Lastly, we derive design recommendations that we hope will inspire future intelligent tutoring systems that automatically evaluate students' learning behaviors and foster them in developing effective learning behaviors. In this work, we presented MindDot, a concept map based learning environment that supports concept mapping through innovative integration of two features, hyperlink navigation and expert template. The strength and novelty of our system lie in its ability to assist students in developing comparative strategies. Our controlled study showed that students with hyperlink support outperformed the ones without hyperlink support. The system also demonstrated promising impact on supporting comparative strategies. We then presented a deeper and fine-grained understanding of how students employ comparative strategies, finding that there are indeed certain sequences of behaviors in the text (e.g., reading before adding a link) that suggest that students are making comparisons. Finally, we derive three major design recommendations for future concept mapping systems to foster the development of comparative strategies and enhancing learning outcomes: integrate tools with learning content, provide support that doubles as assessment, and adaptively support comparative strategies. Using computer-based concept mapping tools to facilitate concept mapping processes has the potential to substantially improve students’ abilities to comprehend expository texts.

"
Changing Perspective: A Co-Design Approach to Explore Future Possibilities of Divergent Hearing,https://dl.acm.org/doi/10.1145/3290605.3300259,"
Conventional hearing aids frame hearing impairment almost exclusively as a problem. In the present paper, we took an alternative approach by focusing on positive future possibilities of 'divergent hearing'. To this end, we developed a method to speculate simultaneously about not-yet-experienced positive meanings and not-yet-existing technology. First, we gathered already existing activities in which divergent hearing was experienced as an advantage rather than as a burden. These activities were then condensed into 'Prompts of Positive Possibilities' (PPP), such as 'Creating a shelter to feel safe in"". In performative sessions, participants were given these PPPs and 'Open Probes' to enact novel everyday activities. This led to 26 possible meanings and according devices, such as ""Being able to listen back into the past with a rewinder"". The paper provides valuable insights into the interests and expectations of people with divergent hearing as well as a methodological contribution to a possibility-driven design. The present study focused on potential advantages of divergent hearing. For a problem-driven technology, such as hearing aids, this perspective is uncommon. We reframed ‘non-normal hearing’ as ‘divergent hearing’ to change its meaning from a disability to a possibility for enhancement and empowerment. One may argue that this is a rather cynical view, mainly sugarcoating the manifold problems arising from hearing loss. While we certainly neither want to trivialize problems nor argue against technological progress, we believe that a change of perspective is not only helpful, but also in the interest of people with divergent hearing. It broadens the view rather than limits it. In addition, participants neither found it inappropriate nor pejorative to talk about positive aspects of their divergent hearing. They enjoyed involving themselves in speculations about future activities and devices. Besides the new perspective on ‘non-normal’ hearing and related technologies, the present study also provided an innovative methodological approach to explore potential positive activities and technologies involved. This approach neither started from a particular technology to speculate about emerging activities, nor from given activities to speculate about novel technologies. It simultaneously provided ambiguous meaning grounded in human experience and ambiguous material grounded in the human body. By instilling a performative dialogue among injected meaning, given material and the participant, possible activities emerged in which all elements mutually constituted themselves. On the one hand, each single of those entanglements is a useful anecdote by itself telling a situated story about a positive future activity, thereby serving as inspiration for design. On the other hand, it was possible to further group activities to provide an idea of the topics and technologies people with divergent hearing are interested in. In sum, we found fostering ‘intra-active’ dynamics through our particular method helpful and promising, both for exploring future activities in possibilitydriven design and for changing the perspective on the predominantly problem-driven approach to hearing technology.
"
Failing with Style: Designing for Aesthetic Failure in Interactive Performance,https://dl.acm.org/doi/10.1145/3290605.3300260,"
Failure is a common artefact of challenging experiences, a fact of life for interactive systems but also a resource for aesthetic and improvisational performance. We present a study of how three professional pianists performed an interactive piano composition that included playing hidden codes within the music so as to control their path through the piece and trigger system actions. We reveal how apparent failures to play the codes occurred for diverse reasons including mistakes in their playing, limitations of the system, but also deliberate failures as a way of controlling the system, and how these failures provoked aesthetic and improvised responses from the performers. We propose that creative and performative interfaces should be designed to enable aesthetic failures and introduce a taxonomy that compares human approaches to failure with approaches to capable systems, revealing new creative design strategies of gaming, taming, riding and serving the system. We have revealed how failure in live performance is both a complex and aesthetically important matter. Failure can be considered as being multi-layered, with failure at one level often relying on success at another and vice versa, leading to variation, improvisation and liveness. We have revealed how humans can adopt diverse strategies to responding to and also deliberately invoking failure in interaction, potentially gaming, taming, riding and even serving the system and have proposed that such strategies may benefit several areas of HCI including cultural and entertainment computing, human-robot interactions and conversational interfaces. We encourage researchers and practitioners to consider failure as both a layered and aesthetic matter and to be open to creative strategies for engaging with failure that: enable humans to better assert their creativity with increasingly capable systems; enable autonomous and intelligent systems to better reason about the complexities of failure; and ultimately design systems that are themselves capable of failing in appropriate and aesthetic ways.

"
"The Channel Matters: Self-disclosure, Reciprocity and Social Support in Online Cancer Support Groups",https://dl.acm.org/doi/10.1145/3290605.3300261,"
People with health concerns go to online health support groups to obtain help and advice. To do so, they frequently disclose personal details, many times in public. Although research in non-health settings suggests that people self-disclose less in public than in private, this pattern may not apply to health support groups where people want to get relevant help. Our work examines how the use of private and public channels influences members' self-disclosure in an online cancer support group, and how channels moderate the influence of self-disclosure on reciprocity and receiving support. By automatically measuring people's self-disclosure at scale, we found that members of cancer support groups revealed more negative self-disclosure in the public channels compared to the private channels. Although one's self-disclosure leads others to self-disclose and to provide support, these effects were generally stronger in the private channel. These channel effects probably occur because the public channels are the primary venue for support exchange, while the private channels are mainly used for follow-up conversations. We discuss theoretical and practical implications of our work. This research examined how members self-disclosed in private and public channels in an online cancer support community, and how channel differences moderated the reciprocity of self-disclosure and the social support they received. To summarize the major results: • H1 and H2: People expressed more negative self-disclosure when starting threads to seek support than when replying to them. When replying to others, people expressed more negative self-disclosure in public channel, revealing more about negative aspects of their lives. • H3: Reciprocity occurred for negative self-disclosure. The reciprocity effect was stronger in the private channel than in the public one. • H4: Senders’ negative self-disclosure was associated with their receiving both informational and emotional support. Again, these effects were stronger in the private channel. The effects of self-disclosure on receiving support were mediated by the effects of self-disclosure on judgments that people were seeking support, with complete mediation in the case of emotional support and partial mediation in the case of informational support. Specifically, senders’ negative self-disclosure was associated with the perception that they were seeking both emotional and informational support which, in turn, was associated with them receiving the corresponding type of support.
"
"Design Goals for Playful Technology to Support Physical Activity Among Wheelchair
                     Users",https://dl.acm.org/doi/10.1145/3290605.3300262,"
Playful technology has the potential to support physical activity (PA) among wheelchair users, but little is known about design considerations for this audience, who experience significant access barriers. In this paper, we lever-age the Integrated Behavioural Model (IBM) to understand wheelchair users' perspectives on PA, technology, and play.First, we present findings from an interview study with eight physically active wheelchair users. Second, we build on the interviews in a survey that received 44 responses from a broader group of wheelchair users. Results show that the anticipation of positive experiences was the strongest predictor of engagement with PA, and that accessibility concerns act as barriers both in terms of PA participation and technology use. We present four design goals - emphasizing enjoyment,involving others, building knowledge and enabling flexibility - to make our findings actionable for researchers and designers wishing to create accessible playful technology to support PA. Playful technology has the potential to support PA, but there has been little research regarding the design of playful technology for wheelchair users. Our work provides a mixedmethods enquiry into manual wheelchair users’ perspectives on PA and technology, leveraging the IBM to provide an indepth exploration. Findings highlight that anticipation of positive experiences is the strongest predictor of engagement with PA, supporting the development of playful solutions that by nature can highlight this element. Thereby, our research contributes to the growing body of HumanComputer Interaction research addressing the development of technology for people with disabilities that extends beyond accessibility considerations, enabling other researchers and designers to create playful technology that can support PA among wheelchair users.
"
Designing 'True Colors': A Social Wearable that Affords Vulnerability,https://dl.acm.org/doi/10.1145/3290605.3300263,"
Vulnerability is a common experience in everyday life and is frequently perceived as a flaw to be excised in technology design. Yet, research indicates it is an essential aspect of wholehearted living among others. In this paper, we present the design and deployment of 'True Colors', a novel wearable device intended to support social interaction in a live action roleplay game (LARP) setting. We describe the Research-through-Design process that helped us to discover and articulate the possibility space of vulnerability in the design of social wearables, as support for producing a sense of social empowerment and connection among wearers within the LARP. We draw conclusions that may be of value to others designing wearables and related technologies aimed at supporting co-located social interaction in games/play. We designed and then tested True Colors, a social wearable meant to augment co-located social interaction in a LARP context. The final design worked well within the LARP narrative and world mechanics, supporting valuable social interactions among players. To both the researchers’ and the LARP designers’ surprise, players eschewed the more combative possibilities of the wearables, instead focusing on the opportunities that the devices afforded for engaging in collaborative, supportive social encounters. By gravitating towards using breakdowns and healing functions, the players emphasized the social value of experiencing vulnerability together. This paper extends previous work on wearable technology in LARPs [41] and includes key components that supported embracing vulnerability, including supporting emotional resonance, social signaling, and spectator sensitivity (e.g. of moments of weakness); supporting authentic self-presentation and choice; and supporting overcoming difficulties together (e.g. through social touch). Although our insights are limited to games/play (particularly similar forms of LARPs), it validates previous works on social affordances for LARPing [41] and in games and play more generally [35]. Our results together with these works point to the value of social affordances as good guiding design concepts that provide a fruitful level of abstraction and concreteness. Similarly to experiential qualities [40], they focus on user experiences, yet explicitly referring to and evoking social ones. They are also concrete enough to relate to properties of the technology, such as its functionality, interactivity, and (technology) affordances [20]. Vulnerability has been researched and established as valuable positive and valuable from an emotional and social point of view [7]. Here, we add to that work relating vulnerability to valuable experiential qualities, design concepts and features (social affordances), which provides a good starting point to extended design work to support embracing vulnerability. Also, our observations of, and data from, players embracing vulnerability through wearables adds to previous empirical data substantiating and illustrating the value of embracing vulnerability [7]. Last, our strategies to design for vulnerability resonate with previous research insights of those who live fully through embracing vulnerability [7]: they cultivate authenticity (Supporting Authentic Self-presentation); “dare greatly” to live “wholeheartedly” (Supporting ‘Big Feels’); do so together (Overcoming Difficulties Together); and do so by choice (Information, Choice, and Consent). Rather than always being a flaw to be excised, strategic vulnerability has the potential to also produce a sense of social empowerment and connection. Although the results presented in this paper were obtained within a LARP setting, the design and reception of these wearables reveal an interesting niche for wearables that might inspire others outside the realms of play. Still, how one designs for vulnerability beyond the magic circle of games remains unclear. We see the design instance presented in this paper as a first exploration of many more to come. We encourage other designers of technologies in HCI to explore new paths that engage people with one another to create deep social connection, and consider the potential value of designing for (appropriate) vulnerability.

"
"Investigating Slowness as a Frame to Design Longer-Term Experiences with Personal
                     Data: A Field Study of Olly",https://dl.acm.org/doi/10.1145/3290605.3300264,"
We describe the design and deployment of Olly, a domestic music player that enables people to re-experience digital music they listened to in the past. Olly uses its owner's Last.FM listening history metadata archive to occasionally select a song from their past, but offers no user control over what is selected or when. We deployed Olly in 3 homes for 15 months to explore how its slow pace might support experiences of reflection and reminiscence. Findings revealed that Olly became highly integrated in participants lives with sustained engagement over time. They drew on Olly to reflect on past life experiences and reactions indicated an increase in perceived value of their Last.FM archive. Olly also provoked reflections on the temporalities of personal data and technology. Findings are interpreted to present opportunities for future HCI research and practice. We designed Olly to critically investigate how its slow pace and temporal expressiveness could sustain longer-term experiences with participants’ respective personal music listening history archives. Findings illustrated that Olly became integrated into participants’ everyday lives, supporting a range of experiences around it, in addition to direct interactions. They also highlighted how, over time, participants’ perceived their relationship with Olly to develop and grow, reinforcing attachment to their Last.FM archive and to Olly itself. Our work highlights the need for future research to explore how new technologies could be designed that embrace unfamiliar constraints, operate independently, emphasize pre- or postinteraction experiences, in addition to moments of direct engagement. Designing for slowness and longer-term human-technology relations represents an important area for future research in the HCI community. Our work contributes a rare account of how slowness can operate as a frame for creating design artifacts that can sustain valued, longer-term experiences with archives of personal data in everyday life. More broadly, our work explores what a different pathway for designing technology that supports rich human experiences of reflection and meaning-making might look like. An overarching goal of our work has been to constructively question normative assumptions in technology design (e.g., creating devices that are always-on, attention grabbing, and ‘waiting for you’). We believe that new possibilities for change can emerge through a design-led approach that combines the crafting of highly resolved design artifacts that offer a concrete ‘feel’ of future possibilities, with longer-term narratives of human experiences of them. Not only change that expands how we question assumptions in design, but also change manifested through envisioning, debating, and exploring real alternatives for a different way of being with technology. As new forms of technology mediate our everyday experiences, there is a critical need in the HCI community to delve deeply into seemingly mundane activities (like listening to digital music) to ask how our normative assumptions of design, technology use, and human experience are adequate, adaptive, and what we want. The application of slowness and temporality as conceptual frames for design have clear links to growing initiatives in HCI investigating how enduring forms of technology can be designed and the implications this might have for people’s everyday lives over time. We hope this research offers another step toward nurturing future initiatives in the HCI community that bring the subject of human relations with technology over time more seriously into focus, now and long into the future.
"
From HCI to HCI-Amusement: Strategies for Engaging what New Technology Makes Old,https://dl.acm.org/doi/10.1145/3290605.3300265,"
Notions of what counts as a contribution to HCI continue to be contested as our field expands to accommodate perspectives from the arts and humanities. This paper aims to advance the position of the arts and further contribute to these debates by actively exploring what a ""non-contribution"" would look like in HCI. We do this by taking inspiration from Fluxus, a collective of artists in the 1950's and 1960's who actively challenged and reworked practices of fine arts institutions by producing radically accessible, ephemeral, and modest works of ""art-amusement."" We use Fluxus to develop three analogous forms of ""HCI-amusements,"" each of which shed light on dominant practices and values within HCI by resisting to fit into its logics. Through the production of a series of HCI-amusements, we have argued that certain practices of design may be more impactful (in a personal and subjective sense) if questions of evaluation, analysis, and contribution are sidelined. We describe forms of exchange, publication, and circulation that could make space for these kinds of highly personalized and subjective gestures and exercises among HCI researchers. Using art history as a guide, we argue that our ""HCI-amusements"" should circulate independently of, but in direct with relationship to HCI, in order to maintain a critical correspondence produced through difference. We hope that the ongoing expansion and acceptance for diverse kinds of research at HCI will eventually find ways to weave in the practices we suggest.
"
"Evaluating the Impact of a Mobile Neurofeedback App for Young Children at School and
                     Home",https://dl.acm.org/doi/10.1145/3290605.3300266,"
About 18% of children in industrialized countries suffer from anxiety. We designed a mobile neurofeedback app, called Mind-Full, based on existing design guidelines. Our goal was for young children in lower socio-economic status schools to improve their ability to self-regulate anxiety by using Mind-Full. In this paper we report on quantitative outcomes from a sixteen-week field evaluation with 20 young children (aged 5 to 8). Our methodological contribution includes using a control group, validated measures of anxiety and stress, and assessing transfer and maintenance. Results from teacher and parent behavioral surveys indicated gains in children's ability to self-regulate anxiety at school and home; a decrease in anxious behaviors at home; and cortisol tests showed variable improvement in physiological stress levels. We contribute to HCI for mental health with evidence that it is viable to use a mobile app in lower socio-economic status schools to improve children's mental health. Our research contributes to HCI for mental health by showing the viability of an anxiety intervention using a low-cost, mobile EEG-NF system with a marginalized, hard to reach population of young children. We hope our work contributes to the uptake of a research agenda in Positive Computing focusing on emotion regulation and socio-emotional learning for young children. We see NF apps like ours as a tool — one of many — needed through children’s lives to help them overcome the negative impacts of poverty and trauma on brain development, and subsequent education and personal development. Our results are promising and we urge other researchers to explore how digital mental health applications can be used to support children and their caregivers from marginalized populations.

"
Geodesy: Self-rising 2.5D Tiles by Printing along 2D Geodesic Closed Path,https://dl.acm.org/doi/10.1145/3290605.3300267,"
Thermoplastic and Fused Deposition Modeling (FDM) based 4D printing are rapidly expanding to allow for space- and material-saving 2D printed sheets morphing into 3D shapes when heated. However, to our knowledge, all the known examples are either origami-based models with obvious folding hinges, or beam-based models with holes on the morphing surfaces. Morphing continuous double-curvature surfaces remains a challenge, both in terms of a tailored toolpath-planning strategy and a computational model that simulates it. Additionally, neither approach takes surface texture as a design parameter in its computational pipeline. To extend the design space of FDM-based 4D printing, in Geodesy, we focus on the morphing of continuous double-curvature surfaces or surface textures. We suggest a unique tool path - printing thermoplastics along 2D closed geodesic paths to form a surface with one raised continuous double-curvature tiles when exposed to heat. The design space is further extended to more complex geometries composed of a network of rising tiles (i.e., surface textures). Both design components and the computational pipeline are explained in the paper, followed by several printed geometric examples. In this paper, we introduced a novel path planning approach for surfaces with self-rising continuous double-curvature textures. Through this work, we hope to empower designers to quickly customize and prototype such morphing surfaces, as well as enrich the toolbox of 4D printing and shape-changing materials. We believe such accumulated advancements following the trajectory of 4D printing literature will enlarge the design space, the practicality, and applicability of FDMbased 4D printing. With the increasing design parameters from single-curvature to double-curvature surfaces; from smooth to textured surfaces; from rigid to flexible structures, we hope a growing community of designers and makers will join us in the journey of democratizing 4D printing.
"
In a Silent Way: Communication Between AI and Improvising Musicians Beyond Sound,https://dl.acm.org/doi/10.1145/3290605.3300268,"
Collaboration is built on trust, and establishing trust with a creative Artificial Intelligence is difficult when the decision process or internal state driving its behaviour isn't exposed. When human musicians improvise together, a number of extra-musical cues are used to augment musical communication and expose mental or emotional states which affect musical decisions and the effectiveness of the collaboration. We developed a collaborative improvising AI drummer that communicates its confidence through an emoticon-based visualisation. The AI was trained on musical performance data, as well as real-time skin conductance, of musicians improvising with professional drummers, exposing both musical and extra-musical cues to inform its generative process. Uni- and bi-directional extra-musical communication with real and false values were tested by experienced improvising musicians. Each condition was evaluated using the FSS-2 questionnaire, as a proxy for musical engagement. The results show a positive correlation between extra-musical communication of machine internal state and human musical engagement. As interaction with Artificial Intelligences – and in particular creative improvisers – becomes more commonplace, how we interact and collaborate with co-creational AI systems is an increasingly important area of research. In this paper we have investigated how extra-musical cues between human and AI improvisers can affect the achievement of flow states in an improvising duet. Our results demonstrate that, at least for the performers evaluated, communication of a confidence metric improves the human performer’s ability to achieve flow states more readily. Additionally, we demonstrated that the resulting improvisational performances are more likely to be perceived positively by non-expert audiences in terms of musical balance between instruements. These results support our hypothesis that the extra-musical communication of ‘internal state’ of the machine to the human can facilitate more engaging human-machine musical interactions. Our experimentation with the use of biometric data (SC) as a proxy for the performer’s musical engagement that could be communicated to the AI did not improve the performer’s experience of musical flow, despite prior research showing this to be the case with pairs of improvising human musicians [11]. A difference between this previous study and our own was the location of the sensor: the Empatica E4 watch used in our experiments can only be worn on the wrist, whereas the previous study used a sensor attached to the ankle of the performer, to minimise inconsistencies due to movement which are common when improvising during performance. That extra-musical communication of ‘internal state’of the human musician to the machine would also facilitate more engaging human-machine musical interactions was not supported in this study but is also not ruled-out. In further research we aim to draw on other kinds of biometric sensors that are more resilient to the effects of sudden moment that is a necessary part of physical playing and explore other modes of extra-musical communication. Conceptualising intelligent machines as creative partners rather than passive tools or instruments is relatively new. While we have a rich and well explored history of improvisation between human performers to draw upon, improvising with an alien, non-human yet active participant creates many exciting new possibilities for human-machine partnerships. The challenge, which we have begun to explore in this paper, is to maximise the creative benefits and possibilities for both performers and audiences.

"
Towards Collaboration Translucence: Giving Meaning to Multimodal Group Data,https://dl.acm.org/doi/10.1145/3290605.3300269,"
Collocated, face-to-face teamwork remains a pervasive mode of working, which is hard to replicate online. Team members' embodied, multimodal interaction with each other and artefacts has been studied by researchers, but due to its complexity, has remained opaque to automated analysis. However, the ready availability of sensors makes it increasingly affordable to instrument work spaces to study teamwork and groupwork. The possibility of visualising key aspects of a collaboration has huge potential for both academic and professional learning, but a frontline challenge is the enrichment of quantitative data streams with the qualitative insights needed to make sense of them. In response, we introduce the concept of collaboration translucence, an approach to make visible selected features of group activity. This is grounded both theoretically (in the physical, epistemic, social and affective dimensions of group activity), and contextually (using domain-specific concepts). We illustrate the approach from the automated analysis of healthcare simulations to train nurses, generating four visual proxies that fuse multimodal data into higher order patterns. To conclude, there is more going on than is visible to the machines. Our goal is to improve the resources for interpretation where none existed before, in order to provoke more productive reflection and discussion, grounded for the first time in empirical evidence. In educational terms, therefore, collaboration proxies should serve as scaffolds for formative assessment (feedback for ongoing learning) rather than summative assessment (which assigns a grade). Conceivably, as the evidence base grows in size and quality, paterns in the data may be so highly correlated with strong or weak teamwork that they come to serve as proxies for quality. Until then, the priority is to further evaluate how students and educators engage with the representations, and how they can be improved.
"
At Your Service: Designing Voice Assistant Personalities to Improve Automotive User Interfaces,https://dl.acm.org/doi/10.1145/3290605.3300270,"
This paper investigates personalized voice characters for in-car speech interfaces. In particular, we report on how we designed different personalities for voice assistants and compared them in a real world driving study. Voice assistants have become important for a wide range of use cases, yet current interfaces are using the same style of auditory response in every situation, despite varying user needs and personalities. To close this gap, we designed four assistant personalities (Friend, Admirer, Aunt, and Butler) and compared them to a baseline (Default) in a between-subject study in real traffic conditions. Our results show higher likability and trust for assistants that correctly match the user's personality while we observed lower likability, trust, satisfaction, and usefulness for incorrectly matched personalities, each in comparison with the Default character. We discuss design aspects for voice assistants in different automotive use cases. In this paper we explored the influence of personalized voice assistant characters on UX, acceptance, trust, and workload compared to a non-personalized assistant. The results show that personalization has a positive effect on trust and likability if the voice assistant character matches the user’s personality. However, a mismatch can cause displeasure. Future voice assistants need to adapt to the user as well as to the environment. Our findings provoke the personalization of voice assistants not only on a cultural level as it is already happening, but on a context-aware basis, e.g. being short and precise in driving-related situations, and getting chatty on an empty road when the driver is bored. A neutral assistant is recommended as starting point before gradually adjusting its personality to the user’s needs, either through implicit system decisions or explicit user input. Successive work should look into how an optimal assistant personality can be chosen for the driver. Furthermore, it would be interesting to investigate how to transfer our findings to driving settings with higher automation levels.

"
"Toward Algorithmic Accountability in Public Services: A Qualitative Study of Affected Community Perspectives on Algorithmic Decision-making
                     in Child Welfare Services",https://dl.acm.org/doi/10.1145/3290605.3300271,"
Algorithmic decision-making systems are increasingly being adopted by government public service agencies. Researchers, policy experts, and civil rights groups have all voiced concerns that such systems are being deployed without adequate consideration of potential harms, disparate impacts, and public accountability practices. Yet little is known about the concerns of those most likely to be affected by these systems. We report on workshops conducted to learn about the concerns of affected communities in the context of child welfare services. The workshops involved 83 study participants including families involved in the child welfare system, employees of child welfare agencies, and service providers. Our findings indicate that general distrust in the existing system contributes significantly to low comfort in algorithmic decision-making. We identify strategies for improving comfort through greater transparency and improved communication strategies. We discuss the implications of our study for accountable algorithm design for child welfare applications. The fairness and interpretability of algorithms deployed in consequential decision-making settings has received significant attention in recent years. This has led to a numerous proposals for auditing strategies [15, 45], and the development of models that are constructed to be in some sense ‘fair’ by design [2, 23, 30]. Existing work has even explored the algorithmic bias properties of tools deployed in the child welfare system [10]. Our study suggests that these technical solutions are in and of themselves insufficient for ensuring that the resulting algorithmic systems are perceived as fair and just. Perceptions of algorithmic fairness are heavily influenced by perceptions of procedural and interpersonal justice of the child welfare system as a whole. Effective design strategies must therefore consider how technological solutions can be designed to work in concert with complementary policy changes to impact community perceptions of system justice.
"
ActiveInk: (Th)Inking with Data,https://dl.acm.org/doi/10.1145/3290605.3300272,"
During sensemaking, people annotate insights: underlining sentences in a document or circling regions on a map. They jot down their hypotheses: drawing correlation lines on scatterplots or creating personal legends to track patterns. We present ActiveInk, a system enabling people to seamlessly transition between exploring data and externalizing their thoughts using pen and touch. ActiveInk enables the natural use of pen for active reading behaviors, while supporting analytic actions by activating any of these ink strokes. Through a qualitative study with eight participants, we contribute observations of active reading behaviors during data exploration and design principles to support sensemaking. ActiveInk represents a foray into active reading applications for heterogeneous data including visualizations, maps, and documents. It blends the operations of externalizing thoughts with analytic actions in a seamless combination of functions to be carried out with digital pen+touch. The hybrid interface of ActiveInk integrates two ways of working with pen actions, which our study revealed to be complementary for sensemaking activities. Prefix inking is useful for batch operations, while postfix ink activation is useful for interleaving multiple actions with externalizing thoughts. This work offers several opportunities for extension. The ink recognition system we used does not distinguish between classes of marks, such as enclosures, underlines, and arrows. Our data selection model associates marks with any data items enclosed or intersected by an ink stroke. Distinguishing types of mark could be interesting to implement a finer inkdata association strategy. ActiveInk was made to demonstrate the possibilities of thinking with ink, but does not scale to a large number of views or with very large volumes of data. Addressing scalability issues requires further development, and implementing the ability to save, recall, and share analysis canvases. It also implies adding support for a wider diversity of charts, which could be achieved by moving to, e.g., Vega-lite [28]. To better understand the impact of ActiveInk on sensemaking and externalization, including any potential effects on the depth and quality of insights, we would like to run a longitudinal study in which participants analyze their own data using the hybrid interface. We suspect performance may improve with long-term use, and sensemaking strategies may change as people feel more engaged with their data. Furthermore, it would be interesting to study the use of ActiveInk in collaborative synchronous and asynchronous sensemaking scenarios.

"
Evaluating the Effect of Feedback from Different Computer Vision Processing Stages: A Comparative Lab Study,https://dl.acm.org/doi/10.1145/3290605.3300273,"
Computer vision and pattern recognition are increasingly being employed by smartphone and tablet applications targeted at lay-users. An open design challenge is to make such systems intelligible without requiring users to become technical experts. This paper reports a lab study examining the role of visual feedback. Our findings indicate that the stage of processing from which feedback is derived plays an important role in users' ability to develop coherent and correct understandings of a system's operation. Participants in our study showed a tendency to misunderstand the meaning being conveyed by the feedback, relating it to processing outcomes and higher level concepts, when in reality the feedback represented low level features. Drawing on the experimental results and the qualitative data collected, we discuss the challenges of designing interactions around pattern matching algorithms. This paper reported a comparative between-groups lab study examining the role of visual feedback in smart camera apps. Leveraging a novel experimental design centered on the creation of stop-motion animations, 40 participants were exposed to four different levels of feedback. Through a combination of quantitative and qualitative methods, our findings indicate a disconnect between user expectations and the information actually represented by the feedback. Participants exposed to keypoint marker feedback derived from early stages of processing showed a tendency to misunderstand it and overall they performed worse than participants who received no feedback at all. Conversely, participants who received keypoint marker feedback derived from later stages of processing demonstrated an improved understanding of the system operation. We conclude that the stage of processing from which feedback is derived plays an important role in users’ ability to develop coherent and correct understandings of a system’s operation. We hope that the results presented in this paper will inform the design of feedback in smart camera apps, and other applications of pattern recognition. More generally, we hope that our study method can be used by HCI researchers in future work exploring the design space of feedback and cues.

"
Smart and Fermented Cities: An Approach to Placemaking in Urban Informatics,https://dl.acm.org/doi/10.1145/3290605.3300274,"
What makes a city meaningful to its residents? What attracts people to live in a city and to care for it? Today, we might see such questions as concerns for HCI, given the emerging agendas of smart and connected cities, IoT, and ubiquitous computing: city residents' perceptions of and attitudes towards smart city technologies will play a role in technology acceptance. Theories of ""placemaking"" from humanist geography and urban planning address themselves to such concerns, and they have been taken up in HCI and urban informatics research. This theory offers ideas for developing community attachment, heightening the legibility of the city, and intensifying lived experiences in the city. We add to this body of research with an analysis of several initiatives of City Yeast, a community-based design collective in Taiwan that proposes the metaphor of fermentation as an approach to placemaking. We unpack how this approach shapes their design practice and link its implications to urban informatics research in HCI. We suggest that smart cities can also be pursued by leveraging the knowledge of city residents and helping to facilitate their participation in acts of perceiving, envisioning, and improving their local communities, including but not limited to smart and connected technologies. We have shown that many citizens of Taipei feel that the city lacks a global identity and that any initiative to make it smart and connected should build on such an identity. In pursuing such an identity collectively, City Yeast focuses on exploring current issues in Taipei’s urban design and city infrastructure and experimenting with (sometimes symbolic) strategies to redesign Taipei for its future. We have also found that citizens fear that smart city technologies – IoT, sensors, mobile networks, and so on – comprise a globalgeneric infrastructure. Similar to sky-scrapers and modern grocery stores, this new infrastructure threatensthe vibrancy of their city even as it modernizes it. In what follows, we develop our thoughts on how the City Yeast approach relates to placemaking by 1) focusing on how smart and connected cities agendas might be joined to an urban identity project as a part of its envisioning and implementation; 2) leveraging the knowledge of city dwellers and helping to facilitate their participation in acts of perceiving, envisioning, and improving their local communities; and 3) ofering design implications for smart city technologists.
"
Smart Home Security Cameras and Shifting Lines of Creepiness: A Design-Led Inquiry,https://dl.acm.org/doi/10.1145/3290605.3300275,"
Through a design-led inquiry focused on smart home security cameras, this research develops three key concepts for research and design pertaining to new and emerging digital consumer technologies. Digital leakage names the propensity for digital information to be shared, stolen, and misused in ways unbeknownst or even harmful to those to whom the data pertains or belongs. Hole-and-corner applications are those functions connected to users' data, devices, and interactions yet concealed from or downplayed to them, often because they are non-beneficial or harmful to them. Foot-in-the-door devices are product and services with functional offerings and affordances that work to normalize and integrate a technology, thus laying groundwork for future adoption of features that might have earlier been rejected as unacceptable or unnecessary. Developed and illustrated through a set of design studies and explorations, this paper shows how these concepts may be used analytically to investigate issues such as privacy and security, anticipatorily to speculate about the future of technology development and use, and generatively to synthesize design concepts and solutions. This research set out to investigate shifting and retreating lines of creepiness in the emerging smart home by focusing on smart home security cameras. Through a process of closely analyzing existing design interfaces combined with generating novel design scenarios and proposals, this inquiry has developed and illustrated three key concepts: digital leakage, hole-and-corner applications, and foot-in-thedoor devices. This paper has introduced these concepts and shown how they may be used analytically to understand relationships between interactive technology, user experience, and issues of privacy, security, trust, accountable, and fairness. These concepts may also be used anticipatorily and generatively, as this paper has shown, to extrapolate future trends and speculate about future possibilities, and to generate design interventions, solutions, and other design-oriented responses. While this paper makes minor contributions with regards to analyzing, anticipating, and generating specific designs related to smart home security cameras, the core contribution is to develop and present new and refined vocabulary for understanding, discussing, and addressing timely social and ethical issues with smart technologies. This contribution builds upon and adds to theoretical, empirical, and critical scholarship that has proposed similar concepts for understanding issues such as privacy and surveillance—such as Nissenbaum’s general theory of contextual integrity [[52]] and Zubeff’s concept of surveillance capitalism [[78]]. The three concepts developed in this research, however, are distinct from those developed in prior research in that they both arise from and are directed toward design broadly and interfaces, interactions, and user experiences specifically. These concepts allow us to go beyond generally labeling certain events as creepy, as invasions of privacy, or as breaches of trust by focusing on specific mechanisms by which design and technology lead to negative experiences and outcomes. By expanding our vocabulary through concepts such as digital leakage, holeand-corner applications, and foot-in-the-door devices, we increase our capacity as designers, researchers, and critics to concretely analyze, discuss, debate, and address the dizzying array of existing and emerging issues at hand.
"
"Deaf and Hard-of-hearing Individuals' Preferences for Wearable and Mobile Sound Awareness
                     Technologies",https://dl.acm.org/doi/10.1145/3290605.3300276,"
To investigate preferences for mobile and wearable sound awareness systems, we conducted an online survey with 201 DHH participants. The survey explores how demographic factors affect perceptions of sound awareness technologies, gauges interest in specific sounds and sound characteristics, solicits reactions to three design scenarios (smartphone, smartwatch, head-mounted display) and two output modalities (visual, haptic), and probes issues related to social context of use. While most participants were highly interested in being aware of sounds, this interest was modulated by communication preference--that is, for sign or oral communication or both. Almost all participants wanted both visual and haptic feedback and 75% preferred to have that feedback on separate devices (e.g., haptic on smartwatch, visual on head-mounted display). Other findings related to sound type, full captions vs. keywords, sound filtering, notification styles, and social context provide direct guidance for the design of future mobile and wearable sound awareness systems. We presented an online survey with 201 DHH participants investigating interest in sound awareness and preferences about the design of a mobile or wearable sound awareness technology. This study builds on past work on sound interests [3, 27, 39] to quantitatively examine the influence of demographic factors on interest level. We also specifically examine mobile and wearable form factors and feedback modalities. Results show a strong preference for having both haptic and visual feedback, the latter particularly for captions, with the most preferred device design being haptic notifications on a smartwatch and visual information on a head-mounted display or smartphone. Finally, our findings surface important cultural and social considerations that will need to be addressed for the successful adoption of mobile or wearable sound awareness technologies, such as a hesitance to use a sound awareness device with strangers or around Deaf friends or family.
"
"The Impact of User Characteristics and Preferences on Performance with an Unfamiliar
                     Voice User Interface",https://dl.acm.org/doi/10.1145/3290605.3300277,"
Voice User Interfaces (VUIs) are increasing in popularity. However, their invisible nature with no or limited visuals makes it difficult for users to interact with unfamiliar VUIs. We analyze the impact of user characteristics and preferences on how users interact with a VUI-based calendar, DiscoverCal. While recent VUI studies analyze user behavior through self-reported data, we extend this research by analyzing both VUI usage data and self-reported data to observe correlations between both data types. Results from our user study (n=50) led to four key findings: 1) programming experience did not have a wide-spread impact on performance metrics while 2) assimilation bias did, 3) participants with more technical confidence exhibited a trial-and-error approach, and 4) desiring more guidance from our VUI correlated with performance metrics that indicate cautious users. This paper presents the impact of user characteristics and VUI preferences on performance metrics of an unfamiliar VUI. We reviewed relevant research and identified user characteristics and VUI design preferences recently discussed in studies to impact users’ behavior with VUIs. From usage data collected from a user study (n=50), we have found the following: programming experience did not have a widespread impact on performance metrics while assimilation bias did, participants with more technical confidence exhibited a trial-and-error approach, and participants who desired visual guidance were more likely to have performance metrics that indicate cautious users. Based on our results, we present design implications for VUI design. A limitation of our study is that our participants were mostly in their 20’s. A sample including a more distributed age range could bring forth further insights of the impact of age. Additionally, we analyzed data retrieved from user interactions with a single context VUI (calendar management) that was multimodal. Future studies can compare our results with a voice-only VUI. Despite these limitations, our current findings can be used to design future adaptive techniques to support users in interacting with an unfamiliar VUI. Additional future work includes designing and evaluating adaptive VUIs that can recognize and tailor to individual user differences.
"
Pinpoint: A PCB Debugging Pipeline Using Interruptible Routing and Instrumentation,https://dl.acm.org/doi/10.1145/3290605.3300278,"
Difficulties in accessing, isolating, and iterating on the components and connections of a printed circuit board (PCB) create unique challenges in PCB debugging. Manual probing methods are slow and error prone, and even dedicated PCB testing equipment remains limited by its inability to modify the circuit during testing. We present Pinpoint, a tool that facilitates in-circuit PCB debugging through techniques such as programmatically probing signals, dynamically disconnecting components and subcircuits to test in isolation, and splicing in new elements to explore potential modifications. Pinpoint automatically instruments a PCB design and generates designs for a physical jig board that interfaces the user's PCB to our custom testing hardware and to software tools. We evaluate Pinpoint's ability to facilitate the debugging of various PCB issues by instrumenting and testing different classes of boards, as well as by characterizing its technical limitations and by soliciting feedback through a guided exploration with PCB designers. Pinpoint facilitates PCB debugging by addressing core issues in access, isolation, and iteration. By automatically instrumenting board designs and simplifying the fabrication of a hardware-software interface, it provides designers with new in-circuit functional testing methods. With continued growth in the space of personal electronic fabrication, we see Pinpoint as an important, yet early, step towards accessible computational support of PCB debugging tasks.

"
A Practice-Led Account of the Conceptual Evolution of UX Knowledge,https://dl.acm.org/doi/10.1145/3290605.3300279,"
The contours of user experience (UX) design practice have been shaped by a diverse array of practitioners and disciplines, resulting in a diffuse and decentralized body of UX-specific disciplinary knowledge. The rapidly shifting space that UX knowledge occupies, in conjunction with a long-existing research-practice gap, presents unique challenges and opportunities to UX educators and aspiring UX designers. In this paper, we analyzed a corpus of question and answer communication on UX Stack Exchange using a practice-led approach, identifying and documenting practitioners' conceptions of UX knowledge over a nine year period. Specifically, we used natural language processing techniques and qualitative content analysis to identify a disciplinary vocabulary invoked by UX designers in this online community, as well as conceptual trajectories spanning over nine years which could shed light on the evolution of UX practice. We further describe the implications of our findings for HCI research and UX education. In this paper, we performed a mixed-methods analysis of UX practitioner conversations on Stack Exchange, using a combination of content analysis and natural language processing to build a preliminary vocabulary of UX knowledge. Our analysis revealed a diverse array of knowledge types and concepts that emerged from a complex set of disciplinary sources, and a promising set of methods that could be used to further probe the dynamic nature of design knowledge in online communities. The approach to bubbling up UX knowledge is inherently generative in opening up new spaces for both methodological discussions and practical applications in education and pedagogy. Along this direction, we are able to ask broader questions such as how qualitative data analysis might productively be used alongside big data techniques, and how sources of big data such as social media conversations might be efectively utilized to guide future educational innovation.
"
Understanding Visual Cues in Visualizations Accompanied by Audio Narrations,https://dl.acm.org/doi/10.1145/3290605.3300280,"
It is often assumed that visual cues, which highlight specific parts of a visualization to guide the audience's attention, facilitate visualization storytelling and presentation. This assumption has not been systematically studied. We present an in-lab experiment and a Mechanical Turk study to examine the effects of integral and separable visual cues on the recall and comprehension of visualizations that are accompanied by audio narration. Eye-tracking data in the in-lab experiment confirm that cues helped the viewers focus on relevant parts of the visualization faster. We found that in general, visual cues did not have a significant effect on learning outcomes, but for specific cue techniques (e.g. glow) or specific chart types (e.g heatmap), cues significantly improved comprehension. Based on these results, we discuss how presenters might select visual cues depending on the role of the cues and the visualization type. We studied visual cues’ effectiveness and user preference by in-lab and online studies. The results show that visual cues may not lead to a significant improvement on people’s recall and comprehension of the visualizations with audio narration, but can help people focus on the relevant regions faster. We also found an inconsistency between the perceived effectiveness and the actual effectiveness of visual cues showing that people’s intuition on visual cues may not be accurate. Based on the results of our study, we propose different factors presenters should consider when choosing a visual cue including the role of the cue, the visual characteristics of the cued region, and the visual characteristics and the type of the visualization.

"
Context-Informed Scheduling and Analysis: Improving Accuracy of Mobile Self-Reports,https://dl.acm.org/doi/10.1145/3290605.3300281,"
Mobile self-reports are a popular technique to collect participant labelled data in the wild. While literature has focused on increasing participant compliance to self-report questionnaires, relatively little work has assessed response accuracy. In this paper, we investigate how participant context can affect response accuracy and help identify strategies to improve the accuracy of mobile self-report data. In a 3-week study we collect over 2,500 questionnaires containing both verifiable and non-verifiable questions. We find that response accuracy is higher for questionnaires that arrive when the phone is not in ongoing or very recent use. Furthermore, our results show that long completion times are an indicator of a lower accuracy. Using contextual mechanisms readily available on smartphones, we are able to explain up to 13% of the variance in participant accuracy. We offer actionable recommendations to assist researchers in their future deployments of mobile self-report studies. As researchers utilising self-report methods rely heavily on the answers provided by their participants, measuring and improving response accuracy is an important research avenue. Through a questionnaire assessing participants’ performance across a set of questions (recall, working memory, arithmetic) we identify contextual and study-wide factors affecting participant accuracy. Our results show that a short survey completion time did not correlate with a low response accuracy in our questionnaire items. In fact, surveys that took a long time to complete were more likely to be inaccurate. Furthermore, we find that contextual usage factors such as phone usage at the time of questionnaire arrival and the number of recent phone interactions influence participant accuracy. These contextual factors can be used to optimise questionnaire scheduling for improved data accuracy. We offer actionable recommendations to assist researchers in their future deployments of self-report studies. Cognition-aware scheduling provides a novel scheduling technique, which can best be used in combination with traditional ESM scheduling techniques (time or event-based) to reduce contextual bias. With an increased shift of Experience Sampling to the observation of external and verifiable phenomena [8, 11], we expect an increased focus on ensuring accuracy of responses in Experience Sampling.
"
BBeep: A Sonic Collision Avoidance System for Blind Travellers and Nearby Pedestrians,https://dl.acm.org/doi/10.1145/3290605.3300282,"
We present an assistive suitcase system, BBeep, for supporting blind people when walking through crowded environments. BBeep uses pre-emptive sound notifications to help clear a path by alerting both the user and nearby pedestrians about the potential risk of collision. BBeep triggers notifications by tracking pedestrians, predicting their future position in real-time, and provides sound notifications only when it anticipates a future collision. We investigate how different types and timings of sound affect nearby pedestrian behavior. In our experiments, we found that sound emission timing has a significant impact on nearby pedestrian trajectories when compared to different sound types. Based on these findings, we performed a real-world user study at an international airport, where blind participants navigated with the suitcase in crowded areas. We observed that the proposed system significantly reduces the number of imminent collisions. We proposed an assistive suitcase system, BBeep, that aimsto clear the path for blind users when walking through crowded spaces, by notifying both the user and sighted pedestrians about the risks of collision. It provides sound notifcations only when needed, based on pedestrian tracking and by predicting their future position in real-time. We frst investigated how to convey the sound feedback efectively to sighted pedestrians and designed the sonic notifcation interface of BBeep. Then, we conducted a real-world user study with visually impaired people in an airport. Results showed that BBeep reduces the number of situations of imminent collision risk when compared to notifying the blind user alone. Moreover, users found BBeep acceptable and appropriate to use in crowded, public spaces such as airports, train stations or shopping malls. Yet, they were more hesitant about using it in places they are supposed to be quiet. In the future, we plan to extend our collision prediction method, by using vision-based attention analysis to reduce the number of unnecessary sound emissions when the pedestrians have already noticed the presence of the blind user.

"
From Gender Biases to Gender-Inclusive Design: An Empirical Investigation,https://dl.acm.org/doi/10.1145/3290605.3300283,"
In recent years, research has revealed gender biases in numerous software products. But although some researchers have found ways to improve gender participation in specific software projects, general methods focus mainly on detecting gender biases -- not fixing them. To help fill this gap, we investigated whether the GenderMag bias detection method can lead directly to designs with fewer gender biases. In our 3-step investigation, two HCI researchers analyzed an industrial software product using GenderMag; we derived design changes to the product using the biases they found; and ran an empirical study of participants using the original product versus the new version. The results showed that using the method in this way did improve the software's inclusiveness: women succeeded more often in the new version than in the original; men's success rates improved too; and the gender gap entirely disappeared. In this paper, we have presented the first investigation into whether and how an HCI method to detect gender biases in software can generate more gender-inclusive designs. The method our study investigated was GenderMag; it was beyond the scope of this study to compare GenderMag with other usability inspection methods, or to generalize our results to other contexts. Also, other factors such as income, age, race, gender identification factors such as those discussed in [52], and algorithmic biases can influence software equity, and GenderMag does not address these. That said, in our setting, GenderMag alone did lead to more inclusive and more usable designs, as our results show. Specifically: • Improved designs in total: Participants in the postGenderMag condition, who used the design changes derived from the GenderMag analyses, were more successful on almost every individual task than Original participants. In total, the post-GenderMag participants failed less than half as often as participants in the Original version. • Accurately found issues: In 100% of the cases where analysts found an issue using GenderMag, the issue happened to one or more participants in the Original condition. Further, in most of the cases, the participants experiencing the issues had the facet values analysts predicted. • The facets pointed to the fixes: The method we used to achieve these results was to use the facets identified in Step 1 as starting points for creating the fixes (Step 2). The results suggest that this GenderMag-based sequence offers an effective, pre-user-study method to pinpoint and fix issues affecting gender- and cognitive-inclusivity. • From gender biases to gender-inclusiveness: After the design changes, the Original version’s gender gap in participants’ failure rates entirely disappeared. The results also illuminate the nuanced relationships between individual cognitive styles and gender. As such, the results have several direct implications for gender-inclusive design. First, designing for cognitive diversity improves software’s gender inclusiveness. Second, it is neither necessary nor desirable to devise two (or more) genderlabeled versions of the same software to serve different genders. Finally, many gender biases in software are cognitive biases, and many cognitive biases in software are gender biases. Thus, software designs that better support cognitive diversity also better support gender diversity—and improve the software for everyone.
"
"How to Work in the Car of the Future?: A Neuroergonomical Study Assessing Concentration, Performance and Workload Based on
                     Subjective, Behavioral and Neurophysiological Insights",https://dl.acm.org/doi/10.1145/3290605.3300284,"
Autonomous driving provides new opportunities for the use of time during a car ride. One such important scenario is working. We conducted a neuroergonomical study to compare three configurations of a car interior (based on lighting, visual stimulation, sound) regarding their potential to support productive work. We assessed participants? concentration, performance and workload with subjective, behavioral and EEG measures while they carried out two different concentration tasks during simulated autonomous driving. Our results show that a configuration with a large-area, bright light with high blue components, and reduced visual and auditory stimuli promote performance, quality, efficiency, increased concentration and lower cognitive workload. Increased visual and auditory stimulation paired with linear, darker light with very few blue components resulted in lower performance, reduced subjective concentration, and higher cognitive workload, but did not differ from a normal car configuration. Our multi-method approach thus reveals possible car interior configurations for an ideal workspace. Our neuroergonomical study shows that self-driving cars have the potential to serve as a workplace and that the interior design of light, sound and visual stimulation can be configured to support concentrated working. The multimethod approach helped us examine possible interior design set-ups and to come up with a holistic picture on the influences on concentration and workload. Our results are cumulative, which increases their robustness and extends our understanding of the constructs under investigation. We proposed three different variants of interior configurations for a workplace in a self-driving car and setup a driving simulator. We designed an experimental study that allowed us to investigate to what extent these configurations support a productive working style. The LC features relaxing light and enables the passenger to receive notifications and media content from their mobile phone on the interactive window screens of the car, thus combining the task of working with a more leisure-oriented environment. Our study shows that in this environment, subjective concentration, behavioral performance, and cognitive workload are comparable to the NC, the regular set-up of a car with standard lighting, traffic sound and a few visual distractors. The third interior variant (CC) was designed to deliberately boost the passenger’s concentration and productivity with an activating lighting concept, blurred window screens and reduced environmental sounds. The results of our study indicate that participants did indeed perform better in this set-up, while their perceived workload and actual cognitive workload were significantly lower than for the other design variants. The CC may be used as a guideline on how to adjust certain configurations in future autonomous cars in order to make them productive workplaces. By understanding the relationship between car configuration modes and cognitive workload, car manufacturers can design future interiors that adequately address human cognitive limitations, skills, and needs by providing optimal working conditions. To arrive at this conclusion, we used a multi-method approach combining subjective evaluation based on a questionnaire, behavioral measures capturing RTs and error rates, and EEG recordings assessing neurophysiological processes in the brain. The results of our study show that these methods complement each other well and, taken together, can provide a holistic picture about a participant’s experience and cognitive state in the context of working in an automotive setting. The study presented in this work should be regarded as a first step towards concrete design propositions for using cars as mobile offices. We ensured that - although being placed in a mock-up car - in all three configurations participants had the impression of being in an automated car (level 5). Thus, we offer an ecologically valid setup for inducing different mental workload and concentration levels by manipulating the interior design configuration. We based our design variants on the configuration of light, sound and visual stimulation. This is, quite obviously, only a subset of the interior features that can be configured. Future studies can build upon our initial design propositions and include other aspects such as temperature or seating configuration. Interestingly, we found that the leisure-oriented set-up did not have a negative effect on participant’s concentration, performance or workload. Still, in this study, the display of notifications and media on the interactive window screens was limited to a medium level. For future studies, it would be interesting to investigate the threshold of the amount of displayed media content for causing a cognitive overload and consequently impeding productive work. The next big step for future research would be to take this, and similar work out of the lab, and apply it to real-world scenarios. In our case, this might entail conducting similar investigations with more realistic office tasks and ultimately apply our findings to commercial self-driving cars. There are also attempts to improve real-time cognitive state assessment with neurophysiological measures and integrate respective sensors in the car interior [39]. This research fuels the design of adaptive in-car interfaces that are able to tailor interior configurations to the individual passenger based on data recorded during the car ride. For working scenarios, EEG-based quantification of workload, as demonstrated in our study, may become a valuable technique. The idea of a system adapting to the individual based on the current cognitive workload requires a robust estimation and user model that collects relevant information about the user during runtime. Our study provides an initial evidence that EEG is a valid approach to disentangle context-based influences on user’s mental workload capacities. We will therefore promote future research on real-time cognitive state assessment based on EEG for adaptive lighting conditions as well as visual and auditory information displays.
"
Sensing Posture-Aware Pen+Touch Interaction on Tablets,https://dl.acm.org/doi/10.1145/3290605.3300285,"
Many status-quo interfaces for tablets with pen + touch input capabilities force users to reach for device-centric UI widgets at fixed locations, rather than sensing and adapting to the user-centric posture. To address this problem, we propose sensing techniques that transition between various nuances of mobile and stationary use via postural awareness. These postural nuances include shifting hand grips, varying screen angle and orientation, planting the palm while writing or sketching, and detecting what direction the hands approach from. To achieve this, our system combines three sensing modalities: 1) raw capacitance touchscreen images, 2) inertial motion, and 3) electric field sensors around the screen bezel for grasp and hand proximity detection. We show how these sensors enable posture-aware pen+touch techniques that adapt interaction and morph user interface elements to suit fine-grained contexts of body-, arm-, hand-, and grip-centric frames of reference. Overall, our work demonstrates how posture awareness can adapt interaction, and morph user interface elements, to suit the fine-grained context of use for pen and touch interaction on tablets. Posture-awareness includes nuances of grip, the angle of the tablet, the presence and orientation of the palm on the screen while writing or sketching, and which direction the user reaches onto the screen from during touch. Taken together, these contributions show how a few simple sensors can enable tablets to more effectively support both ‘mobile’ and ‘stationary’ use—and the many gradations in-between.

"
"VirtualBricks: Exploring a Scalable, Modular Toolkit for Enabling Physical Manipulation in VR",https://dl.acm.org/doi/10.1145/3290605.3300286,"
Often Virtual Reality (VR) experiences are limited by the design of standard controllers. This work aims to liberate a VR developer from these limitations in the physical realm to provide an expressive match to the limitless possibilities in the virtual realm. VirtualBricks is a LEGO based toolkit that enables construction of a variety of physical-manipulation enabled controllers for VR, by offering a set of feature bricks that emulate as well as extend the capabilities of default controllers. Based on the LEGO platform, the toolkit provides a modular, scalable solution for enabling passive haptics in VR. We demonstrate the versatility of our designs through a rich set of applications including re-implementations of artifacts from recent research. We share a VR Integration package for integration with Unity VR IDE, the CAD models for the feature bricks, for easy deployment of VirtualBricks within the community. While Virtual Reality is a platform for endless explorations in the digital world, its developer and user have been limited by one-size-fts-all proprietary controllers. With VirtualBricks, we attempt to empower the VR developer with a toolkit that enables the creation of custom controllers to match the different application scenarios. With our LEGO-based feature bricks that provide a range of functionalities, a VR Integration software package that facilitates easy integration in Unity, we open up the design space of interactive controllers that allow a range of manipulation interactions. We demonstrate the inherent modularity, and scalability of our design through a set of implemented applications. We hope that the creative community of VR designers and developers will adopt and extend this toolkit to their heart’s desire.

"
Painting with CATS: Camera-Aided Texture Synthesis,https://dl.acm.org/doi/10.1145/3290605.3300287,"
We present CATS, a digital painting system that synthesizes textures from live video in real-time, short-cutting the typical brush- and texture- gathering workflow. Through the use of boundary-aware texture synthesis, CATS produces strokes that are non-repeating and blend smoothly with each other. This allows CATS to produce paintings that would be difficult to create with traditional art supplies or existing software. We evaluated the effectiveness of CATS by asking artists to integrate the tool into their creative practice for two weeks; their paintings and feedback demonstrate that CATS is an expressive tool which can be used to create richly textured paintings. We have presented CATS, a novel digital painting system that synthesizes texture from live video in real time. By capturing textures from live video, CATS accelerates the texture creation workflow and encourages an exploratory approach to painting. User study results demonstrate that artists could produce texture rich paintings that could not easily be made through purely digital or traditional means. This opens an exciting new avenue for using computation to facilitate the creative process and produce new materials for art creation.

"
A Comparison of Notification Techniques for Out-of-View Objects in Full-Coverage Displays,https://dl.acm.org/doi/10.1145/3290605.3300288,"
Full-coverage displays can place visual content anywhere on the interior surfaces of a room (e.g., a weather display near the coat stand). In these settings, digital artefacts can be located behind the user and out of their field of view - meaning that it can be difficult to notify the user when these artefacts need attention. Although much research has been carried out on notification, little is known about how best to direct people to the necessary location in room environments. We designed five diverse attention-guiding techniques for full-coverage display rooms, and evaluated them in a study where participants completed search tasks guided by the different techniques. Our study provides new results about notification in full-coverage displays: we showed benefits of persistent visualisations that could be followed all the way to the target and that indicate distance-to-target. Our findings provide useful information for improving the usability of interactive full-coverage environments. Many of the digital objects in a full-coverage display environment may be behind the user, and therefore out of view. When these objects need attention, it can be difficult for the system to notify the user and lead them to the correct object at the back of the environment. Although much research has been done on notification in general, little is known about how best to direct people to out-of-view locations in FCDs. To address this problem, we designed four attention-guiding techniques that take different approaches on several different design dimensions. We evaluated these techniques in a controlled study that simulated a realistic attention-demanding task in an FCD. The study showed that the Wedge technique performed best, but also showed how the fundamental differences between the guiding techniques led to several performance differences. Our findings provide designers with useful new information that can improve the effectiveness and usability of interactive full-coverage environments.

"
"An Evaluation of Radar Metaphors for Providing Directional Stimuli Using Non-Verbal
                     Sound",https://dl.acm.org/doi/10.1145/3290605.3300289,"
We compared four audio-based radar metaphors for providing directional stimuli to users of AR headsets. The metaphors are clock face, compass, white noise, and scale. Each metaphor, or method, signals the movement of a virtual arm in a radar sweep. In a user study, statistically significant differences were observed for accuracy and response time. Beat-based methods (clock face, compass) elicited responses biased to the left of the stimulus location, and non-beat-based methods (white noise, scale) produced responses biased to the right of the stimulus location. The beat methods were more accurate than the non-beat methods. However, the non-beat methods elicited quicker responses. We also discuss how response accuracy varies along the radar sweep between methods. These observations contribute design insights for non-verbal, non-visual directional prompting. On average, participants were able to perceive location stimulus to within 13° across all methods. With the compass method, this improved to below 10°. Beat-based delineations had better accuracy when identifying location stimulus. Non-beat-based delineations (using constant tones) elicited quicker responses from participants. There was a disparity between user preference and accuracy for beat-based methods, with participants preferring fewer delineations in the radar sweep. If accuracy is flexible, designers of radar metaphors should consider exploring non-beat-based methods of delineation to reduce cognitive load. There was variation between participants in both performance and preference for each method. This suggests providing options to personalize a radar sweep system to the method users are most comfortable with would be helpful, but (without further work) at the expense of accuracy in some cases. Participants with musical training performed better in all areas than those without. They were quicker and more accurate. This suggests that others who have enhanced auditory skills may also perform well, and this gives encouragement to the idea that visuallyimpaired users could find these metaphors useful. Investigation into the effect of multiple location stimuli on a single radar arm is the next step towards creating a metaphor that can provide a bigger picture of the user’s surroundings rather than a single location stimulus. It is possible that a method that works well for a single location stimulus breaks down when it is used with multiple stimuli. Similarly, methods that performed less well with a single stimulus may better support multiple prompts. The stimulus location tasks in this study were all conducted in isolation, further work could also include simultaneous tasks to assess the effects of additional cognitive load on accuracy of sweep methods. There is also the possibility to investigate how well a user can identify and distinguish different types of location stimuli, for example, using a different timbre for each location stimulus. Integration of effects, such as low-pass filters, could help encode extra information, such as proximity to objects. This has been used before effectively in other location stimuli systems [7]. Adjusting cutoff/resonance of the radar delineation sound could also be useful in conveying information such as proximity to walls. Further work with different participant groups will also help provide insight into the preferences of each groups, for example, with children or visually-impaired people. This study was a lab-based study carried out on a laptop. Another logical extension to the work is to implement the appropriate radar methods on an AR headset and evaluate with live spatial mapping data.
"
Integrated Workflows: Generating Feedback Between Digital and Physical Realms,https://dl.acm.org/doi/10.1145/3290605.3300290,"
As design thinking shifted away from conventional methods with the rapid adoption of computer-aided design and fabrication technologies, architects have been seeking ways to initiate a comprehensive dialogue between the virtual and the material realms. Current methodologies do not offer embodied workflows that utilize the feedback obtained through a subsequent transition process between physical and digital design. Therefore, narrowing the separation between these two platforms remains as a research problem. This literature review elaborates the divide between physical and digital design, testing and manufacturing techniques in the morphological process of architectural form. We first review the digital transformation in the architectural design discourse. Then, we proceed by introducing a variety of methods that are integrating digital and physical workflows and suggesting an alternative approach. Our work unveils that there is a need for empirical research with a focus on integrated approaches to create intuitively embodied experiences for architectural designers. We have provided an overview of the widening divide between the digital and physical manipulations in the architectural design process, starting from the 2D drafting era to the modern digital design and fabrication techniques. We have introduced critical ideas towards the rapid digitalization and discussed the significance of physical model making for architectural designers. Our review unveils that, despite the growing number of computational methods, materialization and physical input of the designer remain as a fundamental accelerator for the evolutionary process of architectural form. The sources we have included predominantly follow a linear and non-iterative transition between physical and digital workflows. Thus, there is an open area that requires further experimentation and testing with fluid feedbackbased integrated workflows. As future work, we will be further exploring the physical interactions with scanned 3D models to create manipulable 3D settings for architectural designers based on the theoretical background presented in this paper and the transitional method we have suggested in the end. To strengthen the feasibility of our transitional method we will be implementing efficient scanning settings [93]. We also plan to enhance the algorithmic transition phase with accurate translation approaches [83,105]. Findings of these empirical studies can assist us for designing an iterative loop process between the physical and digital realities. Once we are able to reduce the contradiction between virtual and real-world interaction, we will test our integrated method with architectural designers to determine its impact starting from the early creative stages of the design and form-finding processes. By improving our transitional approach, we will be aiming to replace the distorted creative thinking with a multidimensional comprehension of digital design and fabrication tools.
"
