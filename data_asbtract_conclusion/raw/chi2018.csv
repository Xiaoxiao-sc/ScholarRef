title,link,abstract
Breaking! A Typology of Security and Privacy News and How It's Shared,https://dl.acm.org/authorize?N656579,"
News coverage of security and privacy (S&P) events is pervasive and may affect the salience of S&P threats to the public. To better understand this coverage and its effects, we asked: What types of S&P news come into people's awareness? How do people hear about and share this news? Over two years, we recruited 1999 participants to fill out a survey on emergent S&P news events. We identified four types of S&P news: financial data breaches, corporate personal data breaches, high sensitivity systems breaches, and politicized / activist cybersecurity. These event types strongly correlated with how people shared S&P news-e.g., financial data breaches were shared most (42%), while politicized / activist cybersecurity events were shared least (21%). Furthermore, participants' age, gender and security behavioral intention strongly correlated with how they heard about and shared S&P news-e.g., males more often felt a personal responsibility to share, and older people were less likely to hear about S&P news through conversation. News media coverage of cybersecurity and privacy is significant and growing, and this coverage likely affects end-users’ perceptions and behaviors. To better understand what types of security and privacy news events are salient to people, we collected and analyzed 1999 survey responses for 104 distinct S&P news events over the course of two years. Through this work, we made two key contributions. First, we proposed a typology of security and privacy news events. Second, we presented a model of how such news events reach people, are shared by people, and how those factors correlate with people’s age, gender, security behavioral intention, and the type of news event. These contributions should help HCI researchers and practitioners design solutions that address problems that everyday people find especially salient and important. For example, in the creation of systems that allow for stewardship, systems that personalize S&P recommendations to each individual based on their unique information diets, and systems that promote individual agency.
"
Deep Thermal Imaging: Proximate Material Type Recognition in the Wild through Deep Learning of Spatial Surface Temperature Patterns,https://dl.acm.org/authorize?N656570,"
We introduce Deep Thermal Imaging, a new approach for close-range automatic recognition of materials to enhance the understanding of people and ubiquitous technologies of their proximal environment. Our approach uses a low-cost mobile thermal camera integrated into a smartphone to capture thermal textures. A deep neural network classifies these textures into material types. This approach works effectively without the need for ambient light sources or direct contact with materials. Furthermore, the use of a deep learning network removes the need to handcraft the set of features for different materials. We evaluated the performance of the system by training it to recognize 32 material types in both indoor and outdoor environments. Our approach produced recognition accuracies above 98% in 14,860 images of 15 indoor materials and above 89% in 26,584 images of 17 outdoor materials. We conclude by discussing its potentials for real-time use in HCI applications and future directions. Deep Thermal Imaging is a new technique to recognize materials in proximity which is based on the use of mobile thermal imaging and deep learning. To the best of our knowledge, this paper provides the first demonstration that spatial thermal patterns of materials can be used to enable automatic material recognition tasks with good performances (above 89% mean accuracy from 15 indoor and 17 outdoor materials). We leveraged a mobile, low-frame rate thermal camera to build a large dataset (41,444 images) of variant thermal textures of materials. Potential use cases of the systems compared with the state of the art are also discussed highlighting the potential of the system.

"
All Work and No Play?,https://dl.acm.org/authorize?N656571,"
Many conversational agents (CAs) are developed to answer users' questions in a specialized domain. In everyday use of CAs, user experience may extend beyond satisfying information needs to the enjoyment of conversations with CAs, some of which represent playful interactions. By studying a field deployment of a Human Resource chatbot, we report on users' interest areas in conversational interactions to inform the development of CAs. Through the lens of statistical modeling, we also highlight rich signals in conversational interactions for inferring user satisfaction with the instrumental usage and playful interactions with the agent. These signals can be utilized to develop agents that adapt functionality and interaction styles. By contrasting these signals, we shed light on the varying functions of conversational interactions. We discuss design implications for CAs, and directions for developing adaptive agents based on users' conversational behaviors. By studying log data from a field deployment of a questionand-answer conversational agent, we characterize the rich forms of conversational interactions users had with the agent. The main areas of conversations include feedback-giving, playful chit-chat, system inquiry, and habitual communicative utterances. Through the lens of statistical modeling, we highlight the rich signals in conversational interactions for inferring user satisfaction, which can be utilized to develop agents that can adapt algorithmic performances and interaction styles. The results also provide nuanced understanding on the underlying functions of conversational behaviors with QA agents and their deviations from human conversations. Our findings may inform designs of CAs and contribute to the emerging fields of conversational UX, conversational IR and adaptive agents.
"
Designing the Desirable Smart Home: A Study of Household Experiences and Energy Consumption Impacts,https://dl.acm.org/authorize?N656572,"
Research has shown that desirable designs shape the use and experiences people have when interacting with technology. Nevertheless, how desirability influences energy consumption is often overlooked, particularly in HCI studies evaluating the sustainability benefits of smart home technology. In this paper, we present a qualitative study with 23 Australian households who reflect on their experiences of living with smart home devices. Drawing on Nelson and Stolterman's concept of desiderata we develop a typology of householders' desires for the smart home and their energy implications. We structure these desires as three smart home personas: the helper, optimiser and hedonist, which align with desiderata's three approaches to desire (reason, ethics and aesthetics). We use these insights to discuss how desirability can be used within HCI for steering design of the smart home towards sustainability. In this paper, we have analysed the different desires embedded in smart home technologies as interpreted by 23 participating households. Drawing on Nelson and Stolterman’s concept of desiderata we developed a typology of different desires for the smart home. We structured these as three smart home personas: the helper, optimiser and hedonist, which align with desiderata’s three approaches to desire (reason, ethics and aesthetics). What our findings show is that different desires embedded in the smart home also shape household expectations and practices to impact energy consumption in different ways. Most interestingly, these desires both compliment and contrast each other, highlighting an energy paradox in the desirable smart home. While smart home technologies afford households aesthetically pleasing experiences that reflect a modern lifestyle, they can also undermine the desire to live sustainability. Based on these findings, we conclude that if designers of smart home technology seek to aspire towards sustainable change, we need to approach the desirable smart home more holistically through concepts like desiderata. Towards this agenda, we suggest that HCI practitioners and researchers engage with aspirations to both challenge and enhance desirable everyday experiences that also promote more sustainable energy use.
"
The Making of Performativity in Designing [with] Smart Material Composites,https://dl.acm.org/authorize?N656573,"
As the material becomes active in disclosing the fullness of its capabilities, the boundaries between human and nonhuman performances are destabilized in productive practices that take their departure from materials. This paper illuminates the embodied crafting of action possibilities in material-driven design (MDD) practices with electroluminescent materials. The paper describes and discusses aspects of the making process of electroluminescent materials in which matter, structure, form, and computation are manipulated to deliberately disrupt the affordance of the material, with the goal to explore unanticipated action possibilities and materialize the performative qualities of the sample. In light of this account, the paper concludes by urging the HCI community to performatively rupture the material, so to be able to act upon it as if it was always unfinished or underdeveloped. This, it is shown, can help open up the design space of smart material composites and reveal their latent affordances. In this paper, we have presented and discussed a number of material-driven design (MDD) explorations which take their departure from an underdeveloped smart material composite, specifically an electroluminescent material composite. These explorations are focused on the creation of electroluminescent material samples with novel action possibilities and are facilitated by the designer’s skillful engagement with the electroluminescent material. In describing the making processes, we have articulated how bodily manipulations of matter, structure, form, and computation can facilitate the emergence of certain performances. Examining the explorations from the perspective of what we refer to as the ’making of performativity’ in MDD practices, the paper introduces the idea of disruption of affordance as a design strategy for working with smart material composites. We conclude by promoting how such conceptual articulation of smart materials as underdeveloped composites may unpack new ways of bringing about the performative potential of a smart material and revealing its latent affordances. In the MDD approach, as proposed, materials are understood and acted upon as always unfinished or underdeveloped. This offers HCI design practice with smart material composites a better leveraging of the dynamic properties of such materials, and potentially more dynamic responses and performances by the products in which these materials may be infused.

"
Patterns for How Users Overcome Obstacles in Voice User Interfaces,https://dl.acm.org/authorize?N656574,"
Voice User Interfaces (VUIs) are growing in popularity. However, even the most current VUIs regularly cause frustration for their users. Very few studies exist on what people do to overcome VUI problems they encounter, or how VUIs can be designed to aid people when these problems occur. In this paper, we analyze empirical data on how users (n=12) interact with our VUI calendar system, DiscoverCal, over three sessions. In particular, we identify the main obstacle categories and types of tactics our participants employ to overcome them. We analyzed the patterns of how different tactics are used in each obstacle category. We found that while NLP Error obstacles occurred the most, other obstacles are more likely to frustrate or confuse the user. We also found patterns that suggest participants were more likely to employ a ""guessing"" approach rather than rely on visual aids or knowledge recall. This paper identified 4 major categories of obstacles people face when using an unfamiliar VUI and 10 types of tactics they use to overcome them. We analyzed the patterns in which participants employed different tactics for each obstacle category and identified the transition patterns between tactics. In our results, the opposite tactics of Simplification and Use More Info are the most frequently used tactics, after Hyperarticulation. We found that although NLP Error obstacles are the most common, the other obstacles caused more frustration and confusion to our participants. This indicates that improving VUI’s UX requires further research in both NLP and in interaction design (e.g., feedback). Regarding tactic patterns, our participants, even with their technical backgrounds, relied more on guessing and exploration than knowledge recall or visual aids. This highlights the need for further research on supporting user-directed exploration and learning, in addition to standard tutorial and menus in VUIs. Our study is limited to a small size of 36 sessions with 12 participants. Future work can be extended to analyze a larger sample and different demographics; specifically examining the obstacles and tactic patterns of non-technology comfortable users. DiscoverCal is also a VUI with a visual display, certain tactics such as Rely on GUI will not apply to all VUIs. Our study analyzes data retrieved from user interactions with a single context VUI (calendar management). Our categories can be used to evaluate more advanced, multi-context VUIs, like Alexa and Google Assistant. These categories can also be used by VUI designers to evaluate their own VUI systems.
"
ThinkActive: Designing for Pseudonymous Activity Tracking in the Classroom,https://dl.acm.org/authorize?N656585,"
We report on the design of ThinkActive - a system to encourage primary aged school children to reflect on their own personal activity data in the classroom. We deployed the system with a cohort of 30 school children, over a six-week period, in partnership with an English Premier League Football club's health and nutrition programme. The system utilizes inexpensive activity trackers and pseudonymous avatars to promote reflection with personal data using an in-situ display within the classroom. Our design explores pseudonymity as an approach to managing privacy and personal data within a public setting. We report on the motivations, challenges, and opportunities for students, teachers, and third-party providers to engage in the collection and sharing of activity data with primary school children. This paper presents the design of ThinkActive - a system to support primary school students in reflecting upon their personal activity data within the classroom using pseudonymous avatars. Our findings from a six week engagement brings to the fore, issues around handling sensitive personal data, engaging and designing for multiple stakeholder motivations, and promoting sociability and interaction between students around their personal data in the classroom. We hope to inspire work beyond this domain and into other complex and sensitive social contexts such as workplaces, shared housing, and social care environments.

"
Gender Recognition or Gender Reductionism?: The Social Implications of Embedded Gender Recognition Systems,https://dl.acm.org/authorize?N656586,"
Automatic Gender Recognition (AGR) refers to various computational methods that aim to identify an individual's gender by extracting and analyzing features from images, video, and/or audio. Applications of AGR are increasingly being explored in domains such as security, marketing, and social robotics. However, little is known about stakeholders' perceptions and attitudes towards AGR and how this technology might disproportionately affect vulnerable communities. To begin to address these gaps, we interviewed 13 transgender individuals, including three transgender technology designers, about their perceptions and attitudes towards AGR. We found that transgender individuals have overwhelmingly negative attitudes towards AGR and fundamentally question whether it can accurately recognize such a subjective aspect of their identity. They raised concerns about privacy and potential harms that can result from being incorrectly gendered, or misgendered, by technology. We present a series of recommendations on how to accommodate gender diversity when designing new digital systems. With emerging technologies attempting to incorporate complex human attributes, such as gender, it is important to take into account the perspectives of diverse populations who might be directly or indirectly impacted. We studied the perceptions and attitudes of transgender individuals towards automatic gender recognition (AGR), a technology that aims to classify a person’s gender based on their physical characteristics. We found that participants had overwhelmingly negative attitudes towards AGR and questioned if it can offer any beneficial applications to end users. They also expressed doubt about whether AGR can accurately identify gender and described the harm of being misgendered by it. Finally, participants expressed serious concerns about threats that it can pose to their autonomy and privacy. We presented several recommendations for incorporating gender in system design, including informing users if their gender information would be used, giving them the option to opt out and allowing them to communicate their own gender identity to systems. With respect to AGR, we are not necessarily arguing for the elimination of gender recognition from technology, but a careful consideration of the implications of incorporating it. In totality, these recommendations point towards an approach to gender that is more inclusive, collaborative and sensitive to human autonomy and choice.

"
CivilServant: Community-Led Experiments in Platform Governance,https://dl.acm.org/authorize?N656587,"
As online platforms monitor and intervene in the daily lives of billions of people, platforms are being used to govern enduring social problems. Field experiments could inform wise uses of this power if tensions between democratic values and experimentation could be resolved. In this paper, we introduce CivilServant, a novel experimentation infrastructure that online communities and their moderators use to evaluate policies and replicate each others' findings. We situate CivilServant in the political history of policy experiments and present design considerations for community participation, ethics, and replication. Based on two case studies of community-led experiments and public debriefings on the reddit platform, we share findings on community deliberation about experiment results. We also report on uses of evidence, finding that experiments informed moderator practices, community policies, and replications by communities and platforms. We discuss the implications of these findings for evaluating platform governance in an open, democratic, experimenting society.
"
M-Kulinda: Using a Sensor-Based Technology Probe to Explore Domestic Security in Rural Kenya,https://dl.acm.org/authorize?N656588,"
In rural Kenyan households, property theft is a persistent problem. To explore how Information and Communication Technologies (ICTs) may be used to address this problem we designed and deployed ""M-Kulinda""-a sensor-based technology probe. We used interview, observation, diary, and data logging methods to understand 20 households' experiences using the system. Our findings suggest that a probe's approach is useful in this context, more specifically we found that participants used our system in different ways to address their specific needs (e.g., monitoring poultry, electronics, and their family members). We also observed changes in our participants' understanding of sensors; M-Kulinda prompted them to reflect on other areas where sensors could be used in their households. We present design implications based on these findings, and offer new perspectives on the role of technology in deterring crime. M-Kulinda was successfully used to monitor participants’ homes. The use of M-Kulinda in rural Kenya opened new opportunities for participants to realize how sensor-based technologies can be used in their households. These opportunities deepen the HCI community’s understanding of the use of sensor-based technologies for home protection in developing countries. Our findings also suggest major differences from prior work [17]. We attribute this to geographical and cultural difference between rural Kenya and the U.S., and to differences in users’ understanding of the technology. In developing countries like rural Kenya, crime detection systems can be used to strengthen neighborhood cohesion. Lastly, our findings provide direction for future research on sensor-based technologies in developing countries.
"
Nonvisual Interaction Techniques at the Keyboard Surface,https://dl.acm.org/authorize?N656589,"
Web user interfaces today leverage many common GUI design patterns, including navigation bars and menus (hierarchical structure), tabular content presentation, and scrolling. These visual-spatial cues enhance the interaction experience of sighted users. However, the linear nature of screen translation tools currently available to blind users make it difficult to understand or navigate these structures. We introduce Spatial Region Interaction Techniques (SPRITEs) for nonvisual access: a novel method for navigating two-dimensional structures using the keyboard surface. SPRITEs 1) preserve spatial layout, 2) enable bimanual interaction, and 3) improve the end user experience. We used a series of design probes to explore different methods for keyboard surface interaction. Our evaluation of SPRITEs shows that three times as many participants were able to complete spatial tasks with SPRITEs than with their preferred current technology. Loss of spatial layout is one of the biggest challenges of nonvisual accessible technology. The impediment of not being able to understand layout deters visually impaired individuals from being fully empowered in today's digital world. In this paper, we present SPRITEs, an inexpensive paradigm and a suite of interaction techniques that can provide nonvisual access to graphical interfaces while preserving spatial layout and improve the end user experience. Our study shows that for spatial tasks SPRITEs task completion rates are triple that of participants’ preferred access technology.

"
Admixed Portrait: Design to Understand Facebook Portrayals in New Parenthood,https://dl.acm.org/authorize?N656580,"
We report on a design-led study of the photographic representation of self and family on Facebook during and after becoming parents for the first time. Our experience-centered, research-through-design study engaged eight participants across five UK homes, in a month-long deployment of a prototype technology -- a design research artifact, Admixed Portrait, that served to prompt participant reflection on first-time parenthood. In addition to pre- and post-deployment interviews, participants kept diaries capturing personal reflections during the deployment, on daily social media use and interactions with Admixed. Our qualitative insights on social media representations of transitional experience and identity for new parents, reveal how their online 'photowork' related to self-expression and social functioning. We contribute design considerations for developing tools to support photographic expression in social media use, and methodological insights about design-led inquiry for understanding transitional experiences.
"
MABLE: Mediating Young Children's Smart Media Usage with Augmented Reality,https://dl.acm.org/authorize?N656581,"
There has been a growing concern over the huge increase in use of smart media by young children. This study explores the possibility of using augmented-reality(AR) for regulat-ing preschoolers' media usage behavior. With MABLE (mobile application for behavioral learning and education), parents can provide AR-assisted feedback by changing facial expressions and sound effects. When overlaying a smart media, which has MABLE running, in front of a QR marker on a puppet, a facial expression is displayed on top of the puppet's face. A two-week long experiment with 36 parent-child pairs showed that compared to using just the puppet, using MABLE showed higher amount of engage-ment among preschoolers. For the effectiveness of parental mediation in terms of self-control, our data showed mixed results. MABLE had positive effects in that the amount of rule-compliance increased and problematic behaviors de-creased, whereas the level of behavioral dependency on smart media was not influenced. The result of our study shows the possibility of utilizing AR technology for parental mediation of smartphone usage. When parents used MABLE, an AR system, in the SUM program, their children displayed higher level of engagement compared to the non-AR program. Furthermore, parents reported higher mediational effectiveness using MABLE compared to the non-AR program, in terms of the level of rulecompliance and amount of problematic behavior. Although it would be wonderful if parents can be loving, yet firm in teaching children desired developmental behaviors, the task is quite difficult. Our study showed potential in aiding parents with such a task using AR. Therefore, despite the aforementioned limitations of MABLE, the current study is meaningful in that it explored the potential role of AR in facilitating positive social and cognitive developmental behavior of preschoolers. For future work, we hope to extend the current work by exploring the possibilities of using AR in additional behavioral contexts.

"
How Relevant are Incidental Power Poses for HCI?,https://dl.acm.org/authorize?N656582,"
The concept of power pose originates from a Psychology study from 2010 which suggested that holding an expansive pose can change hormone levels and increase risk-taking behavior. Follow-up experiments suggested that expansive poses incidentally imposed by the design of an environment lead to more dishonest behaviors. While multiple replication attempts of the 2010 study failed, the follow-up experiments on incidental postures have so far not been replicated. As UI design in HCI can incidentally lead to expansive body postures, we attempted two conceptual replications: we first asked 44 participants to tap areas on a wall-sized display and measured their self-reported sense of power; we then asked 80 participants to play a game on a large touch-screen and measured risk-taking. Based on Bayesian analyses we find that incidental power poses had little to no effect on our measures but could cause physical discomfort. We conclude by discussing our findings in the context of theory-driven research in HCI. We investigated whether incidental postures, in particular constrictive and expansive postures, influence how users behave in human-computer interaction. The literature raised the expectation that such postures might set about cognitive and physiological reactions, most famously from findings by Carney et al. [14] as well as Yap et al. [65]. While the findings from Carney et al. on explicitly elicited power poses did not hold up to replications, the experiments by Yap et al. had so far not been replicated. We reported findings from two experiments which conceptually replicated experiments on incidental power poses in an HCI context. We observed an at best small effect for felt power and an at best negligible effect for a behavioral measure for risk-taking. Most surprisingly, an exploratory analysis suggested that an interaction with a personality trait, impulsiveness, might reverse the hypothesized effect for posture manipulations. However, replications controlling for this interaction are needed to determine if this interaction reliably replicates and thus poses a relevant design consideration for HCI. Overall we conclude that incidental power poses are unlikely to be relevant for the design of human-computer interfaces and that factors such as comfort play a much more important role.

"
Designing and Evaluating mHealth Interventions for Vulnerable Populations: A Systematic Review,https://dl.acm.org/authorize?N656583,"
Diverse disciplines, including Human-Computer Interaction have explored how mobile health (mHealth) applications can transform healthcare and health promotion. Increasingly, research has explored how mHealth tools can promote healthy behaviors within vulnerable populations-groups that disproportionately experience barriers to wellness. We conducted a systematic review of 83 papers from diverse disciplines to characterize the design and impact of mHealth tools in low-socioeconomic (low-SES) and racial/ethnic minority individuals. Our findings highlight that the diversity within low-SES and racial/ethnic minority groups was not reflected in the populations studied. Most studies focused on improving the health of individuals, often neglecting factors at the community and society levels that influence health disparities. Moreover, few improvements in health outcomes were demonstrated. We further discuss factors that acted as barriers and facilitators of mHealth intervention adoption. Our findings highlight trends that can drive critically needed digital health innovations for vulnerable populations. This systematic review reports on 83 papers focused on mHealth interventions in racial/ethnic minority and lowsocioeconomic groups. Our findings reveal trends that indicate gaps in the current literature, including a need for more: in-depth formative studies, reporting of population characteristics, research on diverse subgroups, evaluation of user engagement strategies (e.g., personalization and gamification), and design and evaluation of community-level, technology-based interventions.

"
The Illusion of Control: Placebo Effects of Control Settings,https://dl.acm.org/authorize?N656584,"
Algorithmic prioritization is a growing focus for social media users. Control settings are one way for users to adjust the prioritization of their news feeds, but they prioritize feed content in a way that can be difficult to judge objectively. In this work, we study how users engage with difficult-to-validate controls. Via two paired studies using an experimental system -- one interview and one online study -- we found that control settings functioned as placebos. Viewers felt more satisfied with their feed when controls were present, whether they worked or not. We also examine how people engage in sensemaking around control settings, finding that users often take responsibility for violated expectations -- for both real and randomly functioning controls. Finally, we studied how users controlled their social media feeds in the wild. The use of existing social media controls had little impact on user's satisfaction with the feed; instead, users often turned to improvised solutions, like scrolling quickly, to see what they want. In this work, we uncovered a placebo effect for control settings on social media. While this placebo effect might suggest adding non-functional controls – or even just a quick and dirty implementation of the controls eventually desired – to increase user satisfaction, we argue here that we must look deeper. Non-functional controls might suffice if social media were used solely for entertainment (as was likely the case for most of our participants). However, research has shown that social media has far greater impact, e.g., on organizing social movements [59] and news access [52]. Reliance on a platform such as Twitter in the context of a violent political conflict might bring high stakes to the questions of reliability and trustworthiness of control settings or of the algorithm itself. The fact that, in our work, users encountered violated expectations for both the real and the random controls suggests a potential for breakdown of trust even with the real controls. Indeed, Kizilcec has shown that when users’ expectations are violated, they trust systems less — unless provided some transparency [29]. While our study did not explicitly measure trust, it is an important factor when considering the impact of such a placebo effect. It is already well-established that users default to considering automated systems trustworthy [16] and may be liable to overtrusting systems [39, 40, 49]. For controls in particular, the likelihood that users accept defaults has potential negative outcomes, including forming inaccurate representations of important relationships or world events that, when acted upon, lead to misunderstanding, strained relationships, social alienation, or worse [17, 51]. With social media playing a growing role in civic and political life, surely it will become ever more important to establish not only whether these platforms are delivering what they promise, as in the growing area of work on algorithmic accountability, but how they establish trustworthiness for users. Relatedly, explorations of the ethics of misrepresentative control settings might fit well within emerging conversations about the ethics of algorithmic systems [5, 11, 13, 44].

"
Easy Return: An App for Indoor Backtracking Assistance,https://dl.acm.org/authorize?N656595,"
We present a system that, implemented as an iPhone app controllable from an Apple Watch, can help a blind person backtrack a route taken in a building. This system requires no maps of the building or environment modifications. While traversing a path from a starting location to a destination, the system builds and records a path representation in terms of a sequence of turns and of step counts between turns. If the user wants to backtrack the same path, the system can provide assistance by tracking the user's location in the recorded path, and producing directional information in speech form about the next turns and step counts to follow. The system was tested with six blind participants in a controlled indoor experiment. We presented a novel system that can assist a blind person attempting to backtrack a path taken in a building. Our easy return system was implemented as an iPhone app, controlled by an Apple Watch. Users don’t need to interact with the iPhone while walking (which could be problematic when handling a long cane or a guide dog), but can keep the phone safely in their pocket. We tested our prototype system in a controlled environment with six blind participants, who walked along eight indoor paths of increasing difficulty with a sighted guide, then, for each path, attempted to return to the starting point, either by themselves, or with assistance from our backtracking system. Use of our system was shown to increase the rate of complete return path traversal, but only for the more difficult routes (containing three or four turns). All of our participants except for one found great potential in the easy return concept, and were enthusiastic about the user interface designed around the Apple Watch.
"
Point-and-Shake: Selecting from Levitating Object Displays,https://dl.acm.org/authorize?N656596,"
Acoustic levitation enables a radical new type of human-computer interface composed of small levitating objects. For the first time, we investigate the selection of such objects, an important part of interaction with a levitating object display. We present Point-and-Shake, a mid-air pointing interaction for selecting levitating objects, with feedback given through object movement. We describe the implementation of this technique and present two user studies that evaluate it. The first study found that users could accurately (96%) and quickly (4.1s) select objects by pointing at them. The second study found that users were able to accurately (95%) and quickly (3s) select occluded objects. These results show that Point-and-Shake is an effective way of initiating interaction with levitating object displays. We presented Point-and-Shake, an interaction for selecting levitating objects, combining ray-cast pointing input with object movement as feedback. It was designed with the capabilities of acoustic levitation in mind and uses only the objects for feedback. We described two studies that evaluated the performance of Point-and-Shake. The first investigated the effect of virtual target size when selecting levitating objects, finding that users could quickly and accurately make selections when the target size was 10mm or greater. Users adapted to the increased difficulty of selecting very small targets (5mm) by moving closer to the levitating objects, which may affect acoustic levitation by reflecting sound back towards the objects. Point-and-Shake used a variation of the Lock Ray technique [5] to allow users to select occluded objects. We evaluated this aspect of the interaction in our second study, which also compared two methods for confirming selection. Users could successfully select occluded objects with both methods, but especially liked Quick Release because of the extra control it gave them over the interaction. Our studies looked at selection between two levitating objects, as the state-of-the-art in acoustic levitation does not yet allow more than two objects to be independently and reliably animated. Our results suggest Point-and-Shake would be effective with more objects, however. They show that our Shake feedback supported accurate ray aiming (Study 1) and depth cursor positioning (Study 2) and we expect this to also apply to more complex levitations. To conclude, we presented Point, a novel interaction for selecting levitating objects, and Shake, a feedback technique based on object movement. Together, these allow efficient selection of mid-air objects for the first time, paving the way to more complex interactions with levitating object displays and showing the potential for interaction with this exciting new technology.

"
When David Meets Goliath: Combining Smartwatches with a Large Vertical Display for Visual Data Exploration,https://dl.acm.org/authorize?N656597,"
We explore the combination of smartwatches and a large interactive display to support visual data analysis. These two extremes of interactive surfaces are increasingly popular, but feature different characteristics-display and input modalities, personal/public use, performance, and portability. In this paper, we first identify possible roles for both devices and the interplay between them through an example scenario. We then propose a conceptual framework to enable analysts to explore data items, track interaction histories, and alter visualization configurations through mechanisms using both devices in combination. We validate an implementation of our framework through a formative evaluation and a user study. The results show that this device combination, compared to just a large display, allows users to develop complex insights more fluidly by leveraging the roles of the two devices. Finally, we report on the interaction patterns and interplay between the devices for visual exploration as observed during our study. We presented a conceptual framework to support visual analysis tasks in a multi-device environment, combining two extremes of interactive surfaces: smartwatches and a large interactive display. In our framework, the devices fulfill different roles based on their strengths: the large display provides a multi-view interface, whereas the smartwatch augments and mediates the functionalities by serving as a personalized toolbox. In interplay with connective areas on the large display, the smartwatch supports exploration based on sets of both data items and visualization properties, which can be stored, manipulated, previewed, as well as applied permanently. We evaluated our prototype implementation to find interaction patterns with increased movements as well as evidence of the effectiveness of this specific device combination. With this work, we provide a starting point for this promising new class of multi-device environments, which we believe are strongly beneficial for visual analysis tasks and also beyond.

"
SteeringWheel: A Locality-Preserving Magnification Interface for Low Vision Web Browsing,https://dl.acm.org/authorize?N656598,"
Low-vision users struggle to browse the web with screen magnifiers. Firstly, magnifiers occlude significant portions of the webpage, thereby making it cumbersome to get the webpage overview and quickly locate the desired content. Further, magnification causes loss of spatial locality and visual cues that commonly define semantic relationships in the page; reconstructing semantic relationships exclusively from narrow views dramatically increases the cognitive burden on the users. Secondly, low-vision users have widely varying needs requiring a range of interface customizations for different page sections; dynamic customization in extant magnifiers is disruptive to users' browsing. We present SteeringWheel, a magnification interface that leverages content semantics to preserve local context. In combination with a physical dial, supporting simple rotate and press gestures, users can quickly navigate different webpage sections, easily locate desired content, get a quick overview, and seamlessly customize the interface. A user study with 15 low-vision participants showed that their web-browsing efficiency improved by at least 20 percent with SteeringWheel compared to extant screen magnifiers. This paper presents the design, implementation and a user study of SteeringWheel, a locality-preserving magnification interface, using an off-the-shelf physical dial for low-vision web browsing. Our study findings indicate that the constructive synergy between the dial, the LS hierarchy of the webpage, and locality preservation in SteeringWheel, makes for a better user experience compared to extant screen magnifiers. In future, we plan to continue work on the space reduction algorithm for small-screen devices such as smartphones and release the algorithm as a browser plug-in so that users can use it as a lightweight magnifier.

"
Extending Keyboard Shortcuts with Arm and Wrist Rotation Gestures,https://dl.acm.org/authorize?N656599,"
We propose and evaluate a novel interaction technique to enhance physical keyboard shortcuts with arm and wrist rotation gestures, performed during keypresses: rolling the wrist, rotating the arm/wrist, and lifting it. This extends the set of shortcuts from key combinations (e.g. ctrl + v) to combinations of key(s) and gesture (e.g. v + roll left) and enables continuous control. We implement this approach for isolated single keypresses, using inertial sensors of a smartwatch. We investigate key aspects in three studies: 1) rotation flexibility per keystroke finger, 2) rotation control, and 3) user-defined gesture shortcuts. As a use case, we employ our technique in a painting application and assess user experience. Overall, results show that arm and wrist rotations during keystrokes can be used for interaction, yet challenges remain for integration into practical applications. We discuss recommendations for applications and ideas for future research. We proposed and evaluated arm and wrist rotation gestures to extend keyboard interaction. These gestures are performed while pressing a key. They extend the set of possible shortcuts and enable continuous control along multiple axes. Thus, our concept increases expressiveness of keyboard interaction. We implemented this concept for isolated single keypresses, using smartwatch sensors, and investigated four key aspects: 1) rotation flexibility per keystroke finger, 2) rotation control, 3) user-defined gesture shortcuts, and 4) user experience. We found that users can control rotations with useful accuracy and ranges for interaction. However, we also revealed challenges: In particular, rotation shortcuts did not outperform normal ones in our case study and elicited assignments indicated that gesture mappings might not be obvious without supporting users in discovering and learning them. Nevertheless, overall we conclude that arm and wrist rotations during keystrokes can be used to extend keyboard interaction and shortcuts, and offer a promising direction for future work.

"
IntroAssist: A Tool to Support Writing Introductory Help Requests,https://dl.acm.org/authorize?N656590,"
Writing introductory help requests is a key part of develop-ing new professional connections, such as through email and other online messaging systems. This paper presents the design and an experimental evaluation of IntroAssist-a web-based tool that leverages cognitive apprenticeship in-structional methods to support writing introductory help requests through an expert-informed checklist, tagged peer examples, self-tagging, and suggested word limit. In a study of IntroAssist with novice entrepreneurs, we find that 1) expert raters consider help requests written with the tool as more effective, 2) participants are able to perform introduc-tory help seeking skills after the tool is removed, and 3) participants report being more likely to send help requests written with the tool. We present implications for the de-velopment of systems that support the initiation of profes-sional relationships. Based on a review of help seeking literature and our empirical studies, we argue that computer-mediated communication systems should be designed to support users beyond recommending new connections. They must also help users to communicate in the first place. IntroAssist provides an opportunity to facilitate the initiation of professional connections through written introductory help request by providing an expert-informed checklist, tagged peer examples, self-tagging, and suggested word limit. We find that expert raters consider help requests written with IntroAssist as more effective than those written without, participants are able to perform introductory help seeking skills after the tool is removed, and participants report being more likely to send help requests written with the tool. With greater skills and confidence in performing introductory help seeking, people can more effectively leverage the myriad connections Internet technologies have come to offer.
"
Surprise Me If You Can: Serendipity in Health Information,https://dl.acm.org/authorize?N656591,"
Our natural tendency to be curious is increasingly important now that we are exposed to vast amounts of information. We often cope with this overload by focusing on the familiar: information that matches our expectations. In this paper we present a framework for interactive serendipitous information discovery based on a computational model of surprise. This framework delivers information that users were not actively looking for, but which will be valuable to their unexpressed needs. We hypothesize that users will be surprised when presented with information that violates the expectations predicted by our model of them. This surprise model is balanced by a value component which ensures that the information is relevant to the user. Within this framework we have implemented two surprise models, one based on association mining and the other on topic modeling approaches. We evaluate these two models with thirty users in the context of online health news recommendation. Positive user feedback was obtained for both of the computational models of surprise compared to a baseline random method. This research contributes to the understanding of serendipity and how to ""engineer"" serendipity that is favored by users. This study presents a framework that models the concept of serendipity as a combination of surprise and value. The framework was implemented using two computational approaches to predicting user surprise, which were then evaluated in a user study. Our results show that the MI, approach based on topic co-occurrence outperformed the KL approach and our random baseline in predicting when users would rate a document as surprising and serendipitous. As to the broader impact, This work addresses a core problem of accuracy-oriented search and recommender systems. Serendipitous retrieval has the potential to transform the way digital systems deliver information by shifting from reinforcing similar information to facilitating unexpected discoveries. This will provide users with expanded access to information that is surprising, yet beneficial. This to a variety of domains that can benefit from such a model.

"
Awe the Audience: How the Narrative Trajectories Affect Audience Perception in Public Speaking,https://dl.acm.org/authorize?N656592,"
Telling a great story often involves a deliberate alteration of emotions. In this paper, we objectively measure and analyze the narrative trajectories of stories in public speaking and their impact on subjective ratings. We conduct the analysis using the transcripts of over 2000 TED talks and estimate potential audience response using over 5 million spontaneous annotations from the viewers. We use IBM Watson Tone Analyzer to extract sentence-wise emotion, language, and social scores. Our study indicates that it is possible to predict (with AUC as high as 0.88) the subjective ratings of the audience by analyzing the narrative trajectories. Additionally, we find that some trajectories (for example, a flat trajectory of joy) correlate well with some specific ratings (e.g. ""Longwinded') assigned by the viewers. Such an association could be useful in forecasting audience responses using objective analysis. In summary, we strive to analyze if the narrative trajectories exist in public speaking and if it has any impact on the audience ratings. Our analysis reveals the existence of several major patterns (clusters) in narrative trajectories. The clusters show statistically significant differences in the audience ratings. The relation of audience ratings with the narrative trajectories provides insights on the behavior of the audience. The results could motivate future research on determining the cause of such audience responses. Additionally, the narrative trajectories and the corresponding cluster analyses were computed objectively in an automated analysis technique. This kind of experiment is reproducible, scalable, and its generalization verifiable over different domains. Objective analysis also makes it possible to build computer algorithms that could automatically predict audience responses from analyzing the transcripts. This approach could potentially be useful in building automated systems to help people practice and prepare their own public speeches. To validate this claim, we attempted classification and regression tasks using features collected from narrative trajectories and off-the-shelf prediction techniques. Our results show that even these simple techniques could discriminate between highly rated and poorly rated TED talks with accuracy as high as 80% (AUC 0.88) which is much higher than random chance. Finally, it is likely that the speakers communicate additional information or inspire deeper connection to the audience through skillful prosody, facial expressions, and gestures. These parameters could impact the way the audience rates a speech. Our analyses and insights in this paper are limited to only the spoken sentences, and not the nonverbal features. It remains part of our future work.

"
Introducing Transient Gestures to Improve Pan and Zoom on Touch Surfaces,https://dl.acm.org/authorize?N656593,"
Despite the ubiquity of touch-based input and the availability of increasingly computationally powerful touchscreen devices, there has been comparatively little work on enhancing basic canonical gestures such as swipe-to-pan and pinch-to-zoom. In this paper, we introduce transient pan and zoom, i.e. pan and zoom manipulation gestures that temporarily alter the view and can be rapidly undone. Leveraging typical touchscreen support for additional contact points, we design our transient gestures such that they co-exist with traditional pan and zoom interaction. We show that our transient pan-and-zoom reduces repetition in multi-level navigation and facilitates rapid movement between document states. We conclude with a discussion of user feedback, and directions for future research. Standard pan-and-zoom techniques suffer from inefficiencies, and often require excessive repeated transitions when performing tasks that require constant transitions between multiple resolutions and locations in a zoomable interface. We introduce a transient technique that expands the design of standard pan-and-zoom interaction to include the fluid movement between states to reduce the need for repetitive zooming to revisit previous states. In an experiment with 18 participants, we demonstrate that our technique requires 57.6% fewer zoom actions than standard pan-and-zoom, for an overall 17.3% improvement in task completion times. Transient pan-and-zoom is comparable to double-tap in ease-of-use and performance, but is found to be more effective than double-tap and preferred by our participants for navigation tasks.

"
MirrorMirror: A Mobile Application to Improve Speechreading Acquisition,https://dl.acm.org/authorize?N656594,"
Many people around the world have difficulties in day-to-day conversation due to hearing loss. Hearing aids often fail to offer enough benefits and have low adoption rates. However, people with hearing loss find that speechreading can improve their understanding during conversation, but speechreading is a challenging skill to learn. Speechreading classes can improve acquisition, however there are a limited number of classes available and students can only practice effectively when attending class. To address this, we conducted a postal survey with 59 speechreading students to understand students' perspectives on practicing. Using our findings, we developed an Android application called MirrorMirror - a new Speechreading Acquisition Tool (SAT) that allows students to practice their speechreading by recording and watching videos of people they frequently speak with. We evaluated MirrorMirror through three case studies with speechreading students and found that they could effectively target their speechreading practice on people, words and situations they encounter during daily conversations. Speechreading can help people with hearing loss improve understanding during conversation, but is a challenging skill to acquire. Current Speechreading Acquisition Tools (SATs) are not adaptable to individual student needs unlike speechreading classes. To address the limitations of current approaches, we conducted a postal questionnaire with students from four speechreading classes to gather requirements for a new SAT called MirrorMirror. MirrorMirror allows speechreaders to practice lipshapes and words by recording videos of people they frequently talk to. We evaluated MirrorMirror through three case studies with speechreading students and found that it improved participants’ ability to practice outside of classes. Our future work is focussed on expanding MirrorMirror with a new practice mode based on sentences and allowing users to share their library of videos with each other.
"
Hybrid-Brailler: Combining Physical and Gestural Interaction for Mobile Braille Input and Editing,https://dl.acm.org/authorize?N656505,"
Braille input enables fast nonvisual entry speeds on mobile touchscreen devices. Yet, the lack of tactile cues commonly results in typing errors, which are hard to correct. We propose Hybrid-Brailler, an input solution that combines physical and gestural interaction to provide fast and accurate Braille input. We use the back of the device for physical chorded input while freeing the touchscreen for gestural interaction. Gestures are used in editing operations, such as caret movement, text selection, and clipboard control, enhancing the overall text entry experience. We conducted two user studies to assess both input and editing performance. Results show that Hybrid-Brailler supports fast entry rates as its virtual counterpart, while significantly increasing input accuracy. Regarding editing performance, when compared with the mainstream technique, Hybrid-Brailler shows performance benefits of 21% in speed and increased editing accuracy. We finish with lessons learned for designing future nonvisual input and editing techniques. We have presented Hybrid-Brailler, a system that combines physical and gestural interaction for Braille input. The prototype can be attached to mobile touchscreen devices to enhance their typing experience. Hybrid-Brailler allows blind users to leverage physical buttons, on the back of the device, to input Braille characters while freeing the touchscreen for editing operations. Results show that Hybrid-Brailler is significantly more accurate than its virtual counterpart while maintaining the same entry rate in typing tasks. Moreover, in a performance comparison to the default editing technique of Android 5.1, Hybrid-Brailler was significantly faster and three times more accurate. Further work is needed to improve our prototype by building slim and ergonomic components. Additionally, future research should focus on improving touchscreenbased editing techniques, particularly text selection and clipboard operations. Devising novel nonvisual feedback mechanisms will likely play a crucial role in improving error detection and editing awareness. Overall, users acknowledged the benefit of augmenting current “flat surfaces” of mobile devices with tactile cues, particularly in data entry tasks. Such approach can be extended to other tasks beyond text entry and be leveraged as an enhancement method for mobile devices. Do-ItYourself movements, 3D-printed technologies, and modular phones will play a crucial role in promoting this culture of inclusion and personalized computing.

"
Flexible and Mindful Self-Tracking: Design Implications from Paper Bullet Journals,https://dl.acm.org/authorize?N656506,"
Digital self-tracking technologies offer many potential benefits over self-tracking with paper notebooks. However, they are often too rigid to support people's practical and emotional needs in everyday settings. To inform the design of more flexible self-tracking tools, we examine bullet journaling: an analogue and customisable approach for logging and reflecting on everyday life. Analysing a corpus of paper bullet journal photos and related conversations on Instagram, we found that individuals extended and adapted bullet journaling systems to their changing practical and emotional needs through: (1) creating and combining personally meaningful visualisations of different types of trackers, such as habit, mood, and symptom trackers; (2) engaging in mindful reflective thinking through design practices and self-reflective strategies; and (3) posting photos of paper journals online to become part of a self-tracking culture of sharing and learning. We outline two interrelated design directions for flexible and mindful self-tracking: digitally extending analogue self-tracking and supporting digital self-tracking as a mindful design practice. The design of self-tracking technologies tends to be predefined and, therefore, often fails to support people’s practical goals, emotional needs, and changes in individual living circumstances [22,36,38]. To inform the design of more flexible self-tracking tools, we have examined the analogue and customisable bullet journaling approach for tracking, organising, and planning [7]. Analysing a corpus of paper bullet journal photos and related conversations on Instagram, we found that individuals extended and adapted bullet journaling systems to meet their practical and emotional needs in everyday life. They crafted and combined personally meaningful textual, numeric, and symbolic representations of different types of trackers (e.g. habit, mood, and symptom trackers). Through design practices and self-reflective strategies, they engaged in mindful reflective thinking, and posted photos of their paper journals online to become part of a visual selftracking culture of sharing and learning. Based on this understanding, we have discussed two interrelated design directions for flexible and mindful self-tracking: digitally extending analogue self-tracking with additional values, rather than replacing the use of pencil and paper, and supporting digital self-tracking as a mindful design practice, as opposed to concentrating only on passive automation and a predefined presentation of personal data.
"
Predicting Human Performance in Vertical Menu Selection Using Deep Learning,https://dl.acm.org/authorize?N656507,"
Predicting human performance in interaction tasks allows designers or developers to understand the expected performance of a target interface without actually testing it with real users. In this work, we present a deep neural net to model and predict human performance in performing a sequence of UI tasks. In particular, we focus on a dominant class of tasks, i.e., target selection from a vertical list or menu. We experimented with our deep neural net using a public dataset collected from a desktop laboratory environment and a dataset collected from hundreds of touchscreen smartphone users via crowdsourcing. Our model significantly outperformed previous methods on these datasets. Importantly, our method, as a deep model, can easily incorporate additional UI attributes such as visual appearance and content semantics without changing model architectures. By understanding about how a deep learning model learns from human behaviors, our approach can be seen as a vehicle to discover new patterns about human behaviors to advance analytical modeling. We presented a deep learning approach for modeling user performance for menu selection, a dominant task in modern interfaces. Our model is highly extensible. It can accommodate various UI aspects without changing the model architecture or using extensive domain knowledge. It outperformed a previous method on predicting performance time based on a public dataset and a large-scale smartphone dataset. We discussed an analysis of the model behaviors, which revealed new findings about how past experience of the user has an influence on the user performance with regard to different menu organizations. We contributed a set of knowledge and technical details about how to design, train and analyze a deep model for performance modeling.
"
Customizing Hybrid Products,https://dl.acm.org/authorize?N656508,"
We explore how the convergence of the digital and physical into hybrid products leads to new possibilities for customization. We report on a technology probe, a hybrid advent calendar with both paper form and digital layers of content, both of which were designed to be customizable. We reveal how over two hundred active users adapted its physical and digital aspects in various ways, some anticipated and familiar, but others surprising. This leads us to contribute concepts to help understand and design for hybrid customization -- the idea of broad customization spanning physical and digital; end-to-end customization by different stakeholders along the value chain for a product; and the combination of these into customization maps. Even though our advent-calendar is a relatively simple paperbased example of a hybrid product, it has revealed a wide range of possibilities for customization. We saw broad customization of both its physical and digital aspects. We also saw end-to-end customization in which various stakeholders along its value chain added value. Drawing these two observations together led to the concept of customization maps as a way of charting the various ways in which a hybrid product might potentially be customized. How might these ideas apply to other kinds of hybrid product, especially to more technically sophisticated ones? Consider the motorcar as an example. Cars have traditionally been physical products. However, over recent years they have become hybrid products, not only in the sense of hybrid engines, but also in the meaning of this paper – that they are now a mixture of physical and digital materials. The modern car includes a software engine management system as well as software services for navigation, communications and entertainment. Customization has long been part of the motor industry, from custom hand-built cars, to allowing consumers to choose their own interiors, fittings and options, to specialized custom shops. Customization can now be broadened to include the digital behaviour of the car, for example configuring different engine settings in software to provide sports or economy modes, and the possibilities will only increase as cars become more autonomous, including learning and adapting to their driver’s behaviour. Applying the concept of custom maps to a car encourages one to consider a wide range of possibilities for both physical and digital customization and also which stakeholders along its value chain can undertake these. What do manufacturers customize? What further value might dealers add through further customizations and what can consumers customize for themselves? Equally, as car is a safety-critical product, unlike an advent calendar, what customization are not appropriate for different stakeholders? It is beyond the remit of this paper to undertake a detailed analysis of this or other similar examples. However, by raising a somewhat extreme example of an emerging hybrid product we hope to suggest that the lessons learned from a simple paper-based technology probe may ultimately be more widely applicable to many future kinds of hybrid products that will soon enter our lives.
"
Fingers' Range and Comfortable Area for One-Handed Smartphone Interaction Beyond the Touchscreen,https://dl.acm.org/authorize?N656509,"
Previous research and recent smartphone development presented a wide range of input controls beyond the touchscreen. Fingerprint scanners, silent switches, and Back-of-Device (BoD) touch panels offer additional ways to perform input. However, with the increasing amount of input controls on the device, unintentional input or limited reachability can hinder interaction. In a one-handed scenario, we conducted a study to investigate the areas that can be reached without losing grip stability (comfortable area), and with stretched fingers (maximum range) using four different phone sizes. We describe the characteristics of the comfortable area and maximum range for different phone sizes and derive four design implications for the placement of input controls to support one-handed BoD and edge interaction. Amongst others, we show that the index and middle finger are the most suited fingers for BoD interaction and that the grip shifts towards the top edge with increasing phone sizes. In this paper, we investigated the areas which are reachable without changing the hand grip or losing grip stability (comfortable area) and the coverable range when fingers are fully stretched (maximum range). We conducted a lab study in which participants were recorded by a high-precision motion capture system while performing finger movements on four differently sized smartphones. We presented the average maximum range and the comfortable area which can inform the design of one-handed interaction on the back and edge of the device for a wide range of smartphone sizes. Based on the results, we derived four design implications that can help designers to place input controls so that users can interact with them one-handedly. Particularly, they help to CHI 2018 Paper CHI 2018, April 21–26, 2018, Montréal, QC, Canada Paper 31 Page 8 find suitable placements of input controls that do not require a change of hand grip or a loss of grip stability which could lead to dropping the device or muscle strain. Amongst others, the key findings include using the index and middle finger for Back-of-Device (BoD) interaction since they are the most flexible fingers and the shift of the hand grip towards the top edge with increasing device sizes. While we derived common design implications for one-handed smartphone interaction, future research should follow up on our analysis on the impact of different hand sizes on the finger range and comfortable area. Resulting findings could be used to propose design implications for specific hand sizes (e.g., smartphones for children). Moreover, the comfortable area and maximum range can be explored for specific types of input controls. While touching a button is usually done with an angled finger, fingerprint scanners require a flat placement of the finger. Investigating one specific type of input control would limit the result’s generalizability due to constraints such as specific finger angles. However, the results can be more precise when only a certain type of input control is used.

"
Exploration and Explanation in Computational Notebooks,https://dl.acm.org/authorize?N656500,"
Computational notebooks combine code, visualizations, and text in a single document. Researchers, data analysts, and even journalists are rapidly adopting this new medium. We present three studies of how they are using notebooks to document and share exploratory data analyses. In the first, we analyzed over 1 million computational notebooks on GitHub, finding that one in four had no explanatory text but consisted entirely of visualizations or code. In a second study, we examined over 200 academic computational notebooks, finding that although the vast majority described methods, only a minority discussed reasoning or results. In a third study, we interviewed 15 academic data analysts, finding that most considered computational notebooks personal, exploratory, and messy. Importantly, they typically used other media to share analyses. These studies demonstrate a tension between exploration and explanation in constructing and sharing computational notebooks. We conclude with opportunities to encourage explanation in computational media without hindering exploration. Computational notebooks address many fundamental challenges with performing, documenting, and sharing data analyses. They support incremental and iterative analyses, enabling users to edit, arrange, and execute small blocks of code in any order. They enable explanation of thought processes by allowing analysts to intersperse code with richly formatted textual explanations. They facilitate sharing by combining code, visualizations, and text in a single document that can be posted online or emailed. Some computational notebooks are truly remarkable in the way they elegantly explain complex analyses [16].

"
Multi-Touch Skin: A Thin and Flexible Multi-Touch Sensor for On-Skin Input,https://dl.acm.org/authorize?N656501,"
Skin-based touch input opens up new opportunities for direct, subtle, and expressive interaction. However, existing skin-worn sensors are restricted to single-touch input and limited by a low resolution. We present the first skin overlay that can capture high-resolution multi-touch input. Our main contributions are: 1) Based on an exploration of functional materials, we present a fabrication approach for printing thin and flexible multi-touch sensors for on-skin interactions. 2) We present the first non-rectangular multi-touch sensor overlay for use on skin and introduce a design tool that generates such sensors in custom shapes and sizes. 3) To validate the feasibility and versatility of our approach, we present four application examples and empirical results from two technical evaluations. They confirm that the sensor achieves a high signal-to-noise ratio on the body under various grounding conditions and has a high spatial accuracy even when subjected to strong deformations. We have presented the first method that allows interaction designers to design and fabricate functional and high-resolution multi-touch sensor skins for the body. This includes a design tool that assists the designer in generating sensors of custom and non-rectangular shape, and the first technique for printing a mutual-capacitance sensor on a commodity inkjet printer. A set of functional sensors and practical application examples, as well as results from two technical studies demonstrate the sensor’s functionality on the human body, in various scales, and when undergoing significant deformation. Promising avenues for future work comprise advanced design tools for the body and simultaneous sensing of multiple modalities.

"
Improving Comprehension of Measurements Using Concrete Re-expression Strategies,https://dl.acm.org/authorize?N656502,"
It can be difficult to understand physical measurements (e.g., 28 lb, 600 gallons) that appear in news stories, data reports, and other documents. We develop tools that automatically re-express unfamiliar measurements using the measurements of familiar objects. Our work makes three contributions: (1) we identify effectiveness criteria for objects used in concrete measurement re-expressions; (2) we operationalize these criteria in a scalable method for mining a large dataset of concrete familiar objects with their physical dimensions from Amazon and Wikipedia; and (3) we develop automated concrete re-expression tools that implement three common re-expression strategies (adding familiar context, reunitization and proportional analogy) as energy minimization algorithms. Crowdsourced evaluations of our tools indicate that people find news articles with re-expressions more helpful and re- expressions help them to better estimate new measurements. We presented a set of tools for automatically re-expressing unfamiliar measurements using the measurements of familiar objects. The key idea of our approach is to identify criteria for effective re-expressions then build a database of familiar objects and their measurements by combining information from semantic databases, object databases and crowdsourcing. We show the database can be used to implement three common re-expression strategies. Our tools make it easier for publishers, educators, or journalists to enhance their audience’s understanding of measurements through concrete measurement re-expressions in various informal and educational contexts.

"
Understanding the Needs of Searchers with Dyslexia,https://dl.acm.org/authorize?N656503,"
As many as 20% of English speakers have dyslexia, a language disability that impacts reading and spelling. Web search is an important modern literacy skill, yet the accessibility of this language-centric endeavor to people with dyslexia is largely unexplored. We interviewed ten adults with dyslexia and conducted an online survey with 81 dyslexic and 80 non-dyslexic adults, in which participants described challenges they face in various stages of web search (query formulation, search result triage, and information extraction). We also report the findings of an online study in which 174 adults with dyslexia and 172 without dyslexia rated the readability and relevance of sets of search query results. Our findings demonstrate differences in behaviors and preferences between dyslexic and non-dyslexic searchers, and indicate that factoring readability into search engine rankings and/or interfaces may benefit both dyslexic and non-dyslexic users. In this paper, we identified that web search accessibility for people with dyslexia is an important and largely unaddressed challenge for the HCI and Information Retrieval communities. By conducting and analyzing interviews with ten adults with dyslexia as well as an online survey with 80 dyslexic and 81 non-dyslexic adults, we found that people with dyslexia experience challenges in finding information through web search relating to all stages of the information seeking process. We also conducted an online study in which 174 dyslexic and 172 non-dyslexic adults rated the readability and relevance of search results. From these studies, we identified several aspects of query formulation, results triage, and information extraction that are challenging for people with dyslexia, and found marked differences in dyslexic and non-dyslexic adults’ search behaviors. Reflecting on these findings, we proposed several design suggestions for improving both search interfaces and search algorithms. These findings and design implications open new avenues for further research on improving the accessibility of information seeking and, more generally, the web, for people with reading disabilities.
"
Evaluation Strategies for HCI Toolkit Research,https://dl.acm.org/authorize?N656504,"
Toolkit research plays an important role in the field of HCI, as it can heavily influence both the design and implementation of interactive systems. For publication, the HCI community typically expects toolkit research to include an evaluation component. The problem is that toolkit evaluation is challenging, as it is often unclear what 'evaluating' a toolkit means and what methods are appropriate. To address this problem, we analyzed 68 published toolkit papers. From our analysis, we provide an overview of, reflection on, and discussion of evaluation methods for toolkit contributions. We identify and discuss the value of four toolkit evaluation strategies, including the associated techniques that each employs. We offer a categorization of evaluation strategies for toolkit researchers, along with a discussion of the value, potential limitations, and trade-offs associated with each strategy. Research toolkits have fundamentally influenced and shaped the way interactive technology is built, and will continue to do so. Despite the impact and success of toolkits, evaluating them remains a challenge. This paper is a first attempt at clarifying what evaluation methods are used, when they are appropriate and how they are performed. We derived four evaluation types and associated techniques for HCI toolkits based on 68 toolkit papers. We hope our categorization and reflection helps strengthen methods for toolkit research and move technical HCI research forward.

"
TopoText: Context-Preserving Text Data Exploration Across Multiple Spatial Scales,https://dl.acm.org/authorize?N656515,"
TopoText is a context-preserving technique for visualizing text data for multi-scale spatial aggregates to gain insight into spatial phenomena. Conventional exploration requires users to navigate across multiple scales but only presents the information related to the current scale. This limitation potentially adds more steps of interaction and cognitive overload to the users. TopoText renders multi-scale aggregates into a single visual display combining novel text-based encoding and layout methods that draw labels along the boundary or filled within the aggregates. The text itself not only summarizes the semantics at each individual scale, but also indicates the spatial coverage of the aggregates and their underlying hierarchical relationships. We validate TopoText with both a user study as well as several application examples. We have presented a text-based visualization technique called TopoText for maintaining the semantic context in the multiscale aggregation space. Our primary contribution includes a set of visual encoding and layout strategies that spatialize visual text labels on the boundary or in the inner space of the aggregates. We have explored and evaluated several design choices that utilize different visual attributes of text labels including color, opacity, density and orientation for multiscale text exploration tasks. Our future work includes optimizing the rendering performance of TopoText by precomputing the visualizations and organizing them as map tiles. We also plan on extending TopoText to supporting other types of spatial data, or non-spatial data that can be spatialized in a meaningful way. Finally, we plan on extending TopoText to exploring the multi-scale aggregation dynamics in real-time applications.

"
Design Patterns for Data Comics,https://dl.acm.org/authorize?N656516,"
Data comics for data-driven storytelling are inspired by the visual language of comics and aim to communicate insights in data through visualizations. While comics are widely known, few examples of data comics exist and there has not been any structured analysis nor guidance for their creation. We introduce data-comic design-patterns, each describing a set of panels with a specific narrative purpose, that allow for rapid storyboarding of data comics while showcasing their expressive potential. Our patterns are derived from i) analyzing common patterns in infographics, datavideos, and existing data comics, ii) our experiences creating data comics for different scenarios. Our patterns demonstrate how data comics allow an author to combine the best of both worlds: spatial layout and overview from infographics as well as linearity and narration from videos and presentations. In this paper, we introduced design patterns for data comics, a novel but scarcely explored genre. We demonstrated design patterns that assist in storyboarding data comics and developing about their expressiveness and potential. However, as comics combine aspects of communication through space and elements from temporal narration, the boundaries with other genres such as infographics and videos or presentations are somewhat fuzzy, thus requiring more research. Our patterns are inspired from and apply to other genres to some extent, while describing a common design space for presentation in data-driven storytelling. We believe data comics and our design patterns will inform future interfaces. They furthermore can change the way we conceptualize and create infographics and presentations, as comics provide a unique way of translating narrative practices and aspects from one medium to the other. Overall, we found data comics to be an engaging way to teach students about data, visualization, the art of storytelling, as well as the need for effective communication based on the evidence in data. We hope our patterns will help designers, data scientists, data journalists, and everyone else engaged in presenting data to create novel data comics and find new ways to express data.

"
"Practices and Technology Needs of a Network of Farmers in Tharaka Nithi, Kenya",https://dl.acm.org/authorize?N656517,"
Farmers in rural areas of Kenya generally rely on traditional agricultural practices inherited from past generations. However, population increases and climate changes have put pressure on resources such as land and water. These resource pressures have created a need to broaden and expand farming practices. We conducted an exploratory study with farmers in Tharaka Nithi, Kenya to explore their practices, if and how they used ICT, and how the technologies used might be designed to aid their practices, if at all. Overall, our results show that farmers desired more knowledge to enable them apply ICT interventions in ways that improved yields. Farmers were also interested in accessing information on soil fertility, water predictability and market opportunities. These findings suggest opportunities for technology design to support farming practices among rural communities in rural settings. We also articulate social challenges that designers will face when thinking about coming up with such solutions. Our paper explores factors shaping how and why rural farmers in Kenya use technology while connecting their routine farming activities. We found that farmers generally relied on traditional farming practices and mainly used mobile phones to share information around learning about better farming practices and markets for their produce. The participants also indicated interest in technology solutions that could provide information on soil fertility, support equitable distribution of irrigation water and also connect farmers with potential buyers. Even though a considerable number of our participants were not technology savvy, our results point to ways through which collaborative technology such as mobile phone chat applications could be leveraged to promote information sharing in rural farming communities where persistent disparities in terms of income and access to financial support opportunities are prevalent. Ultimately, we hope this study will inspire future research into how collaborative technologies might be better designed and be more meaningfully situated within rural farming communities in developing countries.
"
Design for Collaborative Survival: An Inquiry into Human-Fungi Relationships,https://dl.acm.org/authorize?N656518,"
In response to recent calls for HCI to address ongoing environmental crises and existential threats, this paper introduces the concept of collaborative survival and examines how it shapes the design of interactive artifacts. Collaborative survival describes how our (human) ability to persist as a species is deeply entangled with and dependent upon the health of a multitude of other species. We explore collaborative survival within the context of designing tools for mushroom foraging and reflect on how interactive products can open new pathways for noticing and joining-with these entanglements towards preferable futures. In addition to highlighting three tactics-engagement, attunement and expansion-that can guide designs towards multispecies flourishing, our prototypes illustrate the potential for wearable technology to extend the body into the environment. In this paper, we introduce the metaphor of collaborative survival to describe how human life is dependent and entangled upon the health of other species. We then situate the collaborative survival within existing HCI research as a way to address related work and also to see where it can contribute in the realm of design. We then describe prototypes (HSI, Data HarVest, and Spore Stepper) that seek to re-envision human-fungi relationships in the context of a mushroom foray as examples of how collaborative survival can be implemented in design. Three concepts emerged from these designs, engagement, attunement and expansion that provoke acts of noticing, an important tenet of collaborative survival. The process of building tools for collaborative survival shows potential as ways of how wearables can be used to reshape our perspectives of natural systems. In building these objects around the practice of forays used in mycology, the study of fungi, not just as contextual framing but as extended metaphor, these objects pose to become a step towards forming these alternative mutualistic relationships with humans and nonhuman others using technology. Speculative yet plausible, critical yet pragmatic, the tools suggest for a future that can already exist here and now.
"
“An Odd Kind of Pleasure”: Differentiating Emotional Challenge in Digital Games,https://dl.acm.org/authorize?N656519,"
Recent work introduced the notion of emotional challenge as a means to afford more unique and diverse gaming experiences. However, players' experience of emotional challenge has received little empirical attention. It remains unclear whether players enjoy it and what exactly constitutes the challenge thereof. We surveyed 171 players about a challenging or an emotionally challenging experience, and analyzed their responses with regards to what made the experience challenging, their emotional response, and the relation to core player experience constructs. We found that emotional challenge manifested itself in different ways, by confronting players with difficult themes or decisions, as well as having them deal with intense emotions. In contrast to more'conventional' challenge, emotional challenge evoked a wider range of negative emotions and was appreciated significantly more by players. Our findings showcase the appeal of uncomfortable gaming experiences, and extend current conceptualizations of challenge in games. Emotional challenge has been suggested to afford more unique and diverse gaming experiences, beyond traditional notions of challenge in games. Yet the concept has to date never been empirically studied. We analyzed 171 players’ accounts of either challenging or emotionally challenging experiences. Players reported being emotionally challenged by confronting difficult themes, grappling with tough in-game decisions, and by having to keep in control of their own intense negative emotions. Compared to more ‘conventional’ notions of challenge, emotional challenge evoked a wider range of negative emotions, but was nevertheless well received by most players. Moreover, our findings suggest that several core aspects of conventional challenge do not readily apply to emotional challenge. Taken together, our findings highlight the appeal of uncomfortable player experiences, and extend our understanding of (emotional) challenge in games.
"
Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality,https://dl.acm.org/authorize?N656510,"
Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR. In this paper, we studied eyes-free acquisition of targets in the interaction space around body for VR through three studies. In Study 1 and Study 2, we tested the subjective acceptance and control accuracy of the eyes-free acquisition. We explored the positions that users felt comfortable to acquire and the minimum distance between targets that they needed to acquire them with certainty. Then we measured the offset from users’ acquisition point to the target when it was located at different positions and tested the influence of body rotations made on their control accuracy. In Study 3, by comparing the eyesfree approach to the eyes-engaged approach, we showed that eyes-free target acquisition provided the benefits of faster speed, less fatigue and sickness, and less distraction from other ongoing tasks. While it might cause heavier mental demand and relatively lower accuracy (92.59% vs. 98.87% for 18 targets) as the trade-off. Overall, most users (13/16) preferred eyes-free acquisition over eyes-engaged acquisition, especially when the FOV was small or there was a second task. Based on the results and user feedback, we make suggestions on layout design and design implications for real applications.

"
Change Blindness in Proximity-Aware Mobile Interfaces,https://dl.acm.org/authorize?N656511,"
Interface designs on both small and large displays can encourage people to alter their physical distance to the display. Mobile devices support this form of interaction naturally, as the user can move the device closer or further away as needed. The current generation of mobile devices can employ computer vision, depth sensing and other inference methods to determine the distance between the user and the display. Once this distance is known, a system can adapt the rendering of display content accordingly and enable proximity-aware mobile interfaces. The dominant method of exploiting proximity-aware interfaces is to remove or superimpose visual information. In this paper, we investigate change blindness in such interfaces. We present the results of two experiments. In our first experiment we show that a proximity-aware mobile interface results in significantly more change blindness errors than a non-moving interface. The absolute difference in error rates was 13.7%. In our second experiment we show that within a proximity-aware mobile interface, gradual changes induce significantly more change blindness errors than instant changes---confirming expected change blindness behavior. Based on our results we discuss the implications of either exploiting change blindness effects or mitigating them when designing mobile proximity-aware interfaces. In this paper we have presented the results of two studies. First, we have shown that proximity-aware interfaces induce more change blindness errors than non-moving interfaces. Second, we replicated results from classic change blindness experiments in which it was found that gradual changes are harder to detect than instant changes. The highest error rates occurred for gradual change across small, medium and large updates. These are important findings as they relate both to mitigating and exploiting change blindness in new proximity-aware mobile interfaces. We suggest future work should not only be restricted to device motion, as in our experiment, but extended to user motion. Such a future investigation could potentially give rise to a further useful design implication: if change blindness is as prevalent in user motion as it is with device motion, then ubiquitous large displays could exploit change blindness induced due to the user’s forward motion to hide irrelevant changes. Our experimental results suggest that change blindness, and perhaps other perceptional phenomena such as inattentional blindness, merits further investigation from a human-computer interaction perspective for two reasons. First, to resolve problems where users fail to notice interface changes. Second, to understand the design potential of creating unobtrusive and discreet interface changes that can help offload the cognitive burden of the user.
"
Rethinking Thinking Aloud: A Comparison of Three Think-Aloud Protocols,https://dl.acm.org/authorize?N656512,"
This paper presents the results of a study that compared three think-aloud methods: concurrent think-aloud, retrospective think-aloud, and a hybrid method. The three methods were compared through an evaluation of a library website, which involved four points of comparison: task performance, participants' experiences, usability problems discovered, and the cost of employing the methods. The results revealed that the concurrent method outperformed both the retrospective and the hybrid methods in facilitating successful usability testing. It detected higher numbers of usability problems than the retrospective method, and produced output comparable to that of the hybrid method. The method received average to positive ratings from its users, and no reactivity was observed. Lastly, this method required much less time on the evaluator's part than did the other two methods, which involved double the testing and analysis time. This paper has discussed the results of using the traditional think-aloud methods: the concurrent think-aloud method, the retrospective think-aloud method, and the hybrid method. These three methods were compared through an evaluation of a library website, which involved four points of comparison: overall task performance, test participants’ experiences, quantity and quality of usability problems discovered, and the cost of employing methods. Overall, the findings revealed that the concurrent method can be argued to have outperformed the retrospective method and hybrid method in facilitating usability testing. It detected higher numbers of usability problems than the retrospective method, and produced output comparable to that of the hybrid method. The method received average to positive ratings from its users, and the possible reactivity associated with the concurrent think-aloud was not observed in this study, as no differences between participants' task success rates were found for this method compared to the silent condition in the retrospective test. In addition, this method required much less time on the evaluator’s part than the other two methods, which required double the testing and analysis time. These findings imply a basis for preferring the concurrent method over the retrospective and hybrid methods.
"
"Reading on Smart Glasses: The Effect of Text Position, Presentation Type and Walking",https://dl.acm.org/authorize?N656513,"
Smart glasses are increasingly being used in professional contexts. Having key applications such as short messaging and newsreader, they enable continuous access to textual information. In particular, smart glasses allow reading while performing other activities as they do not occlude the user's world view. For efficient reading, it is necessary to understand how a text should be presented on them. We, therefore, conducted a study with 24 participants using a Microsoft HoloLens to investigate how to display text on smart glasses while walking and sitting. We compared text presentation in the top-right, center, and bottom-center positions with Rapid Serial Visual Presentation (RSVP) and line-by-line scrolling. We found that text displayed in the top-right of smart glasses increases subjective workload and reduces comprehension. RSVP yields higher comprehension while sitting. Conversely, reading with scrolling yields higher comprehension while walking. Insights from our study inform the design of reading interfaces for smart glasses. We investigated three text positions (top-right, center, bottomcenter) and two presentation types (RSVP, line-by-line scrolling) on a binocular see-through smart glasses while walking and sitting. We studied how walking, text position, and presentation type affect comprehension, reading and walking speed, and workload. We supported our investigation with quantitative objective and subjective data, and qualitative feedback. We found that presenting text in the top-right of a smart glasses results a significantly lower text comprehension and higher workload with compared to the center and the bottom-center positions while both walking and sitting. RSVP results in higher comprehension while sitting and reading with scrolling results in higher comprehension while walking. Furthermore, we found that mobility affects reading speed: reading while walking was slower than while sitting. The findings can be used as design recommendations for implementing reading-based applications on smart glasses. As in our study, we did not use secondary task while sitting or obstacles while walking, we suggest future work to investigate these effects on reading on smart glasses. As we did our study in an indoor environment, we plan to carry out of these studies in outdoor settings.

"
Mini-Me: An Adaptive Avatar for Mixed Reality Remote Collaboration,https://dl.acm.org/authorize?N656514,"
We present Mini-Me, an adaptive avatar for enhancing Mixed Reality (MR) remote collaboration between a local Augmented Reality (AR) user and a remote Virtual Reality (VR) user. The Mini-Me avatar represents the VR user's gaze direction and body gestures while it transforms in size and orientation to stay within the AR user's field of view. A user study was conducted to evaluate Mini-Me in two collaborative scenarios: an asymmetric remote expert in VR assisting a local worker in AR, and a symmetric collaboration in urban planning. We found that the presence of the Mini-Me significantly improved Social Presence and the overall experience of MR collaboration. In this paper, we presented our concept, design, and implementation of the Mini-Me, a novel adaptive avatar with redirected gaze and gestures for enhancing remote MR collaboration. We evaluated the impact of the Mini-Me on Social Presence, task difficulty, and mental effort for two collaborative scenarios. And we discussed the implications of the Mini-Me for MR collaborative interface design. Overall, we found that the Mini-Me avatar was able to convey the non-verbal communication cues necessary to improve the performance on an asymmetric object placement task. It was also useful for improving Social Presence in both asymmetric and symmetric tasks. In both cases users overwhelmingly preferred having the Mini-Me avatar. This supports our belief that adding the adaptive Mini-Me avatar could improve the user’s awareness of their partner in a collaborative MR interface between AR and VR. For our future work, we would like to improve the MiniMe’s adaptive surface projection technique. We are considering applying an automatic alignment technique, such as SnapToReality [30] that automatically aligns virtual objects to physical constraints calculated from the real world in real-time. A similar feature is also available as part of the Microsoft MR platform called “Spatial Understanding” [28], which we plan to use in our next iteration. We also would like to improve the empathy of the remote collaboration by recognizing and mapping facial expression of the VR/AR user onto the avatar [26]. We plan to conduct a follow up study for more complex tasks.

"
Viewer Experience of Obscuring Scene Elements in Photos to Enhance Privacy,https://dl.acm.org/authorize?N656525,"
With the rise of digital photography and social networking, people are sharing personal photos online at an unprecedented rate. In addition to their main subject matter, photographs often capture various incidental information that could harm people's privacy. While blurring and other image filters may help obscure private content, they also often affect the utility and aesthetics of the photos, which is important since images shared in social media are mainly for human consumption. Existing studies of privacy-enhancing image filters either primarily focus on obscuring faces, or do not systematically study how filters affect image utility. To understand the trade-offs when obscuring various sensitive aspects of images, we study eleven filters applied to obfuscate twenty different objects and attributes, and evaluate how effectively they protect privacy and preserve image quality for human viewers. Our work sheds light on the effects of applying various types of image transforms to scene elements in an image. In particular we studied the relative trade-offs between privacy (revealing and concealing selective attributes of objects) and utility (the visual aesthetics and user satisfaction of the image) of five different image transforms and show that while in some cases a clear privacy vs. utility trade-off is realized, in other scenarios a high degree of privacy can be attained while retaining utility. Our work also contributes significantly to the existing literature by examining these trade-offs for a range of objects and their attributes, whereas previous work had focused largely on obscuring people and faces. We hope our work spurs further research on studying the relative trade-offs of image transformations for enhanced privacy without (significantly) degrading the user experience of the viewers.
"
Navigating the Job Search as a Low-Resourced Job Seeker,https://dl.acm.org/authorize?N656526,"
The Internet is providing increasing access to information about employment opportunities, but not everyone can leverage it effectively. Research suggests that job seekers with limited access to Internet technologies are being left behind, while those with limited social resources are expected to rely on the Internet even more. In this work, we conducted in-depth semi-structured interviews with 11 low-resourced job seekers in a metropolitan area in the Midwestern USA to understand how social and digital resources support their efforts to find work. We find that online resources support job seekers in finding relevant jobs via search, but do not help them identify opportunities to improve their job search process or increase their chances of securing employment. We recommend that systems aiming to support low-resourced job seekers design for deeper engagement with their users across the job search process, to help users recognize ways to improve on their existing practices. In this work, we examine the roles of important information resources in the job search process of low-resourced job seekers through semi-structured interviews. We find that, while personal social connections remain vital for the majority of job seekers, online employment search platforms and personal social media sites enhance job seekers’ existing practices and afford innovative forms of job seeking. However, these online resources do not help job seekers identify problems in their job search or overcome structural barriers. To empower job seekers experiencing these issues, we call for employment platforms to engage with their users by conceiving of the job search as an extended, goal-oriented process. To achieve this, we propose designs recommendations that support access to trustworthy information about jobs and employers, prompt reflection about activities throughout the job search, and identify valuable opportunities to leverage social relationships in both the job search and in long-term career development.
"
The Value of Empty Space for Design,https://dl.acm.org/authorize?N656527,"
We present a study on a group of people who, upon adopting a new lifestyle movement, have discovered and constructed alternative aspects of space. Drawing on 23 interviews with minimalists and participant observations of their Meetup meetings, we highlight the central role of empty space in their lives at home. Our findings show how empty space for minimalists emerge as a new, hitherto unknown space in the home and the ways minimalists seek to create, maintain, and stay sensitive to these empty spaces. Empty spaces for minimalists signify their achievements, exudes aesthetic appeal, and provide a sanctuary away from city life. We propose new opportunities for design based on our findings of empty space. We suggest that design should consider supporting the practices and values that revolve around the absence of artifacts. In this paper, we have described the value of designing for empty space. By focusing on minimalists, we were able to gain close insight on what it means to treat empty space as a first class object, one that deserves respect and represents— both visibly and invisibly—the new values our informants have adopted. While our findings draw from our ethnographic work on minimalists, we believe their practices and values can inform designers concerned with technologies for any sort of space that has long-term engagement. For example, the workplace, second homes, and third places (e.g., libraries, cafes) are all locales we regular interact with and, moreover, contain “other” spaces—spaces we regularly overlook. Spaces like dusty corners, closets, and storage boxes can be considered other spaces. Attuning ourselves to empty space suggests that we scrutinize the relationship between object and empty space, the ways we process empty space, and how we appreciate and interact directly with empty space. Future work should investigate how technologies can support these three focal points of empty space. For instance, while previous designs have examined gazing at spaces, these studies have the end-goal of designing new technologies to be placed in these spaces. Instead, we ask whether we can study the ways in which we interact with (e.g,. gaze upon) space for the sake of designing technologies that support empty space rather than inserting and thus destroying the very qualities of empty space that people enjoy. Such technologies show promise to give the same aesthetic, mental, and spiritual benefits that so transformed our minimalists’ perspectives on space.
"
Disorder or Driver?: The Effects of Nomophobia on Work-Related Outcomes in Organizations,https://dl.acm.org/authorize?N656528,"
Nomophobia, which refers to discomfort or anxiety caused by being unable to use one's smartphone, has become prevalent among smartphone users. However, the influence of nomophobia on employees' work-related outcomes remains unclear. Drawing on the job demands-resources theory, this study develops a model that explores the interplay between employees' nomophobia, work engagement, emotional exhaustion, work interruption, and job productivity. The proposed model was tested using data collected from 187 employees in one organization. The results demonstrate that some employees with high levels of nomophobia feel more engaged with their work and more productive, yet others tend to be emotionally exhausted and feel they are less productive. By illuminating the dual effects of nomophobia on employees' work-related outcomes, this study extends our understanding of how smartphone use positively and negatively affects employees in the workplace. The notion of nomophobia in the workplace is discussed, along with new directions for research. This research extends the existing literature by proposing the dual effects of nomophobia on work-related outcomes. Previous research has shown that employee productivity can be positively predicted by work engagement and negatively predicted by emotional exhaustion. By investigating how nomophobia influences work engagement and emotional exhaustion, our work expands on the studies regarding technology use and employee productivity. We conclude from our empirical test of the model we developed, that although nomophobia can induce work engagement, it also can increase emotional exhaustion, thereby adversely affecting job productivity. Hence, organizations need to pay attention to the paradoxical mechanisms related to employees’ nomophobia. We hope that our findings will assist researchers and managers in understanding the phenomenon of nomophobia in the workplace.
"
"My Telepresence, My Culture?: An Intercultural Investigation of Telepresence Robot Operators' Interpersonal Distance Behaviors",https://dl.acm.org/authorize?N656529,"
Interpersonal distance behaviors can vary significantly across countries and impact human social interaction. Do these cross-cultural differences play out when one of the interaction partners participates through a teleoperated robot? Emerging research shows that when being approached by a robot, people tend to hold similar cultural preferences as they would for an approaching human. However, no work yet has investigated this question from a robot teleoperator's perspective. Toward answering this, we conducted an online study (N = 774) using a novel simulation paradigm across two countries (U.S. and India). Results show that in the role of a telepresence robot operator, participants exhibited cross-cultural differences in interpersonal distance behavior in line with human-human proxemic research, indicating that culture-specific distance behavior can manifest in the way a robot operator controls a robot. We discuss implications for designers who seek to automate path planning and navigation for teleoperated robots. In an increasingly technological and global world, one may find oneself interacting with a coworker from another country through a company’s telepresence robot. What do we need to be aware of in these situations, and how should we design the next level technology to support the social and emotional functioning of those involved? The present study provided initial evidence that a robot teleoperator’s culture-specific behaviors can be transmitted to the intermediary robot through the operator’s actions on the control interface, at least in terms of interpersonal distance behaviors. We also presented a new robot teleoperation simulation paradigm that can facilitate CHI 2018 Paper CHI 2018, April 21–26, 2018, Montréal, QC, Canada Paper 51 Page 8 future online investigations of remote robot operators’ perspectives and behaviors. Finally, our results offer an important design insight for the future development of robot teleoperation that takes into consideration culture-specific behaviors of the robot teleoperator.
"
"Privacy Lies: Understanding How, When, and Why People Lie to Protect Their Privacy in Multiple Online Contexts",https://dl.acm.org/authorize?N656520,"
In this paper, we study online privacy lies: lies primarily aimed at protecting privacy. Going beyond privacy lenses that focus on privacy concerns or cost/benefit analyses, we explore how contextual factors, motivations, and individual-level characteristics affect lying behavior through a 356-person survey. We find that statistical models to predict privacy lies that include attitudes about lying, use of other privacy-protective behaviors (PPBs), and perceived control over information improve on models based solely on self-expressed privacy concerns. Based on a thematic analysis of open-ended responses, we find that the decision to tell privacy lies stems from a range of concerns, serves multiple privacy goals, and is influenced by the context of the interaction and attitudes about the morality and necessity of lying. Together, our results point to the need for conceptualizations of privacy lies-and PPBs more broadly-that account for multiple goals, perceived control over data, contextual factors, and attitudes about PPBs. In this study, we put forth an improved model to predict privacy lies that incorporates actual privacy-protective behaviors, perceived control over data, and attitudes about lying in addition to commonly used scales of privacy-related concerns. We then identified how contextual factors influence privacy lies during interactions with people and with systems, such as perceptions of the request and requestor. Finally, we characterized the different types of privacy lies, the functions they serve, and the reasons people have for telling or abstaining from privacy lies in multiple contexts. Overall, our findings highlight the need to examine a range of contextual factors and motivations, beyond general privacy concerns, in understanding privacy lies as well as privacy-protective behaviors more generally.

"
"""We Are the Product"": Public Reactions to Online Data Sharing and Privacy Controversies in the Media",https://dl.acm.org/authorize?N656521,"
As online platforms increasingly collect large amounts of data about their users, there has been growing public concern about privacy around issues such as data sharing. Controversies around practices perceived as surprising or even unethical often highlight patterns of privacy attitudes when they spark conversation in the media. This paper examines public reaction ""in the wild"" to two data sharing controversies that were the focus of media attention-regarding the social media and communication services Facebook and WhatsApp, as well as the email service unroll.me. These controversies instigated discussion of data privacy and ethics, accessibility of website policies, notions of responsibility for privacy, cost-benefit analyses, and strategies for privacy management such as non-use. An analysis of reactions and interactions captured by comments on news articles not only reveals information about pervasive privacy attitudes, but also suggests communication and design strategies that could benefit both platforms and users. Our analysis of “in the wild” reactions to perceived data sharing and privacy violations supports concepts from prior work around issues such as responsibility [10,24], costbenefit analyses [33,39], the role of privacy policies [6,14,44], non-use as a strategy [39,4,38] trust for platforms [18,47], and expectation violations [31]. Because we were examining concrete reactions, our data has strong ecological validity but often lacks information about motivation or user characteristics. There is a strong case for further qualitative work to validate these user attitudes (beyond comparisons to prior work) based on the rich information we have now about specific reactions. There is also potential for future work in conducting studies with commenters on news articles in order to tease out exactly how media portrayals impact their attitudes. Additionally, in discussing the trade-offs of potential solutions, we put forth increased transparency about the why of privacy practices (in conjunction with increased accessibility of that information) as a solution that could work particularly well in the context of perceived privacy violations around data sharing. Additional next steps would be to study users’ reactions to this proposal, as well as to find out more about their information needs, and to conduct usability studies around the best display mode for this information. Our findings also point to the importance of understanding user expectations when it comes to privacy; whether most users agree that it’s okay to be the product or not, shaping expectations with more transparency could help reduce the frequency of these kinds of privacy controversies.
"
FaceDisplay: Towards Asymmetric Multi-User Interaction for Nomadic Virtual Reality,https://dl.acm.org/authorize?N656522,"
Mobile VR HMDs enable scenarios where they are being used in public, excluding all the people in the surrounding (Non-HMD Users) and reducing them to be sole bystanders. We present FaceDisplay, a modified VR HMD consisting of three touch sensitive displays and a depth camera attached to its back. People in the surrounding can perceive the virtual world through the displays and interact with the HMD user via touch or gestures. To further explore the design space of FaceDisplay, we implemented three applications (FruitSlicer, SpaceFace and Conductor) each presenting different sets of aspects of the asymmetric co-located interaction (e.g. gestures vs touch). We conducted an exploratory user study (n=16), observing pairs of people experiencing two of the applications and showing a high level of enjoyment and social interaction with and without an HMD. Based on the findings we derive design considerations for asymmetric co-located VR applications and argue that VR HMDs are currently designed having only the HMD user in mind but should also include Non-HMD Users. In this work, we presented the design and implementation of FaceDisplay, a mobile VR HMD prototype consisting of three touch sensitive displays and a depth camera attached to its back. FaceDisplay enables people in the surrounding to perceive the virtual world through the displays and interact with the HMD User via touch or gestures. We presented three applications (FruitSlicer, SpaceFace and Conductor), each focusing on one specific aspect of the asymmetric co-located interaction. We further conducted an exploratory user study (n=16), observing pairs of people experiencing two of the applications. Our results showed that FaceDisplay was able to let the Non-HMD User perceive and interact with the HMD User but resulted also in a high level of dominance and responsibility of the Non-HMD User over the HMD User. We argue that VR HMDs are currently designed having only the HMD user in mind but should also include all the people in the environment to break out of the current isolation an HMD User experiences when using VR HMDs.

"
Interactive Guidance Techniques for Improving Creative Feedback,https://dl.acm.org/authorize?N656523,"
Good feedback is critical to creativity and learning, yet rare. Many people do not know how to actually provide effective feedback. There is increasing demand for quality feedback -- and thus feedback givers -- in learning and professional settings. This paper contributes empirical evidence that two interactive techniques -- reusable suggestions and adaptive guidance -- can improve feedback on creative work. We present these techniques embodied in the CritiqueKit system to help reviewers give specific, actionable, and justified feedback. Two real-world deployment studies and two controlled experiments with CritiqueKit found that adaptively-presented suggestions improve the quality of feedback from novice reviewers. Reviewers also reported that suggestions and guidance helped them describe their thoughts and reminded them to provide effective feedback. Looking across the deployments and experiments, adaptive suggestions and interactive guidance significantly improved feedback while static suggestions did not offer significant improvements. These techniques were embodied in the CritiqueKit system, used by 95 feedback providers and 336 recipients. Future work should examine applying other attributes of helpful feedback and further investigate how best to create, curate, and display adaptive suggestions. Much knowledge work features both underlying principles and context-specific knowledge of when and how to apply these principles. Potentially applicable feedback and review areas include domains as disparate as hiring and employee reviews, code reviews, product reviews, and reviews of academic papers, screenplays, business plans, and any other domain that blends context-specific creative choices with common genre structures. We hope that creativity support tools of all stripes will find value in the ideas and results presented here.
"
Environmental Factors in Indoor Navigation Based on Real-World Trajectories of Blind Users,https://dl.acm.org/authorize?N656524,"
Indoor localization technologies can enhance quality of life for blind people by enabling them to independently explore and navigate indoor environments. Researchers typically evaluate their systems in terms of localization accuracy and user behavior along planned routes. We propose two measures of path-following behavior: deviation from optimal route and trajectory variability. Through regression analysis of real-world trajectories from blind users, we identify relationships between a) these measures and b) elements of the environment, route characteristics, localization error, and instructional cues that users receive. Our results provide insights into path-following behavior for turn-by-turn indoor navigation and have implications for the design of future interactions. Moreover, our findings highlight the importance of reporting these environmental factors and route properties in similar studies. We present automated and scalable methods for their calculation and to encourage their reporting for better interpretation and comparison of results across future studies. This paper presents an analysis of factors contributing to trajectory variability and deviation in assistive indoor navigation for blind people. We provide evidence that characteristics of the physical environment affect successful navigation, in addition to generally acknowledged variables such as localization error of the system and instructional cues that users receive. For example, we show that users tend to adhere more to a path where scene and layout elements such stairs and pillars are not present (likely due to varying acoustic properties of the path). Moreover, we show that factors contributing to pathfollowing behavior calculated over noisy location estimates are well-aligned with factors revealed by annotated, groundtruth location data. Given the challenges in annotating blind users trajectories with sub-meter accuracy, we provide a scalable solution for data-driven analysis and evaluation within the field. Our findings highlight the importance of including and reporting these environmental factors in future studies. In future work, we plan to replicate this analysis with larger datasets from estimated trajectories of blind users across multiple environments. A larger dataset would also allow us to investigate how dynamic scene elements, e.g., presence of people, affect the performance of indoor navigation systems for blind users.
"
BSpeak: An Accessible Voice-based Crowdsourcing Marketplace for Low-Income Blind People,https://dl.acm.org/authorize?N656535,"
BSpeak is an accessible crowdsourcing marketplace that enables blind people in developing regions to earn money by transcribing audio files through speech. We examine accessibility and usability barriers that 15 first-time users, who are low-income and blind, experienced while completing transcription tasks on BSpeak and Mechanical Turk (MTurk). Our mixed-methods analysis revealed severe accessibility barriers in MTurk due to the absence of landmarks, unlabeled UI elements, and improper use of HTML headings. Compared to MTurk, participants found BSpeak significantly more accessible and usable, and completed tasks with higher accuracy in lesser time due to its voice-based implementation. In a two-week field deployment of BSpeak in India, 24 low-income blind users earned rupee 7,310 by completing over 16,000 transcription tasks to yield transcriptions with 87% accuracy. Through our analysis of BSpeak's strengths and weaknesses, we provide recommendations for designing crowdsourcing marketplaces for low-income blind people in resource-constrained settings. We recommend designers of mainstream crowdsourcing marketplaces to incorporate comprehensible terminologies, design user interfaces within accessibility guidelines, and add a filter to select accessible tasks. We recommend task requesters to provide simple and clear instructions, design accessible content on external webpages, and indicate upfront if a task is inaccessible for blind people. BSpeak demonstrated that a simple user interface, use of voice input, and untimed tasks could make a crowdsourcing marketplace more accessible for low-income blind people in resource-constrained settings.

"
Understanding Chatbot-mediated Task Management,https://dl.acm.org/authorize?N656536,"
Effective task management is essential to successful team collaboration. While the past decade has seen considerable innovation in systems that track and manage group tasks, these innovations have typically been outside of the principal communication channels: email, instant messenger, and group chat. Teams formulate, discuss, refine, assign, and track the progress of their collaborative tasks over electronic communication channels, yet they must leave these channels to update their task-tracking tools, creating a source of friction and inefficiency. To address this problem, we explore how bots might be used to mediate task management for individuals and teams. We deploy a prototype bot to eight different teams of information workers to help them create, assign, and keep track of tasks, all within their main communication channel. We derived seven insights for the design of future bots for coordinating work. In this paper we introduce TaskBot, a bot designed to help teams manage their tasks. Users delegate the tracking of their CHI 2018 Paper CHI 2018, April 21–26, 2018, Montréal, QC, Canada Paper 58 Page 4 tasks to TaskBot. We described our approach to designing TaskBot, and shared the lessons that we learned from deploying it with eight teams. We focused on identifying design considerations for other bot designers building conversational user interfaces for workplace. As for TaskBot, future work will focus on the following features: exploring the use of multiple communication channels (e.g. email, Skype, etc.), better handling of multi-threaded conversations, and more sophisticated ways of assigning tasks to people based on the task description.

"
Rich Representations of Visual Content for Screen Reader Users,https://dl.acm.org/authorize?N656537,"
Alt text (short for ""alternative text"") is descriptive text associated with an image in HTML and other document formats. Screen reader technologies speak the alt text aloud to people who are visually impaired. Introduced with HTML 2.0 in 1995, the alt attribute has not evolved despite significant changes in technology over the past two decades. In light of the expanding volume, purpose, and importance of digital imagery, we reflect on how alt text could be supplemented to offer a richer experience of visual content to screen reader users. Our contributions include articulating the design space of representations of visual content for screen reader users, prototypes illustrating several points within this design space, and evaluations of several of these new image representations with people who are blind. We close by discussing the implications of our taxonomy, prototypes, and user study findings. In this paper, we argued that the status quo experience of alt text, a standard that is more than two decades old, does not take advantage of the capabilities of modern computing technologies that could be used to provide a rich, immersive, and evocative experience of digital imagery for people who are blind. We articulated a taxonomy comprising five categories (interactivity, stability, representation, structure, and personalization) that can be used to create richer representations of visual content for screen reader users. We then introduced prototypes demonstrating six new experiences that supplement or transform standard alt text by combining different properties of this new design space. Finally, we presented detailed feedback on three of these novel “alt text” interactions from fourteen screen reader users.
"
"Let's Hate Together: How People Share News in Messaging, Social, and Public Networks",https://dl.acm.org/authorize?N656538,"
There are currently a wide variety of ways to share news with others: from sharing in a personal message, to sharing on a social network, to publicly posting. Through a survey with over one thousand people and an artifact analysis of 262 shared articles, we examine differences in motivations and frequency of sharing news on public, social and private platforms. We find that public sharing is more focused on spreading an ideology, while private sharing in messaging is dominated by stories inspired by the recipient's interests or context. The survey revealed three main groups of news sharing practices: those who shared to all channels (public, social, private), those who didn't share at all, and those who shared to private and social. The groups differed in their attitudes toward online discussion; those that shared the most were neutral and those that didn't share had negative attitudes about discussion online. We discuss sharing practices and implications for social systems that support sharing news. In this paper, we presented what we believe to be the first comprehensive look at news sharing behaviors across messaging apps, social media, and public platforms using an analysis of actual artifacts of sharing and a survey. By recruiting a broad sample of online participants from across the US, we were able to explore how a variety of online Americans share and don’t share news, digitally. Our research addressed differences in attitudes and motivations to share across different audiences and the different types and amounts of stories that were shared. Fewer people share news publicly, and on average they share less frequently than through social and private messaging. Those who share publicly have more faith in online discourse and are moderately worried about online conflict and those who never share are pessimistic about online discourse and less worried about conflict. Public shares have different news content: more political ideology and less information sharing. While we intended to study general sharing during a typical news cycle, data was collected and analyzed in an unprecedented high-intensity election year. It might be argued that our results are evidence that after the American election there continues to be heightened after-effects. We encourage other researchers to continue this work by conducting similar studies outside of the United States and over longer time periods in order to investigate regional or temporal differences in sharing and discussing the news. Whether done publicly or toward friends and family, we found that people use news to voice their values and ideology, especially when they believe others will listen and learn from it. People who only share with friends and family are more concerned about pitfalls of online discussion, and share information relevant to their relationships. By studying how people share news, and differences between sharing to different audiences through different platforms, we are understanding how people engage with this type of technology. Sharing links is an important part of the diffusion of news and these emotional, behavioral and technological mechanisms are believed to play a key part in swaying the 2016 US election [16]. This work aids researchers and designers to create systems that better support the sharing of news across a variety of platforms as well as engaging users to comment on news with the audiences that they choose.
"
Reinterpreting Schlemmer's Triadic Ballet: Interactive Costume for Unthinkable Movements,https://dl.acm.org/authorize?N656539,"
In the 1920s, Oskar Schlemmer, artist in the Bauhaus movement, created the Triadic Ballet costumes. These re-strict movement of dancers, creating new expressions. In-spired by this, we designed an interactive wire costume. It restricts lower body movements, and emphasizes arm movements spurring LED-light 'sparks' and 'waves' wired in a tutu-like costume. The Wire Costume was introduced to a dancer who found that an unusual bond emerged be-tween her and the costume. We discuss how sensory altera-tion (sight, kinesthetic awareness and proprioception) and bodily training to adjust to the new soma, can result in nov-el, evocative forms of expression. The interactive costume can foster a certain mood, introduce feelings, and even embody a whole character -- only revealed once worn and danced. We describe a design exploration combining cul-tural and historical research, interviews with experts and material explorations that culminated in a novel prototype. Sensory alteration appears to be crucial for novel forms of dancer’s expression. Obstruction of vision, metamorphosis of body shape and weight distribution, restriction of movements and means of expression opens a dancer for the new experience. Traditional ways of moving, expressing, communicating are disabled or made too complex to perform, providing a dancer and choreographer with a “tabula rasa”, a starting point to experiment with new forms of movement and expression, having an opportunity to create something new, which has not been performed before. At the same time, the restrictions literally “put” the dancer inside of the body of the character, telling its story, temperament, behavior kinesthetically, which is one of the most powerful catalysts of immersion. By affecting kinesthetic awareness and proprioception of the dancers, the costumes contribute to which movements are possible and desirable, therefore having an active role in the construction of the choreography. Our work provides the first empirical investigation into the effect of the Triadic Ballett’s costumes on dancer’s experience of movement, perception and embodied awareness. Furthermore, based on the gained understanding of the costumes’ ‘essence’ for experience, we developed a reinterpretation using interactive technology and tested this. Our modernized version manages to preserve the aesthetics of the original costume and, even more important, the experience inside it. In addition, the interactivity of the costume adds another degree of freedom in the dialog between the dancer and the costume. While the costume alters the soma of the dancer, the dancer is affecting back, by controlling the lighting effects of the costume. We believe that the HCI community can find inspiration from this ‘extreme case study’ of wearables and costumes, which highlights the costumes impact on bodily awareness and movement and shows how design choices (restrictions and limitations on movement) that could be discounted as ‘bad usability’ can be functional and might inspire new uses for wearables.
"
Exploring the Potential of Exergames to affect the Social and Daily Life of People with Dementia and their Caregivers,https://dl.acm.org/authorize?N656530,"
This paper presents the outcomes of an exploratory field study that examined the social impact of an ICT-based suite of exergames for people with dementia and their caregivers. Qualitative data was collected over a period of 8 months, during which time we studied the daily life of 14 people with dementia and their informal and professional caregivers. We focus on the experiential aspects of the system and examine its social impact when integrated into the daily routines of both people with dementia themselves and their professional and family caregivers. Our findings indicate that relatives were able to regain leisure time, whilst people with dementia were able to recapture certain aspects of their social and daily activities that might otherwise have been lost to them. Results suggest that the system enhanced social-interaction, invigorated relationships, and improved the empowerment of people with dementia and their caregivers to face daily challenges. The results from the study have illustrated that serious exergames are able to support the self-confidence and wellbeing of relevant stakeholders and enable people with dementia to face the challenges of a self-determined and dignified aging with dementia. With respect to the individual and social impacts of the system, it seemed to re-facilitate certain aspects of an autonomous lifestyle, such as the mobility related activities of daily living, respectively promoting them and maintaining independence as well as offsetting deficits. The results also suggest that the system enhanced social interaction and invigorated relationships, improving empowerment to face the daily challenges of people with dementia in their social surrounding. It thus seemed to improve the quality of life and well-being of people with dementia and their relatives. For professional caregivers in day-care settings, the study suggested that implementing such ICT-based systems might support workflows and thus improve institutionalized quality of care. Much still needs to be done to build a wider corpus of empirical findings regarding how people with dementia can be supported by the use of these kinds of systems. We also need to further understand the most effective mechanisms for ensuring that this kind of support can be fully integrated into the everyday lives of people with dementia. The aim of this exploratory study was to investigate the integration of a suite of ICT-based exergames into the daily routines of people with dementia and their informal and professional caregivers and to explore the social impacts and benefits of the system over a lengthy period of 8-months. A key feature here was the way in which the system was codesigned with relatives, professional caregivers and other stakeholders from related disciplines and the public sector. This process of open collaboration enabled us to reflect different kinds of knowledge, interests, and aspirations so that a mutually tailored and appropriate technical solution could be arrived at.
"
Grafter: Remixing 3D-Printed Machines,https://dl.acm.org/authorize?N656531,"
Creating new 3D printed objects by recombining models found in hobbyist repositories has been referred to as ""re-mixing"". In this paper, we explore how to best support users in remixing a specific class of 3D printed objects, namely those that perform mechanical functions. In our survey, we found that makers remix such machines by manually extracting parts from one parent model and combine it with parts from a different parent model. This approach often puts axles made by one maker into bearings made by another maker or combines a gear by one maker with a gear by a different maker. This approach is problem-atic, however, as parts from different makers tend to fit poorly, which results in long series of tweaks and test-prints until all parts finally work together. We address this with our interactive system grafter. Grafter does two things. First, grafter largely automates the process of extracting and recombining mechanical elements from 3D printed machines. Second, it enforces a more efficient approach to reuse: it prevents users from extracting indi-vidual parts, but instead affords extracting groups of me-chanical elements that already work together, such as axles and their bearings or pairs of gears. We call this mecha-nism-based remixing. In a final user study, all models that participants had remixed using grafter could be 3D printed without further tweaking and worked immediately. In this paper, we presented grafter, a tool that allows users to remix 3D printed mechanical machines. In addition to the system itself, our main contribution is that grafter does not follow common maker practice of extracting and re-assembling parts. Instead, grafter extracts self-contained mechanical units, aka mechanisms that remain functional throughout remixing. The main benefit for users is that grafter eliminates the need for tweaking and test printing. As future work, we plan on using the model graphs to make machine designs work across different printers and materials.

"
Depth Conflict Reduction for Stereo VR Video Interfaces,https://dl.acm.org/authorize?N656532,"
Applications for viewing and editing 360° video often render user interface (UI) elements on top of the video. For stereoscopic video, in which the perceived depth varies over the image, the perceived depth of the video can conflict with that of the UI elements, creating discomfort and making it hard to shift focus. To address this problem, we explore two new techniques that adjust the UI rendering based on the video content. The first technique dynamically adjusts the perceived depth of the UI to avoid depth conflict, and the second blurs the video in a halo around the UI. We conduct a user study to assess the effectiveness of these techniques in two stereoscopic VR video tasks: video watching with subtitles, and video search. We explore depth conflicts between UI and stereoscopic video and discuss how they can affect user experience in VR video interfaces. We present two techniques to address this problem. Dynamic Depth detects and reduces depth conflicts by analyzing the video content and adjusting the depth of UI widgets to the depth of the video. Halo Blur simply blurs the video around the UI. We evaluated these techniques in a preliminary user study with two video tasks: watching video with subtitles and video searching. Our study compares our techniques with a baseline condition where the UI is fixed at a comfortable distance in VR. Our results suggest that the severity of depth conflict problems might depend on the task characteristics and the UI design. It also shows that Dynamic Depth is a promising solution and was most preferred by our participants for video subtitles. Our results also show that Halo Blur did not work as expected in a dynamic VR video environment.

"
"From Pulse Trains to ""Coloring with Vibrations"": Motion Mappings for Mid-Air Haptic Textures",https://dl.acm.org/authorize?N656533,"
Can we experience haptic textures in mid-air? Typically, the experience of texture is caused by vibration of the fingertip as it moves over the surface of an object. This object's surface also guides the finger's movement, creating an implicit motion-to-vibration mapping. If we wish to simulate a texture in mid-air, such guidance does not exist, making the choice of motion-to-vibration mapping non-obvious. We evaluate the experience of moving a pointer with four different motion-to vibration mappings in an interview study. We found that some mappings lead to a perception shift, transforming the experience. When this occurs, the pointer is no longer perceived as vibrating, interactions become more pleasurable, and users have an increased experience of agency and control. We discuss how to leverage this in the design of haptic interfaces. So, can we experience haptic textures in mid-air? We found that, based on mapping, experiences very similar to texture can be created. If an object just vibrates without reacting to movement, it is experienced as a device, such as a toothbrush or vibrating smartphone. If an object vibrates based on where it is pointed, it feels more useful, but still like a device – maybe a Geiger counter or metal detector. In our Rotation and Translation condition, however, the way the vibrations were experienced transformed, leading to a material experience related to texture. These textures are more pleasing than ‘traditional’ vibration and make moving a device more interesting – as if it had higher material quality. Systems using mid-air textures can provide users with a stronger experience of agency and a better sense of control when interacting with them.

"
Collaborative Dynamic Queries: Supporting Distributed Small Group Decision-making,https://dl.acm.org/authorize?N656534,"
Communication is critical in small group decision-making processes during which each member must be able to express preferences to reach consensus. Finding consensus can be difficult when each member in a group has a perspective that potentially conflicts with those of others. To support groups attempting to harmonize diverse preferences, we propose Collaborative Dynamic Queries (C-DQ), a UI component that enables a group to filter queries over decision criteria while being aware of others' preferences. To understand how C-DQ affects a group's behavior and perception in the decision-making process, we conducted 2 studies with groups who were prompted to make decisions together on mobile devices in a dispersed and synchronous situation. In Study 1, we found showing group preferences with C-DQ helped groups to communicate more efficiently and effectively. In Study 2, we found filtering candidates based on each member's own filter range further improved a groups' communication efficiency and effectiveness. We examined the role for C-DQ to function as a moderator in small group decision-making and found initial evidence that visually externalized group awareness can support a group in making an agreeable and satisfactory decision with reduced cost for communication. We also identified that the way in which the system handles each member’s filter ranges may incur some different effects in different use context. We anticipate that our work sets up the possibility for a deeper understanding of different designs (e.g., group awareness visualization strategies that can consider better privacy, easier persuasion, or indicating different degree of importance each criterion) for C-DQ that may work in various situations (e.g., different use context, different platforms, different size of a group).
"
Crowdsourcing Rural Network Maintenance and Repair via Network Messaging,https://dl.acm.org/authorize?N656545,"
Repair and maintenance requirements limit the successful operation of rural infrastructure. Current best practices are centralized management, which requires travel from urban areas and is prohibitively expensive, or intensively training community members, which limits scaling. We explore an alternative model: crowdsourcing repair from the community. Leveraging a Community Cellular Network in the remote Philippines, we sent SMS to all active network subscribers (n = 63) requesting technical support. From the pool of physical respondents, we explored their ability to repair through mock failures and conducted semi-structured interviews about their experiences with repair. We learned that community members would be eager to practice repair if allowed, would network to recruit more expertise, and seemingly have the collective capacity to resolve some common failures. They are most successful when repairs map directly to their lived experiences. We suggest infrastructure design considerations that could make repairs more tractable and argue for an inclusive approach. In this work we explored the idea of “crowdsourcing” repair knowledge and ability for cell network repair from the local community using SMS messages. Working with a rural community in San Andres, Rizal, Philippines, we sent all active network subscribers (N = 63) an SMS asking them to come to the cell site to help fix a technical issue. 24 of these community members responded, of whom 18 were women. We then asked those who came to try to resolve three mock equipment failures. We found that nearly all were able to solve a simple solar panel issue, many had some capability to solve an antenna alignment issue, and only a few were able to correctly address an overheating CPU. Exploring further, we found that the biggest barriers to local repair were concerns about authority or liability. We believe that these results demonstrate a large latent capacity for local repair in rural areas, especially for equipment similar to consumer electronics used at home.

"
Interactive Interior and Proxemics Thresholds: Empowering Participants in Sensitive Conversations,https://dl.acm.org/authorize?N656546,"
The position and workings of interactive interior elements matter greatly on the relations people may enact. This paper reports on the conception and evaluation of an interactive table and its interior effects designed to support sensitive consultations between healthcare personnel, patients and relatives as they happen during treatment of cancer diseases in a hospital department of oncology. The interior design includes the physical shape of artefact, its digital functionality and how the seating around it is to take place. The design of the table is substantiated through observations of current practice, framing of the design challenge, conceptualization, and exploring form giving alternatives. Through a set of evaluations in actual use settings it is argued how the design concept of the table as interactive interior points to how notions in interaction proxemics should be rearticulated. In particular, this paper argues how proxemics thresholds should be regarded as dynamic and relational. In this paper, we presented a design intervention comprised of a prototype interactive table and its seating arrangement serving as an interior design element regulating the way sensitive consultations in an oncology department are carried out. The interactive table serves as control and structuring mechanism for both the conversation and the audio recording of it. The prototype has been designed in a Research-through-Design/ Constructive Design Research process where field studies, user workshops and theoretical concepts from interaction proxemics have informed the design. A set of use trials in-the-wild revealed that when used according to design intentions the design intervention resulted in successful consultation, and when misused, patients were left frustrated and pacified. The analysis of the use trials pointed to notion that Proxemics Thresholds as identified by Krogh et al. [16] should be understood as experientially personal, dynamic and relational dependent on the exercise of power by users.
"
Blocks4All: Overcoming Accessibility Barriers to Blocks Programming for Children with Visual Impairments,https://dl.acm.org/authorize?N656547,"
Blocks-based programming environments are a popular tool to teach children to program, but they rely heavily on visual metaphors and are therefore not fully accessible for children with visual impairments. We evaluated existing blocks-based environments and identified five major accessibility barriers for visually impaired users. We explored techniques to overcome these barriers in an interview with a teacher of the visually impaired and formative studies on a touchscreen blocks-based environment with five children with visual impairments. We distill our findings on usable touchscreen interactions into guidelines for designers of blocks-based environments. We conducted an evaluation of current blocks-based environments and found five accessibility barriers. We designed multiple techniques to overcome these barriers and conducted a formative study to evaluate these techniques with five children with visual impairments. We distilled the findings from this study into a final design, which we plan to evaluate formally, and a set of design guidelines for designers of these applications.

"
What's at Stake: Characterizing Risk Perceptions of Emerging Technologies,https://dl.acm.org/authorize?N656548,"
One contributing factor to how people choose to use technology is their perceptions of associated risk. In order to explore this influence, we adapted a survey instrument from risk perception literature to assess mental models of users and technologists around risks of emerging, data-driven technologies (e.g., identity theft, personalized filter bubbles). We surveyed 175 individuals for comparative and individual assessments of risk, including characterizations using psychological factors. We report our observations around group differences (e.g., expert versus non-expert) in how people assess risk, and what factors may structure their conceptions of technological harm. Our findings suggest that technologists see these risks as posing a bigger threat to society than do non-experts. Moreover, across groups, participants did not see technological risks as voluntarily assumed. Differences in how people characterize risk have implications for the future of design, decision-making, and public communications, which we discuss through a lens we call risk-sensitive design. The above study applied an instrument from the risk perception literature to analyze thinking about emerging technologies. We found that generally users do not think of risk exposure from technology to be voluntary. There were also considerable differences in how risk is perceived by experts and the lay public, which may explain why problems such as the filter bubble, have become so concerning. Our paper ends with a discussion of risk sensitive design, hoping to provoke continued conversation about how technologists should respond when there are large gaps in how the public and experts think about risk. We hope to see HCI researchers continue study in this area and advance the conversation further.
"
TaskCam: Designing and Testing an Open Tool for Cultural Probes Studies,https://dl.acm.org/authorize?N656549,"
TaskCams are simple digital cameras intended to serve as a tool for Cultural Probe studies and made available by the Interaction Research Studio via open-source distribution. In conjunction with an associated website, instructions and videos, they represent a novel strategy for disseminating and facilitating a research methodology. At the same time, they provide a myriad of options for customisation and modification, allowing researchers to adopt and adapt them to their needs. In the first part of this paper, the design team describes the rationale and design of the TaskCams and the tactics developed to make them publicly available. In the second part, the story is taken up by designers from the Everyday Design Studio, who assembled their own TaskCams and customised them extensively for a Cultural Probe study they ran for an ongoing project. Rather than discussing the results of their study, we focus on how their experiences reveal some of the issues both in producing and using open-source products such as these. These suggest the potential of TaskCams to support design-led user studies more generally.

"
Making Problems in Design Research: The Case of Teen Shoplifters on Tumblr,https://dl.acm.org/authorize?N656540,"
HCI draws on a variety of traditions but recently there have been calls to consolidate contributions around the problems researchers set out to solve. However, with this comes the assumption that problems are tractable and certain, rather than constructed and framed by researchers. We take as a case study a Tumblr community of teen shoplifters who post on how to steal from stores, discuss shoplifting as political resistance, and share jokes and stories about the practice. We construct three different ""problems"" and imagine studies that might result from applying different design approaches: Design Against Crime; Critical Design and Value Sensitive Design. Through these studies we highlight how interpretations of the same data can lead to radically different design responses. We conclude by discussing problem making as a historically and politically contingent process that allow researchers to connect data and design according to certain moral and ethical principles. We have argued that framing a problem is not a natural consequence of working with data but rather an act of creative making that shapes design outcomes and satisfies expectations. Whether a particular perspective is explicitly stated or not there are no neutral design responses to data. We have illustrated this argument with three different constructions of “problems” relating to the same data from an online community of teen shoplifters. Our examples are not intended to cover the whole HCI research field but rather, provide a glimpse into the potential to generate diversity in reponse to data and to acknowledge the specific interests and assumptions of any particular research audience. Problems and solutions are not “out there” waiting to be identified but are rather the product of particular analytical lenses that foreground certain connections between data and design. In turn, there are no inherent or inevitable relationships between data and design but rather historically and politically informed choices.
"
Group vs Individual: Impact of TOUCH and TILT Cross-Device Interactions on Mixed-Focus Collaboration,https://dl.acm.org/authorize?N656541,"
Cross-device environments (XDEs) have been developed to support a multitude of collaborative activities. Yet, little is known about how different cross-device interaction techniques impact group collaboration, including how their impact on independent and joint work that often occurs during group work. In this work, we explore the impact of two XDE data browsing techniques: TOUCH and TILT. Through a mixed-methods study of a collaborative sensemaking task, we show that TOUCH and TILT have distinct impacts on how groups accomplish, and shift between, independent and joint work. Finally, we reflect on these findings and how they can more generally inform the design of XDEs. We presented results from an exploratory, laboratory-based study in which pairs of participants performed a series of collaborative sensemaking tasks using two cross-device data browsing techniques: TOUCH and TILT. Our qualitative analyses show that cross-device interaction techniques can profoundly influence collaborative process. While TILT facilitated access to out-of-reach data, especially during independent data browsing, TOUCH better supported tightly synchronized discussion of data. Further investigation is warranted to determine how best to balance these competing group needs in XDEs. In particular, they point to a need to better understand how techniques support individual and joint work, but also transitions between the two modes.

"
Back to Analogue: Self-Reporting for Parkinson's Disease,https://dl.acm.org/authorize?N656542,"
We report the process used to create artefacts for self-reporting Parkinson's Disease symptoms. Our premise was that a technology-based approach would provide participants with an effective, flexible, and resilient technique. After testing four prototypes using Bluetooth, NFC, and a microcontroller we accomplished almost full compliance and high acceptance using a paper diary to track day-to-day fluctuations over 49 days. This diary is tailored to each patient's condition, does not require any handwriting, allows for implicit reminders, provides recording flexibility, and its answers can be encoded automatically. We share five design implications for future Parkinson's self-reporting artefacts: reduce participant completion demand, design to offset the effect of tremor on input, enable implicit reminders, design for positive and negative consequences of increased awareness of symptoms, and consider the effects of handwritten notes in compliance, encoding burden, and data quality. During the agile prototyping of the different self-reporting tools described above, we found out that pen and paper are a suitable method to collect longitudinal day-to-day fluctuations. P1-7’s feedback provided us with a detailed view of the problems and virtues of our prototypes that were the base to conduct each redesign. Although our decision process was tied up to a set of constraints, a button-based or app-based tool might be the right choice under different circumstances [8]. For example, when the chosen device does not interfere with other aspects of the monitoring study. Therefore, in line with other authors, we do not advocate the use of either analogue or electronic approaches as a silver bullet, but rather recommend tailoring either option to the study’s goals. We highlight the attributes that favour our paper diary compared to some technologies. It is cheap at £3.5 per unit, accessible, frictionless, personalised, portable, low-demand, automatically encoded, straightforward and flexible. These are all characteristics that make it a suitable tool for ground truth data collection in our parent study.
"
The Hide and Seek of Workspace: Towards Human-Centric Sustainable Architecture,https://dl.acm.org/authorize?N656543,"
This contribution exemplifies how the study of space perception and its impact on space-use behavior can inform sustainable architecture. We describe our attempt to integrate the methods of user research in an architectural project that was focused on optimization of space usage. In an office building, two large office rooms were refurbished to provide desk-sharing opportunities through hot-desking. We studied the space-use behavior of 33 office workers over eight weeks in those two rooms as well as their occasional presence in ten other areas (cafeteria, atrium, meeting rooms, etc.). Quantitative and qualitative analyses were performed to understand the nature and nuances of space occupancy at the scope of the building and within the refurbished offices. While at the scope of building the patterns of movements between rooms were found to be related to the professional profile of the users, at the scope of office the occupancy patterns were influenced by the spatial design of workspaces. More precisely, certain visual attributes of a workspace, namely Visual Exposure and Visual Openness, could determine whether or not it was regularly used. In this paper, we describe our findings in detail and discuss their implications for sustainable building design. Our contributions as HCI researchers have complemented and guided the architectural project objectives and procedures at different stages of the project. In addition, our involvement with both the inhabitants and stakeholders coalesced their respective perceptions and expectations towards the establishment of a homogeneous framework for dialogue. With the stakeholders, we collaboratively framed the problem related to the less than optimal use of spaces, followed by the identification of intensification as a potential solution. Besides, the outcomes of co-design sessions and the user evaluation of refurbished offices were subjected to participatory analyses and discussions with the stakeholders. Furthermore, our role in engaging the occupants to collectively devise strategies for the sustainable use of space manifested in the redesign of the Connective and Calm rooms, followed by their evaluation by the occupants. We experienced that it is only through such continual confluent measures that the human factors can be profoundly assimilated and acknowledged as an essential component in the processes of problem formulation and solution seeking in sustainable architecture.

"
"How Teens with Visual Impairments Take, Edit, and Share Photos on Social Media",https://dl.acm.org/authorize?N656544,"
We contribute a qualitative investigation of how teens with visual impairments (VIP) access smartphone photography, from the time they take photos through editing and sharing them on social media. We observed that they largely want to engage with photos visually, similarly to their sighted peers, and have developed strategies around photo capture, editing, sharing, and consumption that attempt to mitigate usability limitations of current photography and social media apps. We demonstrate the need for more work examining how young people with low vision engage with smartphone photography and social media, as they are heavy users of such technologies and have challenges distinct from their totally blind counterparts. We conclude with design considerations to alleviate the usability barriers we uncovered and for making smartphone photography and social media more accessible and relevant for VIPs. In this paper, we presented a qualitative investigation of how visually impaired teens engage with smartphone photography and social media platforms popular among their peers. Though they encounter challenges using these apps, they leverage several compensatory strategies that inspired our design recommendations. Through our interviews with visually impaired teens, we aimed not to provide representative experiences of this user group, but rich accounts that push back on misconceptions about how people with visual impairments experience photography. We learned that teens who are blind or low vision are not disinterested in photography. In fact, they were interested in engaging with photos visually as much as possible, as P4 succinctly described, “I’m a really visual person for being visually impaired.” Despite visual impairment, these teens enjoy smartphone photography and social media apps popular among their peers, even when they are premised on photos and ephemerality. We hope these findings serve as motivation for other types of studies on how visually impaired people, and particularly young people who are heavy technology users, engage with popular technologies and social media.
"
Attending to Slowness and Temporality with Olly and Slow Game: A Design Inquiry Into Supporting Longer-Term Relations with Everyday Computational Objects,https://dl.acm.org/authorize?N656655,"
Slowness has emerged as a rich lens to frame HCI investigations into supporting longer-term human-technology relations. Yet, there is a need to further address how we design for slowness on conceptual and practical levels. Drawing on the concepts of unawareness, intersections, and ensembles, we contribute an investigation into designing for slowness and temporality grounded in design practice through two cases: Olly and Slow Game. We designed these artifacts over two and a half years with careful attention to how the set of concepts influenced key design decisions in terms of their form, materials, and computational qualities. Our designer-researcher approach revealed that, when put into practice, the concepts helped generatively grapple with slowness and temporality, but are in need of further development to be mobilized for design. We critically reflect on insights emerging across our practice-based research to reflexively refine the concepts and better support future HCI research and practice. We have described and critically reflected on our collective practice of designing and making Olly and Slow Game. Our aim was to generatively inquire into how unawareness, intersections, and ensembles could offer conceptual scaffolding for grappling with slowness and temporality in design. We closely attended to how this set of concepts influenced key design decisions that structured and expressed time across the materiality, form, and computational qualities of our design artifacts. Through a reflexive designer-researcher approach, our work provides insights into how the quality of independence generated through unawareness could enable design artifacts to evoke a rich and unique slower temporal expression. Intersections and ensembles helped operationalize unawareness through refocusing our attention beyond interactivity toward subtle design qualities that, over time, could give rise to a wider range of relations among people, things, and environments. Insights from our research revealed that these concepts need not be treated so rigidly when applied in practice. In summary, the more flexible, revised vision of these concepts offer promise to be scaffolded in future efforts to design for slowness and temporality by shifting the primacy of what we attend to in design: (i) for materials, focus shifts to how they evoke and will persist through time beyond how they feel ‘now’; (ii) for physical form, prominence is given to decisions that explicitly invite encounters with other things as well as other people; (iii) for computation, emphasis expands beyond immediate response time to crafting a temporal pacing that is distinct, indeterminate, and ongoing. Importantly, our aim is not to be prescriptive or conclusive. A multiplicity of approaches is needed to open up new ways of conceptualizing and designing diverse expressions of time as technology increasingly becomes embedded in everyday life. We hope our work will inspire future HCI research and practice initiatives into designing for slowness and temporality. More generally, we hope the critical reflective reporting of our designer-researcher approach can be appreciated as an effort to better support design-oriented forms of knowledge production in the HCI community.
"
VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval,https://dl.acm.org/authorize?N656656,"
We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects. We propose VirtualGrasp, an object retrieval approach for VR applications. With VirtualGrasp, a user retrieves a virtual object by performing an in-air gesture which he/she uses to grasp or interact with its physical counterpart. Through three studies, we evaluated the consistency of object-gesture mappings across users, the expressivity of the grasping gestures and the performance of retrieving objects using this approach. Results have confirmed that users could reach high agreement on the mappings, that the object-gesture pairs could be accurately recognized by algorithms, and that users could discover the gestures by themselves and also enjoyed the experience. We also discuss the design implications, potential applications and limitations of this approach.
"
Harvesting Caregiving Knowledge: Design Considerations for Integrating Volunteer Input in Dementia Care,https://dl.acm.org/authorize?N656657,"
Improving volunteer performance leads to better caregiving in dementia care settings. However, caregiving knowledge systems have been focused on eliciting and sharing expert, primary caregiver knowledge, rather than volunteer-provided knowledge. Through the use of an experience prototype, we explored the content of volunteer caregiver knowledge and identified ways in which such non-expert knowledge can be useful to dementia care. By using lay language, sharing information specific to the client and collaboratively finding strategies for interaction, volunteers were able to boost the effectiveness of future volunteers. Therapists who reviewed the content affirmed the reliability of volunteer caregiver knowledge and placed value on its recency, variety and its ability to help bridge language and professional barriers. We discuss how future systems designed for eliciting and sharing volunteer caregiver knowledge can be used to promote better dementia care. In this paper, we describe volunteer-provided content in terms of its structure and mechanisms for iterative improvement of content quality. Despite using the insights of inexperienced, untrained volunteers, we show that the collaborative sharing of such content provides sufficient knowledge for successive volunteers to be more effective when interacting with people with dementia. Additionally, we developed a sense of the relative value of non-expert, non-kin input within the dementia care context. We offered design considerations for further optimizing the contribution of volunteer-provided content to dementia care.
"
Gamification for Self-Tracking: From World of Warcraft to the Design of Personal Informatics Systems,https://dl.acm.org/authorize?N656658,"
World of Warcraft (WoW) may be a source of inspiration to enrich the Personal Informatics systems user's experience and, at the same time, improve gamification design. Through the findings of a four-year reflexive ethnography in WoW, I outline how its game design elements support players in making sense of their own data, emphasizing how ""game numbers"" are turned into meanings. On the basis of the study results, I propose a series of design considerations to be used in the design of self-tracking systems, which recommend to embody data into digital entities, provide different analytical tools depending on the users' expertise through a flexible model, and foster the formation of ""communities of practice"" in order to support learning processes. In this work, I made two main contributions. First, I described how WoW’s players learn to manage the quantitative information needed to play the game at best, how they ascribe meaning to numbers, and how they turn them into action. Second I provided a series of design considerations to transfer WoW’s best practices in supporting players making sense of their data to PI domain.
"
Pinpointing: Precise Head- and Eye-Based Target Selection for Augmented Reality,https://dl.acm.org/authorize?N656659,"
Head and eye movement can be leveraged to improve the user's interaction repertoire for wearable displays. Head movements are deliberate and accurate, and provide the current state-of-the-art pointing technique. Eye gaze can potentially be faster and more ergonomic, but suffers from low accuracy due to calibration errors and drift of wearable eye-tracking sensors. This work investigates precise, multimodal selection techniques using head motion and eye gaze. A comparison of speed and pointing accuracy reveals the relative merits of each method, including the achievable target size for robust selection. We demonstrate and discuss example applications for augmented reality, including compact menus with deep structure, and a proof-of-concept method for on-line correction of calibration drift. In summary, this work has taken a close look at a variety of multimodal techniques for precision target selection in AR. We investigated both eye gaze and head pointing combined with refinement provided by a handheld device, hand gesture input and scaled head motion. A user study showed trade-offs of different variations of Pinpointing. Confirming previous work, eye gaze input alone is faster than head pointing, but the head pointing allows greater targeting accuracy. A previously unexplored use of scaled head refinement proved to be the most accurate, although participants primarily preferred device input and found gestures required the most effort. We further demonstrated two applications for Pinpointing, compact menu selection and online correction of eye gaze calibration. Further work is required to reproduce these techniques on a wider variety of device hardware and to explore more sophisticated applications such as situated analytics and in-situ CAD.

"
Sketch&Stitch: Interactive Embroidery for E-textiles,https://dl.acm.org/authorize?N656650,"
E-Textiles are fabrics that integrate electronic circuits and components. Makers use them to create interactive clothing, furniture, and toys. However, this requires significant manual labor and skills, and using technology-centric design tools. We introduce Sketch&Stitch, an interactive embroidery system to create e-textiles using a traditional crafting approach: Users draw their art and circuit directly on fabric using colored pens. The system takes a picture of the sketch, converts it to embroidery patterns, and sends them to an embroidery machine. Alternating between sketching and stitching, users build and test their design incrementally. Sketch&Stitch features Circuitry Stickers representing circuit boards, components, and custom stitch patterns for wire crossings to insulate, and various textile touch sensors such as pushbuttons, sliders, and 2D touchpads. Circuitry Stickers serve as placeholders during design. Using computer vision, they are recognized and replaced later in the appropriate embroidery phases. We close with technical considerations and application examples. This paper introduced Sketch&Stitch, an interactive system for creating e-textiles quickly and iteratively. It enables a new design workflow that combines physical sketching with an embroidery machine, offering users the benefits of direct making and the power of digital tools. It implements a digitization algorithm for converting a sketch to an embroidery. We described novel stitch patterns for attaching electronics, shielding wire crossings, and integrating sensors (pushbuttons, sliders, touchpads) directly on fabric. An empirical evaluation of the technical stitches was presented. Finally, we demonstrated the potential and advantages of our workflow.

"
Using Stakeholder Theory to Examine Drivers' Stake in Uber,https://dl.acm.org/authorize?N656651,"
Uber is a ride-sharing platform that is part of the 'gig-economy,' where the platform supports and coordinates a labor market in which there are a large number of ephemeral, piecemeal jobs. Despite numerous efforts to understand the impacts of these platforms and their algorithms on Uber drivers, how to better serve and support drivers with these platforms remains an open challenge. In this paper, we frame Uber through the lens of Stakeholder Theory to highlight drivers' position in the workplace, which helps inform the design of a more ethical and effective platform. To this end, we analyzed Uber drivers' forum discussions about their lived experiences of working with the Uber platform. We identify and discuss the impact of the stakes that drivers have in relation to both the Uber corporation and their passengers, and look at how these stakes impact both the platform and drivers' practices.

"
"Conversations in the Eye of the Storm: At-Scale Features of Conversational Structure in a High-Tempo, High-Stakes Microblogging Environment",https://dl.acm.org/authorize?N656652,"
This work propels social media research beyond the single post as the unit of analysis toward fuller treatment of interaction by making the construct of the conversation analytically available. We offer a method for constructing @reply conversations in Twitter to apprehend social media conversational features at scale. We apply this method to the high-tempo, high-stakes environment of 2012's Hurricane Sandy, with its high volume of online talk by affected locals and distinct disaster-stage phasing by which to consider interactional difference. We investigate the temporality of conversations; the relationality of who speaks to whom; the number and kind of conversationalists; and how content affects temporal features. The analysis reveals that, during the height of the emergency, people expand conversations both in number and kind of conversational partners-just as their information search intensifies. This expansion contributes to longer, slower-paced conversations in the high-emergency period, suggesting reliance on online relationships during times of greatest uncertainty. This work expands social media research from analyzing single Twitter posts to considering whole conversations by placing monologues, dialogues, and group discussions of twitterers who are geographically vulnerable at the analytical center. In offering approaches for making conversations analytically available, this work contributes to social computing research for crisis informatics that can now investigate more nuanced questions about the social roles that twitterers play, how they draw on social capital in protective decision making, and how online conversations interweave with the activities on the ground.

"
Interactive Extraction of Examples from Existing Code,https://dl.acm.org/authorize?N656653,"
Programmers frequently learn from examples produced and shared by other programmers. However, it can be challenging and time-consuming to produce concise, working code examples. We conducted a formative study where 12 participants made examples based on their own code. This revealed a key hurdle: making meaningful simplifications without introducing errors. Based on this insight, we designed a mixed-initiative tool, CodeScoop, to help programmers extract executable, simplified code from existing code. CodeScoop enables programmers to ""scoop"" out a relevant subset of code. Techniques include selectively including control structures and recording an execution trace that allows authors to substitute literal values for code and variables. In a controlled study with 19 participants, CodeScoop helped programmers extract executable code examples with the intended behavior more easily than with a standard code editor. We developed CodeScoop, a mixed-initiative interaction technique to enable programmers to extract executable code examples from existing code. Our study shows that programmers can use such tools to successfully extract examples. Furthermore, the resulting “scoops” provide value over automatically extracted slices. We believe tools like CodeScoop will ultimately enable programmers to more quickly and effectively share their knowledge through examples.

"
"Haptic Revolver: Touch, Shear, Texture, and Shape Rendering on a Reconfigurable Virtual Reality Controller",https://dl.acm.org/authorize?N656654,"
We present Haptic Revolver, a handheld virtual reality controller that renders fingertip haptics when interacting with virtual surfaces. Haptic Revolver's core haptic element is an actuated wheel that raises and lowers underneath the finger to render contact with a virtual surface. As the user's finger moves along the surface of an object, the controller spins the wheel to render shear forces and motion under the fingertip. The wheel is interchangeable and can contain physical textures, shapes, edges, or active elements to provide different sensations to the user. Because the controller is spatially tracked, these physical features can be spatially registered with the geometry of the virtual environment and rendered on-demand. We evaluated Haptic Revolver in two studies to understand how wheel speed and direction impact perceived realism. We also report qualitative feedback from users who explored three application scenarios with our controller. Haptic Revolver is a general-purpose handheld VR controller that goes beyond vibrotactile stimulation to render touch contact with virtual surfaces, motion along a surface, textures, and shapes using interchangeable haptic wheels. By customizing wheels for the virtual environment, designers can use Haptic Revolver to render realistic haptic feedback on the fngertip. We demonstrated techniques to render motion along a surface in two dimensions and adapt a particular wheel for use in arbitrary scenes. We conducted two user studies to inform and validate the design of our haptic rendering techniques and a third study to elicit qualitative feedback from participants. We believe that Haptic Revolver offers high-fdelity haptic rendering with clear advantages over vibrotactile solutions and we hope others will build upon our design to continue enabling better haptic experiences for VR.

"
Influences of Human Cognition and Visual Behavior on Password Strength during Picture Password Composition,https://dl.acm.org/authorize?N656665,"
Visual attention, search, processing and comprehension are important cognitive tasks during a graphical password composition activity. Aiming to shed light on whether individual differences on visual behavior affect the strength of the created passwords, we conducted an eye-tracking study (N=36), and adopted an accredited cognitive style theory to interpret the results. The analysis revealed that users with different cognitive styles followed different patterns of visual behavior which affected the strength of the created passwords. Motivated, by the results of the first study, we introduced adaptive characteristics to the user authentication mechanism, aiming to assist specific cognitive style user groups to create more secure passwords, and conducted a second study with a new sample (N=40) to test the adaptive characteristics. Results strengthen our assumptions that adaptive mechanisms based on users' differences in cognitive and visual behavior uncover a new perspective for improving the password's strength within graphical user authentication realms. In this paper, we first reported the results of an eye-tracking study aiming to investigate the effects of FD-I cognitive style on the created passwords’ strength using a cued recall GUA scheme and explain the results considering the visual behavior. Significant differences were revealed between the created passwords’ strength of FDs and FIs, and on the visual behavior of FDs and FIs which were strongly correlated with the passwords’ strength. Hence, this paper provides evidence that users with different cognitive style follow different strategies when creating graphical passwords on images of varying complexity and their visual behavior suggests whether their choices will lead to strong passwords. Triggered by the results of the first study, we designed an assistive mechanism based on the visual behavior of the FDs and the FIs, which we used as saliency mask on the same background images, and conducted a comparative study. Results reveal that the approach we suggested improved the created passwords’ strength, reinforcing our assumption that adaptive mechanisms based on the cognitive styles, can provide a feasible solution for creating stronger passwords. Therefore, this work provides evidence that the cognitive styles of the users can be used to provide personalized experiences. The results of our study are summarized in Table 2. We are encouraged by the results of our work that using cognitive styles for designing personalized assistive features for GUA mechanisms is worth further exploration, and we are eager to design and evaluate more cognitive style-based features to better support the users when creating graphical passwords. Apart from password creation, we intend to investigate the effects of cognitive styles in login, and design and test different adaptive policies. Considering the shift towards immersive technologies and the increasing role of the eye in such environments, there is room for expanding our research in this context.
"
Supporting Workplace Detachment and Reattachment with Conversational Intelligence,https://dl.acm.org/authorize?N656666,"
Research has shown that productivity is mediated by an individual's ability to detach from their work at the end of the day and reattach with it when they return the next day. In this paper we explore the extent to which structured dialogues, focused on individuals' work-related tasks or emotions, can help them with the detachment and reattachment processes. Our inquiry is driven with SwitchBot, a conversational bot which engages with workers at the start and end of their work day. After preliminarily validating the design of a detachment and reattachment dialogue frame-work with 108 crowdworkers, we study SwitchBot's use in-situ for 14 days with 34 information workers. We find that workers send fewer e-mails after work hours and spend a larger percentage of their first hour at work using productivity applications than they normally would when using SwitchBot. Further, we find that productivity gains were better sustained when conversations focused on work-related emotions. Our results suggest that conversational bots can be effective tools for aiding workplace detachment and reattachment and help people make successful use of their time on and off the job. In this study, we reported findings from an in-situ study that indicate bots can be effective tools for helping information workers detach from and reattach with work. We introduced a conversational detachment-reattachment framework in which we included two, unique models of dialogue for detaching from work and reattaching with work. We presented and evaluated SwitchBot, a bot that implements the detachment-reattachment framework. We showed evidence that suggests interacting with SwitchBot before the start and end of the workday assists information workers in psychologically detaching from work and reattaching with work the next day. Future work includes studying non-information workers, examining more hybrid models of dialogue, and examining how bots can be tools for transitioning between tasks in the workplace as well as at home.
"
iTurk: Turning Passive Haptics into Active Haptics by Making Users Reconfigure Props in Virtual Reality,https://dl.acm.org/authorize?N656667,"
We present a system that complements virtual reality experiences with passive props, yet still allows modifying the virtual world at runtime. The main contribution of our system is that it does not require any actuators; instead, our system employs the user to reconfigure and actuate otherwise passive props. We demonstrate a foldable prop that users reconfigure to represent a suitcase, fuse cabinet, railing, and a seat. A second prop, suspended from a long pendulum, not only stands in for inanimate objects, but also for objects that move and demonstrate proactive behavior, such as a group of flying droids that physically attack the user. Our approach conveys a sense of a living, animate world, when in reality the user is the only animate entity present in the system, complemented with only one or two physical props. In our study, participants rated their experience as more enjoyable and realistic than a corresponding no-haptics condition. In this paper, we presented iTurk, a system that complements virtual reality experiences with passive props, yet still allows modifying the virtual world at runtime. The main contribution behind iTurk is the idea of using users to reconfigure and animate otherwise passive props in virtual reality. First, user-based reconfiguration of props allows creating arbitrary sequences of rooms, each of which reuses the same physical space and the physical props. Second, animated props, such as the pendulum, allow rendering animate objects, which brings liveliness to the resulting virtual worlds. This allows iTurk to realize some of the benefits that have traditionally only been achieved with passive haptics such as actuated either by mechanical actuators (Robotic graphics [6] or human actuators (TurkDeck [2]). As future work, we are planning on exploring self-reconfiguring props based on energy harvesting.

"
"Clusters, Trends, and Outliers: How Immersive Technologies Can Facilitate the Collaborative Analysis of Multidimensional Data",https://dl.acm.org/authorize?N656668,"
Immersive technologies such as augmented reality devices are opening up a new design space for the visual analysis of data. This paper studies the potential of an augmented reality environment for the purpose of collaborative analysis of multidimensional, abstract data. We present ART, a collaborative analysis tool to visualize multidimensional data in augmented reality using an interactive, 3D parallel coordinates visualization. The visualization is anchored to a touch-sensitive tabletop, benefiting from well-established interaction techniques. The results of group-based, expert walkthroughs show that ART can facilitate immersion in the data, a fluid analysis process, and collaboration. Based on the results, we provide a set of guidelines and discuss future research areas to foster the development of immersive technologies as tools for the collaborative analysis of multidimensional data. In this work we address the challenge of collaboratively analyzing multidimensional, abstract data. We present ART, a system that leverages the strengths of AR environments. The stereoscopic presentation in large-scale space allows for egocentric navigation and can therefore reduce the effects of overplotting when large data sets are visualized. The AR environments further allow for natural communication and coordination between collaborators. ART combines the interaction on a touch-sensitive tabletop with a 3D visualization in AR above the tabletop. This combination of AR with touch input potentially enhances interaction over a gesture-based system due to its familiar, precise, physically undemanding, and fluid interaction. The results of group-based expert walkthroughs show that ART can facilitate immersion in the data, a fluid analysis process, and collaboration. The interaction based on touch input allowed participants to fluidly operate the feature-rich systems, from which the integrated features provided the means to analyze multidimensional clusters, trends, and outliers. Based on these findings, we provide guidelines for the design of interactive visualizations using immersive technologies. In addition, we identify research directions to further facilitate the collaborative analysis of multidimensional data in AR environments.

"
Methods for Evaluation of Imperfect Captioning Tools by Deaf or Hard-of-Hearing Users at Different Reading Literacy Levels,https://dl.acm.org/authorize?N656669,"
As Automatic Speech Recognition (ASR) improves in accuracy, it may become useful for transcribing spoken text in real-time for Deaf and Hard-of-Hearing (DHH) individuals. To quantify users' comprehension and opinion of automatic captions, which inevitably contain some errors, we must identify appropriate methodologies for evaluation studies with DHH users, including quantitative measurement instruments suitable to the various literacy levels among the DHH population. A literature review guided our selection of several probes (e.g. multiple-choice comprehension-question accuracy or response time, scalar-questions about user estimation of ASR errors or their impact, users' numerical estimation of accuracy), which we evaluated in a lab study with DHH users, wherein their literacy levels and the actual accuracy of each caption stimulus were factors. For some probes, participants with lower literacy had more positive subjective responses overall, and, for participants with particular literacy score ranges, some probes were insufficiently sensitive to distinguish between caption accuracy levels. Through a lab study with 107 DHH participants evaluating ASR-based captions of various known accuracy levels, we have compared the scores recorded from various question types. We found that some question types are effective at distinguishing accuracy levels of captions, for responses collected from participants within particular literacy ranges. We also found that for some question types, participants with higher literacy levels give more critical subjective responses and have higher comprehension question scores. Based on this, we provide the following methodological recommendations to researchers conducting evaluations of captioning technology in studies with DHH participants: • Because the literacy level of participants influences their response scores, researchers should report the literacy level of the participants in their studies – or control for this factor through reported screening criteria. • If participants’ literacy levels are diverse, researchers should report results separately for those at each level. • When selecting questions to include in the study, researchers should consider the literacy level of their participants and the accuracy level of their caption stimuli, to select an appropriate question type capable of discriminating caption quality (i.e. consult Table 1). • Given that question types may differ in their ability to distinguish caption quality at different accuracy levels, researchers should consider their caption accuracy, e.g. using metrics such as WER, and report these values. • Since comprehension question scores are based on several factors, including: the quality of the original text, the difficulty of the question, and the reading skill level of the respondent – such questions must be appropriately tailored to the participants’ literacy skills. For instance, participants with above-average fluency may better infer the erroneous/missing information in captions. There were several limitations to our study: We chose the ubiquitous WER as an accuracy metric, but newer metrics that weigh the importance of each word may better represent accuracy perceptions of DHH users [19]. Our caption stimuli consisted of a single genre (simulation of a meeting) with a script at a particular reading level (as measured by Flesch-Kincaid’s formulas); however, we did not experiment with genres or reading-levels of stimuli in this study, and doing so might reveal unforeseen effects. Further, while we used WRAT to group participants into literacy levels, with our lowest group including the lowest range of WRAT scores, we could have recruited greater numbers of participants with high literacy levels, to enable more subdivision at the higher end of the WRAT scale – or utilized a different literacy test with greater ability to distinguish individuals at the extremes of the literacy scale. Our study included participants recruited from a university campus; researchers should use caution when generalizing our results to other DHH users, e.g. younger students or older adults in the workforce. Finally, our study did not include all possible question-type probes, e.g. open-ended questions or eye-tracking metrics, as in some prior work. This methodological study has been conducted as part of a larger research project at our lab to investigate the use of ASR-based captioning for use by DHH individuals in oneon-one meetings with hearing colleagues, e.g. in workplace settings. In future work, we plan to design and develop a prototype system, which we will evaluate in user studies. We will make use of the methodological guidelines above, and we may generate revisions to them based on our experiences in conducting further studies with DHH users.
"
Effects of Individual Differences in Blocking Workplace Distractions,https://dl.acm.org/authorize?N656660,"
Information workers are experiencing ever-increasing online distractions in the workplace, and software to block distractions is becoming more popular. We conducted an exploratory field study with 32 information workers in their workplace using software to block online distractions for one week. We discovered that with online distractions blocked, participants assessed their focus and productivity to be significantly higher. Those who benefited most were those who reported being less in control of their work, associated with personality traits of lower Conscientiousness and Lack of Perseverence. Unexpectedly, those reporting higher control of work experienced a cost of higher workload with online distractions blocked. Those who reported the greatest increase in focus with distractions blocked were those who were more susceptible to social media distractions. Without distractions, people with higher control of work worked longer stretches without physical breaks, with consequently higher stress. We present design recommendations to promote focus for our observed coping behaviors. Online distractions are a controversial aspect of our current technology-mediated workplaces. A number of commercial solutions, prototypes and workplace policies have been developed to address the perceived negative aspects of online distractions. However, whether these strategies help has not been well studied. Rather than ask people hypothetically to report experiences, our participants actually experienced how blocking distractions affected their work. For most, cutting off workplace distractions increases focus and productivity. Our study is a first step in suggesting that individual differences in managing distractions is important to consider in designing software to block distractions. People who benefited the most were those who felt less in control of computer work and most susceptible to social distractions. However, our results on the role of personality need to be verified with further research. As we construct workplaces of the future, we need to understand how technology support can help people better manage their distractions, so as to integrate them more beneficially into their work practices.
"
Dynamic Demographics: Lessons from a Large-Scale Census of Performative Possibilities in Games,https://dl.acm.org/authorize?N656661,"
While much popular discussion of representation in games exists, there is very little rigorously collected data available from which to draw direct conclusions. In this study, we set out to address this gap by performing a census of playable characters across a large sample of contemporary games. We gathered data from 200 games including independently published (""indie"") games and so-called ""AAA"" titles from large publishers. While our initial analysis yielded some insight into the landscape of playable characters, it also highlighted the contingent, negotiated, and interpretive nature of representation in games. This led to additional analysis that emphasized the ways in which this negotiation manifests in research in the methods and metrics used to quantify representation. We argue that researchers studying representation in games need to treat it as a possibility space for a multitude of potential interpretations rather than a singular, measurable, phenomenon. Nakamura describes the concept of “menu-driven identities” to indicate how structurally and functionally imposed limitations narrow “the field of representation” and “work to deny the existence of ways of being raced that don’t fit neatly into categorizable boxes” [24]. While Nakamura’s focus is race, she includes gender in this analysis as well. Nakamura argues how menus serve to constrain possibility, rather than expand it. We might adapt Nakamura’s concept by considering “menu-driven bodies,” or “menu-driven performances,” as we are not expressly able to address the identities of either the characters or the players in a study like this, only the possibility space within which identity work may occur. We too are concerned with the ways that race, gender, and more do not fit neatly into boxes, despite our own, and many other scholars’ attempts at classification, and quantification. Any discussion of representation in media, especially as it pertains to race and gender carries a risk of poorly translating rigid, marginalizing categorizations from broader culture to the virtual. As we transform our research, we attempt to mitigate this risk by describing characters only as collective, probabilistic, interpretations. For the next steps of this research, rather than coding each character according to a single, or even small consensus reading, we are developing an online tool that enables collaborative, composite readings. Instead of a certain character being recorded as White, Black, Latinx, or Asian, etc., they are recorded as being read White X%, Latinx Y% by those who have contributed their judgements through an online survey-based tool. We may not be able to rely on the perspective of only one or two researchers, but we can invite others to collaborate. This data becomes about so much more than race, or any single identifier as we collect information on apparent sex and gender, ability, age, etc. We live in a world where these identities are complex, contingent, and constructed, and any attempt at capturing that prismatic reality must refrain from foreclosing upon them. In this paper we have provided our reading on a corpus of playable characters that illustrates a certain perspective on the landscape of representation in these games. But, perhaps more importantly, this collection of interpretations and the methodological shift away from inter-rater reliability and toward inter-rater variability that we suggest based on it, should help not only our ongoing research as we continue to develop it, but scholars interested in related work.
"
Philosophers Living with the Tilting Bowl,https://dl.acm.org/authorize?N656662,"
This paper reports on a postphenomenological inquiry of six trained philosophers, who as study participants lived with and reflected on a research product we designed known as the Tilting Bowl: a ceramic bowl that unpredictably but gently tilts multiple times daily. The Tilting Bowl is a counterfactual artifact that is designed specifically for this study as part of a material speculation approach to design research. A postphenomenological inquiry looks to describe and analyze accounts of relationships between humans and technological artifacts, and how each mutually shapes the other through mediations that form the human subjectivity and objectivity of any given situation. This paper contributes an empirical account and analysis of the relations that emerged (background and alterity) and the relativistic views that co-constitute the philosophers, Tilting Bowl, and their specific worlds. The findings demonstrate the relevance of this philosophical framing to fundamentally and broadly understand how people engage digital artifacts. An aim of this study is to enhance our philosophical understanding of digital artifacts through exploring alternatives with a counterfactual artifact and lived-with reflections of trained philosophers. We provided descriptive and analytical accounts of background and alterity relations shared with the Tilting Bowl that focus on different qualities of presence within these relations. We provide detailed and rich relativistic views of our philosophers, Tilting Bowl, and their specific worlds that account for technologies in new ways for HCI. Additionally, we detailed the characteristics of our material speculation and co-speculation approach in the light of the postphenomenological studies, which we see as having great potential to be extended with the support of other HCI and design research methods. More generally, we aimed to further a mutually beneficial dialogue between HCI and postphenomenology.
"
LumiWatch: On-Arm Projected Graphics and Touch Input,https://dl.acm.org/authorize?N656663,"
Compact, worn computers with projected, on-skin touch interfaces have been a long-standing yet elusive goal, largely written off as science fiction. Such devices offer the potential to mitigate the significant human input/output bottleneck inherent in worn devices with small screens. In this work, we present the first fully functional and self-contained projection smartwatch implementation, containing the requisite compute, power, projection and touch-sensing capabilities. Our watch offers roughly 40 sq. cm of interactive surface area -- more than five times that of a typical smartwatch display. We demonstrate continuous 2D finger tracking with interactive, rectified graphics, transforming the arm into a touchscreen. We discuss our hardware and software implementation, as well as evaluation results regarding touch accuracy and projection visibility. In this paper, we have presented LumiWatch, a first-of-itskind smartwatch with integrated projector and touch sensing in a commercially viable form factor. Developing this prototype required solving a number of difficult problems, including development of a suitable projector module, shallow-angle projection onto curved arms, and accurate 2D finger tracking. Through a combination of custom hardware and software, our prototype smartwatch provides a large touchscreen-like interface directly on a wearer’s arm. LumiWatch presents a novel combination of hardware and software capabilities, moving the vision of on-skin interaction significantly closer to reality and illuminating the imminent feasibility of projection-enabled wearables.

"
Hit-or-Wait: Coordinating Opportunistic Low-effort Contributions to Achieve Global Outcomes in On-the-go Crowdsourcing,https://dl.acm.org/authorize?N656664,"
We consider the challenge of motivating and coordinating large numbers of people to contribute to solving local, communal problems through their existing routines. In order to design such ""on-the-go crowdsourcing"" systems, there is a need for mechanisms that can effectively coordinate contributions to address problem solving needs in the physical world while leveraging people's existing mobility with minimal disruption. We thus introduce Hit-or-Wait, a general decision-theoretic mechanism that intelligently controls decisions over when to notify a person of a task, in ways that reason both about system needs across tasks and about a helper's changing patterns of mobility. Through simulations and a field study in the context of community-based lost-and-found, we demonstrate that using Hit-or-Wait enables a system to make efficient use of people's contributions with minimal disruptions to their routines without the need for explicit coordination. Interviews with field study participants further suggest that highlighting an individual's contribution to the global goal may help people value their contributions more.
"
An Empirical Exploration of Mindfulness Design Using Solo Travel Domain,https://dl.acm.org/authorize?N656675,"
Despite recent popularity of mindfulness smartphone applications and an interest in incorporating mindfulness into new technologies, existing applications tend to focus mainly on its meditation dimension. In this paper, we review existing literature on digital and traditional mindfulness to map its design space and synthesize the findings with our prior research on designing for aesthetic needs. We identify ""recollection"" and ""evaluation"" as two important dimensions of mindfulness that have not yet been incorporated into popular digital tools. Through a two-phase design activity over 16 months, we developed ColorAway, an innovative tool that promotes mindfulness through interaction with modified travel photos. Recruited participants evaluated ColorAway and offered unique insights into how mindfulness can be better designed. We also discuss how the process of designing for mindfulness can possibly inform the design of personal technology. This research is part of a larger study that builds on scholarly research and theories with the goal of designing interactive technologies for solo travelers. This research is a small contribution to an ongoing effort to study the design of technology to support mindfulness, in which we identified and focused on two relatively unexplored dimensions of mindfulness: “recollection” and “evaluation.” We presented the results of a qualitative analysis that explored new opportunities to design for these two dimensions. Our study consisted of two design phases based on the data from the larger study. In Phase 1, we designed ColorAway using aesthetics principles. In this phase, we noticed a connection between aesthetics and mindfulness, which led us to redesign ColorAway in Phase 2 based on the two dimensions of mindfulness. We discussed the challenges of designing for mindfulness and suggested important design considerations. We initially named the project ColorAway, envisioning that a coloring book for adults may be a fun way to satisfy their aesthetic needs. However, after Phase 2, we realized that the project name and the paper prototype in the shape of a coloring book might have influenced participants’ interactions with the photos. For future developments, we would change the name to something that wouldn’t restrict users to only think of it as a coloring book. We also believe that turning the paper prototype into a digital format can address this concern as well. In addition, abstracting the photos by leaving all the outlines naturally guided and restricted some of the participants to follow the lines and fill them out. In the future developments, we’d like to experiment with other photo alteration techniques to leave more room for creativity and imagination, including activities other than drawing and coloring. We will also look for better ways to engage users in selecting their photos while lowering the burden of going through all their photos. Our goal was to gain insights from the iterative HumanCentered Design (HCD) process to both understand and inform the design of personal technology. We suggest that (a) carefully engaging users in the process of collecting data, (b) setting no explicit goals for users to achieve, and (c) providing minimal instructions to allow for creativity and playfulness, might promote broader adoption of personal technology by addressing some of the challenges and mental burden identified in other research [6, 18]. Our findings led us to ask: How can we design personal technology to support users without limiting them to achieve specific goals? How would this be different from other applications that require achieving specific goals? We plan to conduct future research to investigate these questions as they also fit well within our larger study. We believe this project helped identify the connections between mindfulness, aesthetic needs, and aesthetic principles. We explored potential technologies that combine these three concepts. We also identified future research directions in developing new technologies to support mindfulness through the use of photos. Given the growing popularity of photography, the omnipresence of smartphone cameras, and new applications such as Instagram and Snapchat, we believe there is potential for digital technology to leverage photography to support mindfulness while satisfying users’ needs for aesthetical experience.
"
Inspiring AWE: Transforming Clinic Waiting Rooms into Informal Learning Environments with Active Waiting Education,https://dl.acm.org/authorize?N656676,"
This research explores patient education in pediatric hematology and oncology clinics. Based on interviews, observations, and a review of existing patient materials, we argue that education in clinic waiting rooms is in need of reform. We applied design principles from research in science museums along with tangible interaction techniques to create the Sickle Cell Station, an interactive learning experience about sickle cell disease. To evaluate the effectiveness of this design we observed approximately 580 participants in a pediatric hematology clinic waiting area in four different design conditions. These observations included detailed video analysis of 81 patients and their parents to understand their interaction and learning with the Sickle Cell Station. Our results show an engaging learning experience with relevant conversation, inquiry, and collaboration. We describe how patient engagement varied in the four design conditions and conclude with implications for new designs in the area of Active Waiting Education (AWE). Waiting room education has had static form and progression for decades. Museums, on the other hand, have spent years formulating designs and experiences that engage users, and are inviting and effective. We used the APE and FLP learning standards in addition to Montessori design concepts to create a station that plays to the waiting room community. The design aims to inspire patients’ intrinsic motivations to use their wait time to learn about relevant life-dependent issues. The vessel pulled in more users than any other form of material. With the exception of retention, all rates for the vessel show improvement over other material types - especially in regards to young children with average dwell times well over museum standards. We see inquiry and exploration, reading quietly and aloud, reading to answer questions, and reading because the poster is there, and commentary and explanations of the exhibit parts and their behaviors. Users mapped the poster information to their work with the vessel and vice versa, tying in what they were reading to encourage their explorations. The conversations among participant groups were rich with biological content, covering blood composition, definitions of blood cells, clots, the heart beat, and blood transfusions to name a few. We heard many examples of personal associations, primarily relating the blood vessel to veins in the child’s own body, but also to friends with sickle cell and even chemotherapy. Users demonstrated an understanding that sickle cells are stiff and that this causes blockages in the blood vessels. Beyond the potential collaborations and learning experiences with the station, children enjoyed using it, showing verbal confirmations of fun, and dwell times well over the APE goals. Naturalistic studies show substantial increases in interest from patients groups using interactive technologies for learning. Interaction analysis show learning through inquiry, exploration, and collaboration with adults, children, family, and strangers that mimic those found in museum literature. This evidence supports the idea of Active Waiting Education as a means to improve patient involvement in their own health education.
"
Object Manipulation in Virtual Reality Under Increasing Levels of Translational Gain,https://dl.acm.org/authorize?N656677,"
Room-scale Virtual Reality (VR) has become an affordable consumer reality, with applications ranging from entertainment to productivity. However, the limited physical space available for room-scale VR in the typical home or office environment poses a significant problem. To solve this, physical spaces can be extended by amplifying the mapping of physical to virtual movement (translational gain). Although amplified movement has been used since the earliest days of VR, little is known about how it influences reach-based interactions with virtual objects, now a standard feature of consumer VR. Consequently, this paper explores the picking and placing of virtual objects in VR for the first time, with translational gains of between 1x (a one-to-one mapping of a 3.5m*3.5m virtual space to the same sized physical space) and 3x (10.5m*10.5m virtual mapped to 3.5m*3.5m physical). Results show that reaching accuracy is maintained for up to 2x gain, however going beyond this diminishes accuracy and increases simulator sickness and perceived workload. We suggest gain levels of 1.5x to 1.75x can be utilized without compromising the usability of a VR task, significantly expanding the bounds of interactive room-scale VR. The physical space available for room-scale Virtual Reality (VR) is limited by common home or office spaces. Utilising translational gain, the size of fully explorable virtual environments can be quickly and easily expanded with existing hardware. For the first time, this research has shown how these movement amplifications impact common object interactions such as reaching and picking. This paper has explored how the picking and placing of virtual objects in VR is affected by translational gains of up to 3.0x the physical movement. We found that reaching accuracy is maintained at up to 2x gain, however, going beyond this diminishes accuracy and increases simulator sickness and perceived workload. We have provided guidelines regarding how translational gain can be utilized by practitioners without compromising the VR user’s capability to interact with their virtual surroundings.

"
Watch Me Play: Does Social Facilitation Apply to Digital Games?,https://dl.acm.org/authorize?N656678,"
The presence of observers and virtual characters can significantly shape our gaming experience. Researchers suppose that most of the basic socio-psychological phenomena are also applicable for digital games. However, the social processes in gaming setups can differ from our experience in other social situations. Our work emphasizes that awareness. Insights are needed for the purposeful design of a game's social setting, specifically in applied contexts of learning and training. Here, we focus on the social facilitation effect, which describes an unconscious change in performance due to the presence of others, by investigating the impact of real observers and virtual agents on player experience and performance in four different games. The results of our four studies show that, in contrast to previous assumptions, in-game success was not significantly influenced by the presence of any social entity, indicating that social facilitation does not generally apply to the context of playing digital games. Our four studies do not provide evidence for the effectiveness of social facilitation in the context of digital gaming. Neither a virtual agent nor the presence of a co-located observer during play has significantly influenced the performance of players. Although this finding is not generalizable to all kinds of gaming situations, we have discussed several reasons why the social facilitation effect is supposed to be less influential in most gaming scenarios compared to other social situations due to the special characteristics of digital games. These assumptions illustrate possible directions of future work in this area and will be addressed in follow-up studies. The different theoretical approaches to explain social facilitation (arousal, attention, social valuation) can be further illuminated by measuring the physiological arousal of players while facing different challenges, testing their level of distraction as well as their feeling of being evaluated. Furthermore, the effect should be tested in other social settings with observers with different characteristics, which can be systematically varied according to the factors depicted in our model (see Figure 1). Our work contributes to the understanding of social processes in digital games and provide guidance for game designers and researchers who seek to take advantage of social effects like social facilitation in their games. This is particularly useful in applied areas like digital game-based learning. Furthermore, our results demonstrate that the applicability of basic sociopsychological phenomena to digital games has to be proven before being used as rationale for game design decisions. Otherwise, false assumptions regarding social effects may lead to game designs that induce unintended experiences or unwanted player behavior.
"
Do You Think What I Think: Perceptions of Delayed Instant Messages in Computer-Mediated Communication of Romantic Relations,https://dl.acm.org/authorize?N656679,"
In romantic relationships, Instant Messaging (IM) can serve as a communication channel to maintain a sense of mutual presence and relational closeness when being physically separated. However, IM is asynchronous by design. There can exist time delay for people to receive and reply to incoming messages, which may violate romantic partner's mutual expectation. Limited understanding is available around how unintended and intended delays affect the relationship of romantic partners. This work examines how romantic partners grow, perceive, and use mutual knowledge about each other in delayed IM to resolve the expectancy violation. We conducted a 7-day diary study on 16 pairs of romantic couples and used the diary entries as probes for post-study one-on-one interviews. Our findings show that couples employ different strategies of information grounding to parse and resolve delayed IM. Based on these findings, we propose several theoretical and practical implications. In this paper, we explore grounding mechanisms between romantic couples in delayed IM communication, and through the theoretical lens of expectancy violation theory. We conducted a 7-day diary study on 16 pairs of romantic couples and post one-on-one interviews. Our findings show that couples employ different strategies of information grounding to parse and resolve delayed IM, including status grounding, process grounding, and context grounding. Different sources of grounding provide cues to resolve expectancy violations between romantic partners. Based on our findings, we present research and design implications to understand and support romantic communication.

"
From Research to Practice: Informing the Design of Autism Support Smart Technology,https://dl.acm.org/authorize?N656670,"
Smart technologies (wearable and mobile devices) show tremendous potential in the detection, diagnosis, and management of Autism Spectrum Disorder (ASD) by enabling continuous real-time data collection, identifying effective treatment strategies, and supporting intervention design and delivery. Though promising, effective utilization of smart technology in aiding ASD is still limited. We propose a set of implications to guide the design of ASD-support technology by analyzing 149 peer-reviewed articles focused on children with autism from ACM Digital Library, IEEE Xplore, and PubMed. Our analysis reveals that technology should facilitate real-time detection and identification of points-of-interest, adapt its behavior driven by the real-time affective state of the user, utilize familiar and unfamiliar features depending on user-context, and aid in revealing even minuscule progress made by children with autism. Our findings indicate that such technology should strive to blend-in with everyday objects. Moreover, gradual exposure and desensitization may facilitate successful adaptation of novel technology. We investigated how to design effective ASD-support smart technologies for children. To uncover needs, challenges and opportunities, we conducted an extensive literature review on ACM DL, IEEE Xplore, and PubMed. Our findings indicate that smart technologies have been utilized to support different areas pertaining to ASD - early detection of autism, gaze and facial expression recognition and understanding, affect detection, identifying and managing repetitive and stereotypical behavior, and improving social, educational, and learning skills. Despite this wide coverage of clinical challenges unique to ASD, we believe smart technologies are still under-utilized. Specifically, such technologies were rarely used for continuous real-time data collection, monitoring, and intervention, in the natural environment and/or for longer periods. We also found that at present ASD-research is primarily conducted in the developed countries. As autism is a global phenomenon and developing countries lack advanced technologies and trained professionals [96, 134], we argue that developing countries deserve the same, if not more, attention from the ASD research community. We propose a set of actionable guidelines collectively drawing from our findings, research on assistive technology design, and our experience of working with related stakeholders. We believe that to be effective, smart technologies should facilitate real-time collection, detection, and identification of events-of-interest such as the beginning of a lapse episode or distress triggered by a specific context. These technologies should be capable of adapting their behavior in real-time based on user’s preference, affective state, and user and environmental context. Smart technologies should also act as analytical tools, facilitating capture, detection, and highlighting even small progress made by children with autism. We argue that to be effective, smart technologies need to be inconspicuous and low-burden. We believe that our findings and insights would lead to better ASD-support smart technologies.

"
Explanations as Mechanisms for Supporting Algorithmic Transparency,https://dl.acm.org/authorize?N656671,"
Transparency can empower users to make informed choices about how they use an algorithmic decision-making system and judge its potential consequences. However, transparency is often conceptualized by the outcomes it is intended to bring about, not the specifics of mechanisms to achieve those outcomes. We conducted an online experiment focusing on how different ways of explaining Facebook's News Feed algorithm might affect participants' beliefs and judgments about the News Feed. We found that all explanations caused participants to become more aware of how the system works, and helped them to determine whether the system is biased and if they can control what they see. The explanations were less effective for helping participants evaluate the correctness of the system's output, and form opinions about how sensible and consistent its behavior is. We present implications for the design of transparency mechanisms in algorithmic decision-making systems based on these results.
"
On the Design of OLO Radio: Investigating Metadata as a Design Material,https://dl.acm.org/authorize?N656672,"
With the massive adoption of music streaming services globally, metadata is being generated that captures people's music listening histories in more precise detail than ever before. These metadata archives offer a valuable and overlooked resource for designing new ways of supporting people in experiencing the music they have listened to over the course of their lives. Yet, little research has demonstrated how metadata can be applied as a material in design practice. We describe the design of OLO Radio, a device that leverages music listening history metadata to support experiences of exploring and living with music from one's past. We unpack and reflect on design choices that made use of the exacting precision captured in listening history metadata archives to support relatively imprecise qualities of feedback and interaction to encourage rich, open-ended experiences of contemplation, curiosity, and enjoyment over time. We conclude with implications for HCI research and practice in this space. Through grounding our research in the creation of a highly finished design artifact, our work contributes concrete insights into a design approach that responds to growing calls in HCI to design technologies capable of: (i) sustaining longer-term experiences [19,33], (ii) opening new possibilities for forming relations to our personal data in everyday life [8,51], and (iii) expressing more diverse perspectives on temporality through design [32,44,54]. Insights from our design process revealed the practical need for richer, more expressive tools to support interaction designers in making use of metadata archives as a design material early in the exploratory stages of the design process. Our work also suggests opportunities for future HCI research to design new ways of temporally organizing, manifesting, and engaging with other forms of personal content and their attendant metadata (e.g., photos, video, digital messages, etc.) in everyday life. Beyond personally generated content, this approach could equally help extend nascent and growing design efforts exploring how dynamic ready-made data (e.g., related to climate [26], seismic [47], or transportation patterns [16]) could be meaningfully situated in people’s everyday lives over time. Importantly, our aim is not to be prescriptive or conclusive. Rather, we intend to unpack the OLO Radio design artifact in the service of inspiring and framing future designoriented work inquiring into the role, place, and pace of digital data in everyday life. In our future work, we aim to produce low-volume productions of OLO Radio to explore people’s experiences with it over time in their everyday lives. On a broader level, we hope that our detailed unpacking the design of OLO Radio and discussion of the resulting implications can be appreciated as an effort to better support design-oriented forms of knowledge production in the HCI community.
"
"Engage Early, Correct More: How Journalists Participate in False Rumors Online during Crisis Events",https://dl.acm.org/authorize?N656673,"
Journalists are struggling to adapt to new conditions of news production and simultaneously encountering criticism for their role in spreading misinformation. Against the backdrop of this ""crisis in journalism"", this research seeks to understand how journalists are actually participating in the spread and correction of online rumors. We compare the engagement behaviors of journalists to non-journalists- and specifically other high visibility users-within five false rumors that spread on Twitter during three crisis events. Our findings show journalists engaging earlier than non-journalists in the spread and the correction of false rumors. However, compared to other users, journalists are (proportionally) more likely to deny false rumors. Journalists are also more likely to author original tweets and to be retweeted-underscoring their continued role in shaping the news. Interestingly, journalists scored high on ""power user"" measures, but were distinct from other power users in significant ways-e.g. by being more likely to deny rumors. In this research, we compare the rumoring behaviors of journalists and non-journalists. Our analyses revealed that journalists and non-journalists differ in terms of the content they produce, their propensity to correct false rumors, and their temporal patterns of participation. Journalists produced significantly more original tweets than non-journalists. Journalists were more likely to deny a false rumor, an effect that held up even when we controlled for power user status. Journalists also tended to engage in both affirming and denying a rumor earlier than other users. These findings suggest that journalists remain a distinct professional community of practice within the social media crowd. Though we did not find consistent evidence to support journalists utilizing the “publish first, then correct” strategy (or the more traditional “confirm first, then publish” one), when compared with other users we did see an “engage earlier, correct more” strategy. These findings demonstrate journalists both struggling to adapt to the new conditions of real-time news reporting and continuing to serve a distinguished role in reporting breaking news and helping to correct misinformation.
"
Measuring the “Why” of Interaction: Development and Validation of the User Motivation Inventory (UMI),https://dl.acm.org/authorize?N656674,"
Motivation is a fundamental concept in understanding people's experiences and behavior. Yet, motivation to engage with an interactive system has received only limited attention in HCI. We report the development and validation of the User Motivation Inventory (UMI). The UMI is an 18-item multidimensional measure of motivation, rooted in self-determination theory (SDT). It is designed to measure intrinsic motivation, integrated, identified, introjected, and external regulation, as well as amotivation. Results of two studies (total N = 941) confirm the six-factor structure of the UMI with high reliability, as well as convergent and discriminant validity of each subscale. Relationships with core concepts such as need satisfaction, vitality, and usability were studied. Additionally, the UMI was found to detect differences in motivation for people who consider abandoning a technology compared to those who do not question their use. The central role of motivation in users' behavior and experience is discussed. We present the development and validation of a multidimensional measurement tool rooted in self-determination theory that helps to deepen our understanding of why users interact with a technology. The development and validation followed best practices and all data collected in the two studies together with the materials and analysis code is available online. The UMI has been tested with over 900 participants and shows promising psychometric properties, high reliability, convergent and discriminant validity. The UMI has implications for theory and practice and opens up opportunities for future research on motivation and user experience.
"
GestureWiz: A Human-Powered Gesture Design Environment for User Interface Prototypes,https://dl.acm.org/authorize?N656685,"
Designers and researchers often rely on simple gesture recognizers like Wobbrock et al.'s $1 for rapid user interface prototypes. However, most existing recognizers are limited to a particular input modality and/or pre-trained set of gestures, and cannot be easily combined with other recognizers. In particular, creating prototypes that employ advanced touch and mid-air gestures still requires significant technical experience and programming skill. Inspired by $1's easy, cheap, and flexible design, we present the GestureWiz prototyping environment that provides designers with an integrated solution for gesture definition, conflict checking, and real-time recognition by employing human recognizers in a Wizard of Oz manner. We present a series of experiments with designers and crowds to show that GestureWiz can perform with reasonable accuracy and latency. We demonstrate advantages of GestureWiz when recreating gesture-based interfaces from the literature and conducting a study with 12 interaction designers that prototyped a multimodal interface with support for a wide range of novel gestures in about 45 minutes. We have presented the GestureWiz prototyping environment that provides designers with an integrated solution for gesture definition, conflict checking, and recognition in nearly realtime using Wizard of Oz or crowdsourcing set-ups. Informed by initial experiments with crowd workers, we developed optimized interfaces for requesters and workers and suitable strategies for recognition. To prove the feasibility and effectiveness of our approach, we prototyped and tested three applications featuring different input modalities and kinds of gestures, which would typically require multiple gesture recognition libraries and significant programming skills. An explorative study with 12 interaction designers revealed benefits and limitations of our approach. Participants generally appreciated the possibility to quickly and easily define and test new gestures, as well as the integrated conflict resolution functionality and enjoyed using GestureWiz. All of the interaction designers, in pairs of two, were able to create and test gesture sets from scratch and have them recognized by a WOZ and crowd workers in about 45 minutes. Code and data are available at https://github.com/mi2lab/gesturewiz. Our larger vision behind this work is to enable interactive surfaces and spaces anytime, anywhere. GestureWiz provides an important first step towards this vision. It is now possible to walk into an interactive room, experiment with different kinds of non-trivial gestures and test them on the spot, without having to write a single line of gesture recognition code. In the future, it will be interesting to formalize the gesture definition and recognition approach using crowds to support guessability studies while enabling quick testing and revision using GestureWiz. On the crowdsourcing side, future work needs to investigate how to mitigate the shortcomings related to accuracy and latency that were revealed during the user study. Another direction of work we intend to address is making use of the data provided by WOZ and crowds to automatically train corresponding machine learning models, thus facilitating recognition after the prototyping stage.

"
In Search of the Dream Team: Temporally Constrained Multi-Armed Bandits for Identifying Effective Team Structures,https://dl.acm.org/authorize?N656686,"
Team structures---roles, norms, and interaction patterns---define how teams work. HCI researchers have theorized ideal team structures and built systems nudging teams towards them, such as those increasing turn-taking, deliberation, and knowledge distribution. However, organizational behavior research argues against the existence of universally ideal structures. Teams are diverse and excel under different structures: while one team might flourish under hierarchical leadership and a critical culture, another will flounder. In this paper, we present DreamTeam: a system that explores a large space of possible team structures to identify effective structures for each team based on observable feedback. To avoid overwhelming teams with too many changes, DreamTeam introduces multi-armed bandits with temporal constraints: an algorithm that manages the timing of exploration--exploitation trade-offs across multiple bandits simultaneously. A field experiment demonstrated that DreamTeam teams outperformed self-managing teams by 38%, manager-led teams by 46%, and teams with unconstrained bandits by 41%. This research advances computation as a powerful partner in establishing effective teamwork. Effective teamwork is a wicked problem [48]: it cannot be planned in advance, and requires adaptation to the people, task, and environment. Prior work focused on pre-selecting effective structures, such as convening the right members or using the right collaborative system. Acknowledging that this correct identification is impossible in the limit, we introduce a system that helps teams adaptively identify a set of roles, norms, and interaction patterns that is effective for them. To enable DreamTeam, we contribute a model for multi-armed bandits with temporal constraints, which ensures that these structures evolve at a feasible pace and at the right period in a team’s overall arc. Evidence from our evaluation suggests that teams using DreamTeam can be far more effective than existing modes of organizing for these rapidly convened teams. We envision computation as a partner in helping groups achieve their goals. It can aid us when we exhibit biases or limited self-knowledge—such as identifying effective team structures—and it will help us re-plan when the environment shifts. DreamTeam represents a step toward this future of computationally augmented teams and organizations.
"
Troubling Vulnerability: Designing with LGBT Young People's Ambivalence Towards Hate Crime Reporting,https://dl.acm.org/authorize?N656687,"
HCI is increasingly working with 'vulnerable' people, yet there is a danger that the label of vulnerability can alienate and stigmatize the people such work aims to support. We report our study investigating the application of interaction design to increase rates of hate crime reporting amongst Lesbian, Gay, Bisexual and Transgender young people. During design-led workshops, participants expressed ambivalence towards reporting. While recognizing their exposure to hate crime, they simultaneously rejected being identified as victim as implied in the act of reporting. We used visual communication design to depict the young people's ambivalent identities and contribute insights into how these fail and succeed to account for the intersectional, fluid and emergent nature of LGBT identities through the design research process. We argue that by producing ambiguously designed texts alongside conventional outcomes, we 'trouble' our design research narratives as a tactic to disrupt static and reductive understandings of vulnerability within HCI. Making space for stories of experiences of hate crime and resilience to circulate is one of the ways in which design research can support vulnerable people [46]. However, there is a need for reflexive awareness of our role in shaping such stories and a need to actively work to disrupt narratives that cast participants as passive victims [29]. As we learnt from our experience presenting our research at a Pride event, it is easy for research accounts to become static clichés that close down meaning making, and so the troubling of narratives needs to be an ongoing process. Ambiguity is often cited as a resource for design within HCI and design research [22], but its role in design research has been less clearly defined. In the context of this project, we found that ambiguity was a resource for an ongoing ‘troubling’ of artefacts, narratives and identities which emerged from the research process. It forces us to be engaged with reflexivity, not as a one-time declaration of our positions, but an on-going practice of looking hard at oneself and listening carefully to others. It draws our attention to the edges and outliers of our data, but, perhaps most importantly, opens up space from which new meanings and understandings can emerge.
"
PHUI-kit: Interface Layout and Fabrication on Curved 3D Printed Objects,https://dl.acm.org/authorize?N656688,"
We seek to make physical user interface (PHUI) design more like graphical user interface (GUI) design by using a drag-and drop interface to place widgets, allowing widgets to be repositioned and by hiding implementation details. PHUIs are interfaces built from tangible widgets arranged on the surfaces of physical objects. PHUI layout will become more important as we move from rectangular screens to purpose-built interactive devices. Approaches to PHUI layout based on sculpture make it difficult to reposition widgets, and software approaches do not involve placing widgets on the device exterior. We created PHUI-kit, a software approach to PHUI layout on 3D printed enclosures, which has a drag-and-drop interface, supports repositioning of widgets, and hides implementation details. We describe algorithms for placing widgets on curved surfaces, modifying the enclosure geometry, and routing wiring inside the enclosure. The tool is easy to use and supports a wide range of design possibilities. Making PHUI layout more like GUI layout on physical interfaces(3D models) will likely lead ta a better set of interface design tools. Better interface design tools may accelerate the creation of physical interactive devices which do not look like rectangular boxes with screens as found in cellphones and tablets. Curved elements are prominent in physical devices and PHUI works well with curved geometry. We have identified core problems associated with this approach and proposed a set of solutions. Future work might explore different approaches to the core problems We supported only basic operations of interface layout such as place, move, rotate and delete. Additional work will be needed to support more complex operations such as aligning a group of widgets and scaling the interface in response to changing the cuvature or size of the enclosure.

"
Crowd-Guided Ensembles: How Can We Choreograph Crowd Workers for Video Segmentation?,https://dl.acm.org/authorize?N656689,"
In this work, we propose two ensemble methods leveraging a crowd workforce to improve video annotation, with a focus on video object segmentation. Their shared principle is that while individual candidate results may likely be insufficient, they often complement each other so that they can be combined into something better than any of the individual results---the very spirit of collaborative working. For one, we extend a standard polygon-drawing interface to allow workers to annotate negative space, and combine the work of multiple workers instead of relying on a single best one as commonly done in crowdsourced image segmentation. For the other, we present a method to combine multiple automatic propagation algorithms with the help of the crowd. Such combination requires an understanding of where the algorithms fail, which we gather using a novel coarse scribble video annotation task. We evaluate our ensemble methods, discuss our design choices for them, and make our web-based crowdsourcing tools and results publicly available. We introduced two novel crowd-guided ensemble methods that combine multiple inputs from the crowd as well as automated algorithms to generate final segmentations that are better than any of their individual components. First, our results show that we can acquire image segmentations of higher quality by combining the work of multiple individuals. They confirm that negative polygon annotations are effective for crowdsourced segmentation. They also show that using our scribble-based ensemble method, we can delegate the review process of our oracle to the crowd to a certain extent. In practice, the oracle may well be required because rotoscoping work is often tied to some artistic decision that requires human validation. On the propagation side, our scribble-based ensembles are promising given their flexibility and the potential quality increase over naive ensembles, although they do not achieve cost efficiency compared to increase segmentation rates. Finally, the web interfaces, tools, and data of our experiments will be made public for future research.

"
Phone vs. Tangible in Museums: A Comparative Study,https://dl.acm.org/authorize?N656680,"
Despite years of HCI research on digital technology in museums, it is still unclear how different interactions impact on visitors'. A comparative evaluation of smart replicas, phone app and smart cards looked at the personal preferences, behavioural change, and the appeal of mobiles in museums. 76 participants used all three interaction modes and gave their opinions in a questionnaire; participants interaction was also observed. The results show the phone is the most disliked interaction mode while tangible interaction (smart card and replica combined) is the most liked. Preference for the phone favour mobility to the detriment of engagement with the exhibition. Different behaviours when interacting with the phone or the tangibles where observed. The personal visiting style appeared to be only marginally affected by the device. Visitors also expect museums to provide the phones against the current trend of developing apps in a ""bring your own device"" approach. In conclusion, our study showed the limited effect of the interaction mode on the visiting behaviour and preferences. Instead it highlights the importance of the personal attitude and visiting style. Key to appreciation and engagement is the match between the personal visiting style and the interaction that better affords that style. Our findings offer museum professionals, exhibition designers and interaction designers evidence that multiple options are possible and how they affect visitors. Finally, we provide researchers working on tangible interaction in museums with some important elements that can help explain why some people do not engage with the design as expected, e.g. not all visiting style appreciate the form of “forced” engagement proposed by tangibles.
"
Evaluating User Satisfaction with Typography Designs via Mining Touch Interaction Data in Mobile Reading,https://dl.acm.org/authorize?N656681,"
Previous work has demonstrated that typography design has a great influence on users' reading experience. However, current typography design guidelines are mainly for general purpose, while the individual needs are nearly ignored. To achieve personalized typography designs, an important and necessary step is accurately evaluating user satisfaction with the typography designs. Current evaluation approaches, e.g., asking for users' opinions directly, however, interrupt the reading and affect users' judgments. In this paper, we propose a novel method to address this challenge by mining users' implicit feedbacks, e.g., touch interaction data. We conduct two mobile reading studies in Chinese to collect the touch interaction data from 91 participants. We propose various features based on our three hypotheses to capture meaningful patterns in the touch behaviors. The experiment results show the effectiveness of our evaluation models with higher accuracy on comparing with the baseline under three text difficulty levels, respectively. As the first step to provide personalized typography designs, evaluating user satisfaction without interrupting reading is still a challenge for designers. In this study, we conduct two reading studies and demonstrate that users’ touch behaviors in reading can reflect their satisfaction with the typography design. We also find out text difficulty has an influence on the touch behaviors. The results show that evaluation models based on our features lead to higher accuracy than the baseline.
"
"Keeping a Low Profile?: Technology, Risk and Privacy among Undocumented Immigrants",https://dl.acm.org/authorize?N656682,"
Undocumented immigrants in the United States face risks of discrimination, surveillance, and deportation. We investigate their technology use, risk perceptions, and protective strategies relating to their vulnerability. Through semi-structured interviews with Latinx undocumented immigrants, we find that while participants act to address offline threats, this vigilance does not translate to their online activities. Their technology use is shaped by needs and benefits rather than risk perceptions. While our participants are concerned about identity theft and privacy generally, and some raise concerns about online harassment, their understanding of government surveillance risks is vague and met with resignation. We identify tensions among self-expression, group privacy, and self-censorship related to their immigration status, as well as strong trust in service providers. Our findings have implications for digital literacy education, privacy and security interfaces, and technology design in general. Even minor design decisions can substantially affect exposure risks and well-being for such vulnerable communities. For undocumented immigrants living in the United States, typical struggles of immigration and integration are exacerbated by a fear of discovery and deportation. How undocumented immigrants perceive and manage status-related risks in technology use has not been well understood previously. Through our interviews with 17 Latinx undocumented immigrants, we provide insights into this community’s technology use practices, risk perceptions and protective strategies. We find that many struggle to translate awareness and risk mitigation strategies they employ in the physical world to technology use and the online environment. Due to uncertain risks of various kinds, including surveillance and a sense that the government knows a lot about them already, many do not fully consider how their behavior online may affect risks of discovery. For others, technology use is associated with tensions among convenience, intimate engagement, self-disclosure, self-censorship and community participation. Furthermore, we find latent yet uncertain concerns about what others in their network might inadvertently reveal about them. These tensions and boundary turbulences create stress that may affect the well-being of themselves and their families. Our findings demonstrate an opportunity for the design and provision of educational resources, and the design of transparency and privacy mechanisms. Community organizations, such as schools or churches, as well as service providers, such as Facebook, also have an important role to play in mitigating, or potentially exacerbating, risks from technology use for the undocumented immigrant community and vulnerable communities more broadly.

"
Comparing Computer-Based Drawing Methods for Blind People with Real-Time Tactile Feedback,https://dl.acm.org/authorize?N656683,"
In this paper, we present a drawing workstation for blind people using a two-dimensional tactile pin-matrix display for in- and output. Four different input modalities, namely menu-based, gesture-based, freehand-stylus and a Time-of-Flight (ToF) depth segmentation of real-world object silhouettes, are utilized to create graphical shapes. Users can freely manipulate shapes after creation. Twelve blind users evaluated and compared the four image creation modalities. During evaluation, participants had to copy four different images. The results show that all modalities are highly appropriate for non-visual drawing tasks. There is no generally preferred drawing modality, but most participants rated the robust and well-known menu-based interaction as very good. Furthermore, menu was second in performance and the most accurate drawing modality. Our evaluation demonstrated direct manipulation works well for blind users at the position of the reading hand. In general, our drawing tool allows blind users to create appealing images. In this paper, we have presented a powerful drawing workstation for blind users with real-time tactile feedback. Within our evaluation, we enabled blind users to draw accurate images. The used input modality was effecting the quality of the result but not the satisfaction felt by the user. None of the offered drawing modalities was preferred in general, but we saw that visually impaired users often rated the reliable, robust and well-known menu modality as one of their favorites. The participants stated out, that they would prefer a combination of different modalities, choosing that one that suites best for the current needs. However, only a few users would accept the additional technical equipment and effort for searching a suitable object for drawing by camera-based object segmentation. The results show that the human factor has a high influence on the tools that people like to use, especially when it comes to creativity tasks. Of course, the evaluation design does not give the two free form creation methods (freehand stylus and object segmentation) the chance to shine. Drawings with more complex figures or real-world objects could enable these methods to outperform the conservative methods (menu and gestures). Our workstation is designed to allow the user to draw and interact with any kind of black and white images. However, this advantage in flexibility leads to the lack of automation in creating specific kinds of graphics (e. g. diagrams). Compared to more specialized tools, in our workstation the user must compose his/her image from standard or free form shapes. Our system is best suited for creativity tasks as well as cooperation with sighted people (e. g. presenting ideas graphically). Our evaluation demonstrated that direct manipulation works well for blind users at the position of the reading hand, but not far beyond the current region of interest. A better understanding of the mental model related to operations along symmetry axes pertinent to shapes is needed. Referring the list of requirements and recommendations for digital drawing tools for blind people from [9], the crucial requirements named (1) tactile feedback, (2) error correction methods and (4) sharing the resulting images are met. The next step is to check our system against the whole list of requirements, including (3) compatibility with common AT. Based on the evaluation results, it became clear that blind users want to have even more drawing support than what is offered yet. For instance, they want to have more predefined shapes in the menu and gesture drawing modality, such as different kinds of triangles, polygons, circle segments or complex figures. More predefined shape palettes lead to more complex menus, making the search for the desired shape even more time-consuming. To counteract this problem, additional speech input could support usability. We saw that drawing freehand figures is a hard task, especially for congenitally blind users. We all know how hard it is to draw a complex shape in a single stroke. As Kamel and Landay [21] noticed, connecting two lines exactly to each other is one of the main problematic tasks while drawing. Therefore, the support for drawing complex shapes by connecting different subparts to each other is the next important feature to offer. In general, users often criticized the fact that vectorized shapes lack in straight edges and sharp corners. To overcome this problem it is possible to add shape recognition functionality to detect standard shapes in freehand sketches or segmented objects like other sketching systems do.

"
Enabling People with Visual Impairments to Navigate Virtual Reality with a Haptic and Auditory Cane Simulation,https://dl.acm.org/authorize?N656684,"
Traditional virtual reality (VR) mainly focuses on visual feedback, which is not accessible for people with visual impairments. We created Canetroller, a haptic cane controller that simulates white cane interactions, enabling people with visual impairments to navigate a virtual environment by transferring their cane skills into the virtual world. Canetroller provides three types of feedback: (1) physical resistance generated by a wearable programmable brake mechanism that physically impedes the controller when the virtual cane comes in contact with a virtual object; (2) vibrotactile feedback that simulates the vibrations when a cane hits an object or touches and drags across various surfaces; and (3) spatial 3D auditory feedback simulating the sound of real-world cane interactions. We designed indoor and outdoor VR scenes to evaluate the effectiveness of our controller. Our study showed that Canetroller was a promising tool that enabled visually impaired participants to navigate different virtual spaces. We discuss potential applications supported by Canetroller ranging from entertainment to mobility training. In this paper, we presented Canetroller, a haptic controller that simulates white cane interactions, enabling people with visual impairments to navigate a virtual environment by transferring their cane skills into the virtual world. We started with a formative study to understand VIPs’ cane strategies, and distilled implications to guide the design of Canetroller. We also evaluated the effectiveness of Canetroller with indoor and outdoor virtual scenarios, showing it to be a promising tool to enable visually impaired users to explore and navigate the virtual world. Canetroller has potential in different areas, such as entertainment, O&M training, and environment preparation. We hope our work can inspire researchers and designers to design more effective tools to make VR more inclusive.

"
Pulp Nonfiction: Low-Cost Touch Tracking for Paper,https://dl.acm.org/authorize?N656695,"
Paper continues to be a versatile and indispensable material in the 21st century. Of course, paper is a passive medium with no inherent interactivity, precluding us from computationally-enhancing a wide variety of paper-based activities. In this work, we present a new technical approach for bringing the digital and paper worlds closer together, by enabling paper to track finger input and also drawn input with writing implements. Importantly, for paper to still be considered paper, our method had to be very low cost. This necessitated research into materials, fabrication methods and sensing techniques. We describe the outcome of our investigations and show that our method can be sufficiently low-cost and accurate to enable new interactive opportunities with this pervasive and venerable material. We have described a new approach for creating touchsensing paper. We reported the outcomes of our search for compatible fabrication methods and materials, and discussed how we can support tracking of both fingers and implements. We quantified the performance of our touch tracking through a user study, which suggests the overall approach has merit. Although much future work remains, we believe this illuminates one possible approach for achieving low-cost, interactive, paper-based experiences in the future.

"
Exploring New Metaphors for a Networked World through the File Biography,https://dl.acm.org/authorize?N656696,"
We present a body of work undertaken in response to the challenge outlined by Harper et al. in their paper, ?'What is a File?' [9]. Through a conceptual and design-led exploration of new file metaphors, we developed the 'file biography', a digital entity that encompasses the provenance of a file and allows the user to keep track of how it propagates. We explored this through prototyping and utilised it in two user studies. In the studies, we (i) asked people to sketch out file biographies for their own content, and (ii) deployed a tool enabling users to build their own simple file biographies across multiple versions of Word documents. We conclude that new file metaphors may need to play different roles for different types of digital content, with a distinction being drawn between content that is 'in production' and virtual possessions that are, in a sense, a 'finished' artefact. The question of how to design new file metaphors for a networked world is challenging to address. In this paper, we have outlined our approach, which combines research through design, development, and with users, to explore this problem space. Our work centres on the file biography, used as an analytic tool to ground prototyping and interviews with users. Our research has led us to reflect on how new file metaphors can work across components that are stored in different places or that represent different points in time. It has also pushed us to consider how metaphors might play different roles in different contexts, with a distinction being drawn between content that is ‘in production’ and virtual possessions that are, in some sense, a finished form. We argue that these different lenses require different emphases when developing a new grammar of action; while the former might be bound up with ways of syncing content that enables work to be done in private, the latter might be bound up with more nuanced models of sharing and ownership.
"
Understanding Older Users' Acceptance of Wearable Interfaces for Sensor-based Fall Risk Assessment,https://dl.acm.org/authorize?N656697,"
Algorithms processing data from wearable sensors promise to more accurately predict risks of falling -- a significant concern for older adults. Substantial engineering work is dedicated to increasing the prediction accuracy of these algorithms; yet fewer efforts are dedicated to better engaging users through interactive visualizations in decision-making using these data. We present an investigation of the acceptance of a sensor-based fall risk assessment wearable device. A participatory design was employed to develop a mobile interface providing visualizations of sensor data and algorithmic assessments of fall risks. We then investigated the acceptance of this interface and its potential to motivate behavioural changes through a field deployment, which suggested that the interface and its belt-mounted wearable sensors are perceived as usable. We also found that providing contextual information for fall risk estimation combined with relevant practical fall prevention instructions may facilitate the acceptance of such technologies, potentially leading to behaviour change. Our survey and interview results showed that overall the SFRA system implemented in this study achieved high user acceptance (Hypothesis H1), and participants consider the fall risk estimation useful and the accessibility is appreciated. Participants have also indicated they are more aware of the risk of falling from this experience (Hypothesis H2), and they also believe this information can motivate them to adopt fall prevention interventions. With the simulated high risk data, we also found that older users consider fall risk estimation more useful when the fall risk is high, and they have also pointed out the value of estimation is limited when the fall risk is low. This highlights the importance of attaching contextual information to the estimation data to help users interpret and understand their situations better, thus providing more values in the information to keep users engaged. Furthermore, it is necessary to provide specific instructions in the case of high risk, in order to guide the older user to take most appropriate actions while they are motivated to improve their situation. In addition, any misconception of technology must be carefully addressed as it could lead to risk-taking behaviours. Lastly, feedback from field evaluation were collected regarding features suggested through participatory design, leading to recommendations on improving the design of mobile UIs for SFRA systems.
"
How People Form Folk Theories of Social Media Feeds and What it Means for How We Study Self-Presentation,https://dl.acm.org/authorize?N656698,"
Self-presentation is a process that is significantly complicated by the rise of algorithmic social media feeds, which obscure information about one's audience and environment. User understandings of these systems, and therefore user ability to adapt to them, are limited, and have recently been explored through the lens of folk theories. To date, little is understood of how these theories are formed, and how they tie to the self-presentation process in social media. This paper presents an exploratory look at the folk theory formation process and the interplay between folk theories and self-presentation via a 28-participant interview study. Results suggest that people draw from diverse sources of information when forming folk theories, and that folk theories are more complex, multifaceted and malleable than previously assumed. This highlights the need to integrate folk theories into both social media systems and theories of self-presentation. In this paper, we have taken the first steps in understanding the folk theory formation process as it applies to algorithmic social media feeds, and provided a preliminary conceptual toolkit for integrating this important process into modern models of self-presentation. These contributions provide guidance to designers on how to aid social media users in fulfilling their presentation goals in a complex, algorithmic online world.
"
Analogy Mining for Specific Design Needs,https://dl.acm.org/authorize?N656699,"
Finding analogical inspirations in distant domains is a powerful way of solving problems. However, as the number of inspirations that could be matched and the dimensions on which that matching could occur grow, it becomes challenging for designers to find inspirations relevant to their needs. Furthermore, designers are often interested in exploring specific aspects of a product-- for example, one designer might be interested in improving the brewing capability of an outdoor coffee maker, while another might wish to optimize for portability. In this paper we introduce a novel system for targeting analogical search for specific needs. Specifically, we contribute an analogical search engine for expressing and abstracting specific design needs that returns more distant yet relevant inspirations than alternate approaches. In this paper, we contribute a novel system for tuning analogical search for specific design needs, consisting of an interface for designers to express their specific needs in abstract terms, and an analogy search engine that uses this focus-abstracted query to find inspirations from a corpus that are both relevant and domain-distant. This work contributes a novel path forward to computational support for mining large databases of potential inspirations on the Web to improve design work.

"
Measuring Employment Demand Using Internet Search Data,https://dl.acm.org/authorize?N656690,"
We are in a transitional economic period emphasizing automation of physical jobs and the shift towards intellectual labor. How can we measure and understand human behaviors of job search, and how communities are adapting to these changes? We use internet search data to estimate employment demand in the United States. Starting with 225 million raw job search queries in 2015 and 2016 from a popular search engine, we classify queries into one of 15 fields of employment with accuracy and F-1 of 97%, and use the resulting query volumes to estimate per-sector employment demand in U.S. counties. We validate against Bureau of Labor Statistics measures, and then demonstrate benefits for communities, showing significant differences in the types of jobs searched for across socio-economic dimensions like poverty and education level. We discuss implications for macroeconomic measurement, as well as how community leaders, policy makers, and the field of HCI can benefit. In this paper, we proposed a new measure of job search trends called employment demand that captures the demands individuals have while searching for a job. We used 225 million search queries in 2015 and 2016 from Bing to map and explore trends in employment demand. We developed a classification system that labels these search queries into one of 15 categories of job search that performs with .97 F1. We validated our measure by showing expected correlations with changes in monthly non-farm payrolls, and across geographies by showing relationships to demographic characteristics of U.S. counties such as population size, existing labor force measures, and socio-economic measures. We believe that employment demand can be used to explore many community, policy, and economic trends, and encourage HCI to engage in research around gainful employment.
"
Data Illustrator: Augmenting Vector Design Tools with Lazy Data Binding for Expressive Visualization Authoring,https://dl.acm.org/authorize?N656691,"
Building graphical user interfaces for visualization authoring is challenging as one must reconcile the tension between flexible graphics manipulation and procedural visualization generation based on a graphical grammar or declarative languages. To better support designers' workflows and practices, we propose Data Illustrator, a novel visualization framework. In our approach, all visualizations are initially vector graphics; data binding is applied when necessary and only constrains interactive manipulation to that data bound property. The framework augments graphic design tools with new concepts and operators, and describes the structure and generation of a variety of visualizations. Based on the framework, we design and implement a visualization authoring system. The system extends interaction techniques in modern vector design tools for direct manipulation of visualization configurations and parameters. We demonstrate the expressive power of our approach through a variety of examples. A qualitative study shows that designers can use our framework to compose visualizations. We present a novel framework that describes the generation of visualizations from the perspective of graphic design. Based on the framework, we design and build Data Illustrator, a system that augments vector design tools with lazy data binding for visualization authoring. We demonstrate the expressivity of our approach through visualization examples. A study with 13 designers shows that the system is learnable and designers can use the framework to compose visualizations. The Data Illustrator system is available at http://www.data-illustrator.com. The framework we created provides descriptive and generative power for visualization design, but in its current form, it is not complete. The framework needs to be expanded to include area as a shape primitive, so that it can describe visualizations such as area charts and stream graphs. Further research is also necessary to include the support for hierarchical, network and geographic data, and the corresponding visualizations. In terms of implementation, Data Illustrator demonstrates the feasibility and power of a subset of the framework. Layouts such as packing and polar coordinate positioning are not yet supported in the system. We also need to implement ring as a shape primitive for creating donut charts. Finally, systems such as Lyra build on top of Vega, which have access to the functionalities offered by D3 , including interpolation methods for lines and curves. Data Illustrator should provide those capabilities in order to be more powerful. For future work, we would also like to explore how to turn visualization designs into reusable templates. Once users create a visualization inside Data Illustrator, they should be able to export it into formats readable by other tools and to share it with other users, who can customize the design with their own data and visual styles. Adding authoring support for interactivity is also a direction to investigate further.

"
Safety vs. Surveillance: What Children Have to Say about Mobile Apps for Parental Control,https://dl.acm.org/authorize?N656692,"
Mobile applications (""apps"") developed to promote online safety for children are underutilized and rely heavily on parental control features that monitor and restrict their child's mobile activities. This asymmetry in parental surveillance initiates an interesting research question -- how do children themselves feel about such parental control apps? We conducted a qualitative analysis of 736 reviews of 37 mobile online safety apps from Google Play that were publicly posted and written by children (ages 8-19). Our results indicate that child ratings were significantly lower than that of parents with 76% of the child reviews giving apps a single star. Children felt that the apps were overly restrictive and invasive of their personal privacy, negatively impacting their relationships with their parents. We relate these findings with HCI literature on mobile online safety, including broader literature around privacy and surveillance, and outline design opportunities for online safety apps. Mobile technologies should support children in their developmental goals, including information-seeking, learning about rules and boundaries, and maintaining social relationships [7], in addition to keeping them safe from online risks. However, this goal will only be accomplished once designers listen more intently to children as end users. In the results presented in this paper, we chose to use a purely descriptive approach to present the key themes and unfiltered quotes from the child users. Our goal in doing this was to provide a non-judgmental account of what children had to say in their reviews. To conclude, however, we use a more interpretive lens to reflect on our results. We observed a lot of frustration and anger in the child comments. Thus, it is possible that the reviews may have been overly biased toward negative impressions of the apps in an angered attempt to retaliate against their parents. However, the direct quotes from the child reviews actually illustrated a rather surprising level of maturity, selfawareness, and reason that researchers do not typically capture when viewing children through the eyes of adults. Children in our study wanted their parents to give them more freedom, a chance to prove they can make good online choices, and space to make some mistakes. They were not upset that online safety apps prevent them from risk-seeking behaviors; they were mad that they prevented them from doing other useful tasks. Children liked features within apps that helped them with problematic behavior but gave them some level of control, or at least gave them a way to negotiate or compromise with their parents regarding rules and restrictions. Given this sociotechnical gap, children provided well-articulated and honest commentary around how these apps did not reinforce positive parenting practices and, based on the literature [62], they were right. These beneficial features are clearly under-supported within the existing app offerings [63]. Thus, our conclusion here was that we (as researchers, designers, and parents) might want to consider turning the critical lens around to look at ourselves and understand the negative biases we may hold about children when trying to keep them safe online.

"
Co-performance: Conceptualizing the Role of Artificial Agency in the Design of Everyday Life,https://dl.acm.org/authorize?N656693,"
This paper introduces the notion of co-performance, with the aim to offer Human-Computer Interaction (HCI) researchers and practitioners a new perspective on the role of artificial agency in everyday life, from automated systems to autonomous devices. In contrast to 'smartness,' which focuses on a supposed autonomy of artifacts, co-performance considers artifacts as capable of learning and performing next to people. This shifts the locus of design from matters of distributions of agency at design time, to matters of embodied learning in everyday practice for both human and artificial performers. From this perspective, co-performance acknowledges the dynamic differences in capabilities between humans and artifacts, and highlights the fundamentally recursive relation between professional design and use. Implications for HCI design practice are unpacked through reflections on smart thermostat design in light of historic changes in roles between humans and heating systems, and changing ideas of appropriateness in everyday practices of domestic heating. In this paper we have introduced the concept of coperformance as a novel perspective on the role of artificial agency in everyday life. Interest in what is often referred to as smart and autonomous devices, and more recently embedded artificial intelligence, has grown in HCI. Increased proliferation of such computational artefacts in the messy and unpredictable settings of everyday life, however, poses significant challenges. To address these challenges, we have reflected on smart thermostat design. We have done so from the perspective of historic changes in both human and artificial performances and roles, in relation to the social practice of domestic heating. Through the links that can be made between changes in ideas of appropriate indoor climates, we have unpacked the main tenets of co-performance and offered the beginnings of a new research agenda. Considering the interplay between humans and artefacts in everyday life is not new, but linking it to the conceptual framework of social theories of practice in HCI allows to conceptualize the relations between people and artefacts in a wider context, which includes the relation between everyday use and professional practices of technology development. Co-performance opens new avenues for HCI researchers and designers to (1) develop richer accounts of the dynamic role of computational artefacts in everyday life, and by implications related design practices; and (2) develop new design theory and strategies that thoughtfully take into account differences in capabilities between human and artificial co-performers, and show sensitivity to the power dynamics involved when different ideas of appropriate practice come together in situated performance.
"
Social Influence and Reciprocity in Online Gift Giving,https://dl.acm.org/authorize?N656694,"
Giving gifts is a fundamental part of human relationships that is being affected by technology. The Internet enables people to give at the last minute and over long distances, and to observe friends giving and receiving gifts. How online gift giving spreads in social networks is therefore important to understand. We examine 1.5 million gift exchanges on Facebook and show that receiving a gift causes individuals to be 56% more likely to give a gift in the future. Additional surveys show that online gift giving was more socially acceptable to those who learned about it by observing friends' participation instead of a non-social encouragement. Most receivers pay the gift forward instead of reciprocating directly online, although surveys revealed additional instances of direct reciprocity, where the initial gifting occurred offline. Thus, social influence promotes the spread of online gifting, which both complements and substitutes for offline gifting. This research makes progress towards gaining a better understanding of the spread of a fundamental social phenomenon in the real world. This required overcoming a number of methodological challenges. We adopted a mixed-methods approach to triangulate and gain a broader picture of online gift exchange. An exploratory analysis of behavioral data provided initial evidence for peer effects and homophily in age, and the quasi-experimental method provided a credible estimate of peer effects in online gift giving. The survey allowed us to fill in the gaps for offline behavior and highlighted variation in perceived social norms. Mixed-methods research approaches like this one can be applied in many other domains, in particular for studying the spread of other socio-technical phenomena.
"
Time for Break: Understanding Information Workers' Sedentary Behavior Through a Break Prompting System,https://dl.acm.org/authorize?N656605,"
Extended periods of uninterrupted sedentary behavior are detrimental to long-term health. While prolonged sitting is prevalent among information workers, it is difficult for them to break prolonged sedentary behavior due to the nature of their work. This work aims to understand information workers' intentions & practices around standing or moving breaks. We developed Time for Break, a break prompting system that enables people to set their desired work duration and prompts them to stand up or move. We conducted an exploratory field study (N = 25) with Time for Break to collect participants' work & break intentions and behaviors for three weeks, followed by semi-structured interviews. We examined rich contexts affecting participants' receptiveness to standing or moving breaks, and identified how their habit strength and self-regulation are related to their break-taking intentions & practices. We discuss design implications for interventions to break up periods of prolonged sedentary behavior in workplaces. We conducted an exploratory study with Time for Break, a desktop-based prompting system built to collect information workers’ work & break intentions and behaviors, and the contexts around them. From the three weeks of data collection with Time for Break, pre- & post-study questionnaire, and post-study interviews, we distilled rich contexts affecting participants’ receptiveness to standing or moving breaks, and identified how their habit strength and self-regulation are related to their break-taking intentions & practices. This work opens up a new research avenue going beyond existing works on productivity and interruptibility. We presented many research opportunities to promote healthy workplace environment. Specifically, we discussed the importance of “long-term health” as an important design value, needs to promote moving breaks in workplaces, and ways to help information workers form healthy and consistent work & break rhythm. We also identified several ways to extend Time for Break to be an effective and sustainable intervention to help people break prolonged sedentary behavior in workplaces.

"
Breaking the Tracking: Enabling Weight Perception using Perceivable Tracking Offsets,https://dl.acm.org/authorize?N656606,"
Virtual reality (VR) technology strives to enable a highly immersive experience for the user by including a wide variety of modalities (e.g. visuals, haptics). Current VR hardware however lacks a sufficient way of communicating the perception of weight of an object, resulting in scenarios where users can not distinguish between lifting a bowling ball or a feather. We propose a solely software based approach of simulating weight in VR by deliberately using perceivable tracking offsets. These tracking offsets nudge users to lift their arm higher and result in a visual and haptic perception of weight. We conducted two user studies showing that participants intuitively associated them with the sensation of weight and accept them as part of the virtual world. We further show that compared to no weight simulation, our approach led to significantly higher levels of presence, immersion and enjoyment. Finally, we report perceptional thresholds and offset boundaries as design guidelines for practitioners. In this paper we presented a solely software based approach for tracking based VR interactions to generate a visual perception of weight. Our approach is based on introducing a controlled and perceivable tracking offset whereas heavier weights are represented as a higher offset. To gain a deeper understanding of the perception and effectiveness of our approach we conducted two user studies. We were able to show that people associate different tracking offset as different weights. Using a two-alternative forced-choice task we could quantify the detection threshold between two weights for light (approx. 2.5 cm) and heavy objects (approx. 3.6 cm). We also found, that users like even obvious and perceivable offsets of 24 cm as weight representation. By testing our approach not only for measuring perceptual thresholds, as it was done in related works, but also in a fully immersive VR game, we contribute to the positive effects on haptic perception as well as presence, immersion and enjoyment. We could show that our pseudohaptic approach results in a significant better experiences than the currently non-existent simulation of weight. Since our approach does not require any additional hardware, we argue that our approach can easily be implemented inside all current tracking based VR applications.
"
Remixed Reality: Manipulating Space and Time in Augmented Reality,https://dl.acm.org/authorize?N656607,"
We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time. In this work we introduced Remixed Reality, a novel form of mixed reality. It allows users to see the actual physical world but perform changes as easily as in virtual reality. This is enabled by presenting and modifying a live 3D reconstruction of the environment, captured through multiple external depth cameras. We believe this approach allows for applications that require altering users’ surrounding space such as diminished reality, as well applications where users want to make temporal changes like pausing time, or recording and playing back specific events.

"
Mapping Machine Learning Advances from HCI Research to Reveal Starting Places for Design Innovation,https://dl.acm.org/authorize?N656608,"
HCI has become particularly interested in using machine learning (ML) to improve user experience (UX). However, some design researchers claim that there is a lack of design innovation in envisioning how ML might improve UX. We investigate this claim by analyzing 2,494 related HCI research publications. Our review confirmed a lack of research integrating UX and ML. To help span this gap, we mined our corpus to generate a topic landscape, mapping out 7 clusters of ML technical capabilities within HCI. Among them, we identified 3 under-explored clusters that design researchers can dig in and create sensitizing concepts for. To help operationalize these technical design materials, our analysis then identified value channels through which the technical capabilities can provide value for users: self, context, optimal, and utility-capability. The clusters and the value channels collectively mark starting places for envisioning new ways for ML technology to improve people's lives. The goal of this paper was to provide an analysis of the current landscape of HCI research in relation to ML, and organize the diverse design opportunities that comprise this field. To do so, we categorized the HCI-ML work into two frameworks. We created seven topic clusters of ML technologies and a conceptual model showing how users experience value from ML enhanced systems. The technical topics and value channels can serve as descriptors for what has been done and provide starting points for future UX innovations. We encourage fellow researchers to use, evaluate, discuss and improve these frameworks, and join us in releasing the potential for better, more creative, more sophisticated design with ML. We also want to step back for a moment to more broadly consider the idea of mining ML advances in research to inform and inspire design researchers. One limitation of the presented frameworks is a lack of evaluation. The intended outcome of this approach — inspiring design researchers and making them desire to create sensitizing concepts or undertaking research on ML — is a very difficult thing to measure because it is nearly impossible to control for. Instead, if HCI design researchers accept the argument that ML as a design material adds value to UX, what our frameworks provide is a concrete way for them to begin to investigate ML in their own research and design. In addition, the ML research topic landscape and the inference-acting ML capability schema offer the practical value for their design, taking stock of what is known and to identify major unknown topics as a basis for their future research endeavor. We strongly encourage the UX and HCI research community to join us and start a serious discussion around the innovation issues related to the idea that ML is the new UX.
"
Pentelligence: Combining Pen Tip Motion and Writing Sounds for Handwritten Digit Recognition,https://dl.acm.org/authorize?N656609,"
Digital pens emit ink on paper and digitize handwriting. The range of the pen is typically limited to a special writing surface on which the pen's tip is tracked. We present Pentelligence, a pen for handwritten digit recognition that operates on regular paper and does not require a separate tracking device. It senses the pen tip's motions and sound emissions when stroking. Pen motions and writing sounds exhibit complementary properties. Combining both types of sensor data substantially improves the recognition rate. Hilbert envelopes of the writing sounds and mean-filtered motion data are fed to neural networks for majority voting. The results on a dataset of 9408 handwritten digits taken from 26 individuals show that motion+sound outperforms single-sensor approaches at an accuracy of 78.4% for 10 test users. Retraining the networks for a single writer on a dataset of 2120 samples increased the precision to 100% for single handwritten digits at an overall accuracy of 98.3%. We reported the design of a digital pen for common writing surfaces, such as paper, that does not require a special pattern and is not confined to a particular area or drawing order. The designed pen does not strongly differ from regular pens in appearance (except for a wired USB connection) and is built from cheap and robust components. The evaluation based on a corpus of digit data shows that the pen achieves high recognition rates. The combination of motion and audio data performs substantially better than motion data alone or audio data alone. It turns out that these types of sensor data have complementary characteristics. It was also shown that majority voting neural networks, each with the same topology, provide a more accurate classification than single networks. Furthermore we found that combining the strengths of audio and motion features achieves better results when motion data are classified first and then the result is evaluated by classifiers trained on the sound emissions of the pen tip. Retraining the classifiers for a single writer on a dataset of 900 samples turned out to achieve very high recognition rates of 98.33%. It was also stated from a survey that users accept the recording of their handwriting for individualizing the neural networks. In future research the wired connection should be replaced by a wireless implementation, which was also the outcome of the questionnaire. To provide a wide field of application the dataset has to be extended to letters and gestures. Deep neural networks with a higher amount of training data and different topologies could be observed to achieve higher recognition rates on all test users before individualization as well as the impact of different surfaces or background noises. A live application such as a phone number dialing or a calculator as well was numeral password entry can be implemented to evaluate the user experience. Writer identification is also a research topic in which this pen could be applied. Future research will focus on letters, bigrams, and words with subsequent auto-correction to cover a wide spectrum of handwriting recognition.

"
Exploring Multimodal Watch-back Tactile Display using Wind and Vibration,https://dl.acm.org/authorize?N656600,"
A tactile display on the back of a smartwatch is an attractive output option; however, its channel capacity is limited owing to the small contact area. In order to expand the channel capacity, we considered using two perceptually distinct types of stimuli, wind and vibration, together on the same skin area. The result is a multimodal tactile display that combines wind and vibration to create ""colored"" tactile sensations on the wrist. As a first step toward this goal, we conducted in this study four user experiments with a wind-vibration tactile display to examine different ways of combining wind and vibration: Individual, Sequential, and Simultaneous. The results revealed the sequential combination of wind and vibration to exhibit the highest potential, with an information transfer capacity of 3.29 bits. In particular, the transition of tactile modality was perceived at an accuracy of 98.52%. The current results confirm the feasibility and potential of a multimodal tactile display combining wind and vibration. In this study, we investigated the feasibility of a multimodal watch-back tactile display using wind and vibration. We proposed a series of application scenarios for combining wind and vibration patterns. To verify if the scenarios are feasible, we conducted four pattern recognition experiments. The results revealed that the sequential patterns are highly perceivable. We hope that the experimental results of the current study will contribute toward motivating further studies on multimodal wearable tactile displays.

"
eKichabi: Information Access through Basic Mobile Phones in Rural Tanzania,https://dl.acm.org/authorize?N656601,"
This paper presents eKichabi, a tool for retrieving contact information for agriculture-related enterprises in Tanzania. eKichabi is an Unstructured Supplementary Service Data (USSD) application which users can access through basic mobile phones. We describe our focus groups, a design iteration, deployment in four villages, and follow up interviews by phone. This work demonstrates the feasibility of USSD for information access applications that have the potential for deployment on a large scale in the developing world. From user interviews, we identified strong evidence of eKichabi fulfilling an unmet need for business related information, both in identifying business contacts in other villages, as well locating specific service providers. One of our key findings demonstrates that users access information through multiple modes, including text search, in addition to menu navigation organized by both business sector category and geographic area. The initial deployment of the eKichabi demonstrated the feasibility, usability, and acceptability of a third-party USSD application for on-demand information access. The use of USSD for applications such as the eKichabi is well suited to people living in rural, developing areas, as USSD functions on all mobile phones, and can be deployed inexpensively. The eKichabi project also demonstrates the successful conversion of a paper phone book into an electronic application, which is the only feasible method of scaling such a directory to include more businesses, cover a larger geographical area, and reach a greater number of users.
"
Understanding Identity Presentation in Medical Crowdfunding,https://dl.acm.org/authorize?N656602,"
People desire to present themselves favorably to others. However, medical crowdfunding beneficiaries are often expected to present their dire medical conditions and financial straits to solicit financial support. To investigate how beneficiaries convey their situation on medical crowdfunding pages and how contributors perceive the presented information, we interviewed both medical crowdfunding beneficiaries and contributors. While beneficiaries emphasized the serious of their medical situations to signal their deservedness of support, contributor participants gave less attention to that content. Rather, they focused on their impression of the beneficiary's character formed by various features of contributions such as the contributor's names, messages, and shared pictures. These contribution features further signaled common connections between the beneficiary and contributors and each contributor's unique involvement in the beneficiary's medical journey. However, the contribution amount resulted in judgement about other contributors. We suggest design opportunities and challenges that apply these results to the design of medical crowdfunding interfaces. In this work we first examined how medical crowdfunding beneficiaries present their identity on a campaign and how contributors perceive them. We then explored how beneficiaries and contributors prefer to present monetary and non-monetary contributions. From the interviews with both beneficiaries and contributors of medical crowdfunding campaigns, we found that beneficiary participants often felt obligated to emphasize their ailing identity to signal that they deserved financial support. However, contributor participants were more interested in signals conveying the beneficiary’s character and their common connections to the beneficiary. Interestingly, the beneficiary’s character and common connections were often inferred from various contribution features (e.g., contributor names, messages, shared images). These contribution features also supported beneficiary participants in telling stories about how each contributor was involved in their medical journey. Furthermore, the storytelling aspect allowed beneficiary participants to view themselves as people who are valued members of a community. However, our participants did not want to publicly display the contribution amount because the amounts sometimes made contributors judge others negatively. Thus, we suggest designing a medical crowdfunding interface that can better assist storytelling rather than focus on the donation amount. We hope this medical crowdfunding storytelling interface can help both beneficiaries and contributors be more aware of how they are involved in a supportive community throughout the process of medical crowdfunding.

"
Pocket Transfers: Interaction Techniques for Transferring Content from Situated Displays to Mobile Devices,https://dl.acm.org/authorize?N656603,"
We present Pocket Transfers: interaction techniques that allow users to transfer content from situated displays to a personal mobile device while keeping the device in a pocket or bag. Existing content transfer solutions require direct manipulation of the mobile device, making inter-action slower and less flexible. Our introduced tech-niques employ touch, mid-air gestures, gaze, and a mul-timodal combination of gaze and mid-air gestures. We evaluated the techniques in a novel user study (N=20), where we considered dynamic scenarios where the user approaches the display, completes the task, and leaves. We show that all pocket transfer techniques are fast and seen as highly convenient. Mid-air gestures are the most efficient touchless method for transferring a single item, while the multimodal method is the fastest touchless method when multiple items are transferred. We provide guidelines to help researchers and practitioners choose the most suitable content transfer techniques for their systems. We presented Pocket Transfers: interaction techniques that allow content being transferred from a situated display to a personal mobile device, while keeping the mobile device in a pocket or bag throughout the interaction process. In a 20- participant user study, we evaluated four techniques employing touch, mid-air gestures, gaze, and a multimodal technique combining mid-air gestures and gaze, and compared them to QR codes, which served as a baseline condition. We found that pocket transfers are fast and convenient across different modalities and designs. Users highly appreciate not having the manipulate the mobile device, independent of the technique used. Touch and Mid-air gestures were the fastest techniques for quick interactions wherein only a single content item is transferred. Touch and Multimodal were the fastest techniques for interactions wherein multiple items are transferred. For situations where other people are present, Gaze was the most preferred technique due to its subtlety. Our work is useful to researchers and practitioners in a multitude of ways. First, we showed that content transfer methods where the recipient device remains in a pocket are generally fast and useful, and are therefore a solid consideration for a variety of content transfer systems. Second, we presented four designs for state-of-the-art pocket transfer techniques employing three different modalities as well as a combination of two modalities. Third, we recognized strengths and weaknesses for each technique, and presented guidelines to help researchers and practitioners choose the most suitable modalities and techniques for their content transfer systems. Finally, we presented a novel user study design, wherein participants completed tasks that included the full interaction process, including preparation for, and halting of, the interaction. This way, we argue we reached more ecologically valid results. We encourage researchers to utilize such approaches in future studies.

"
"Understanding the Family Perspective on the Storage, Sharing and Handling of Family Civic Data",https://dl.acm.org/authorize?N655416,"
Across social care, healthcare and public policy, enabled by the ""big data"" revolution (which has normalized large-scale data-based decision-making), there are moves to ""join up"" citizen databases to provide care workers with holistic views of families they support. In this context, questions of personal data privacy, security, access, control and (dis-)empowerment are critical considerations for system designers and policy makers alike. To explore the family perspective on this landscape of what we call Family Civic Data, we carried out ethnographic interviews with four North-East families. Our design-game-based interviews were effective for engaging both adults and children to talk about the impact of this dry, technical topic on their lives. Our findings, delivered in the form of design guidelines, show support for dynamic consent: families would feel most empowered if involved in an ongoing co-operative relationship with state welfare and civic authorities through shared interaction with their data. Using our novel Family Design Games technique, we have shown that families – both adults and children – can be engaged in meaningful conversation about the complex and esoteric topic of Family Civic Data. Through these homebased activities we have gained a rich qualitative understanding of families’ requirements for the handling of their data, and derived guidelines that can inform the design of future data-handling systems and associated organizational processes: state organizations must support a dynamic consent model of data handling, and plan for a new paradigm of co-operative, data-based relationships with families, one where meaningful, representative data is nurtured for mutual benefit and families remain involved throughout.

"
The Effects of Adding Search Functionality to Interactive Visualizations on the Web,https://dl.acm.org/authorize?N655417,"
The widespread use of text-based search in user interfaces has led designers in visualization to occasionally add search functionality to their creations. Yet it remains unclear how search may impact a person's behavior. Given the unstructured context of the web, users may not have explicit information-seeking goals and designers cannot make assumptions about user attention. To bridge this gap, we observed the impact of integrating search with five visualizations across 830 online participants. In an unguided task, we find that (1) the presence of text-based search influences people's information-seeking goals, (2) search can alter the data that people explore and how they engage with it, and (3) the effects of search are amplified in visualizations where people are familiar with the underlying dataset. These results suggest that text-search in web visualizations drives users towards more diverse information seeking goals, and may be valuable in a range of existing visualization designs. Across the web, designers build thousands of data-dense visualizations for the public to explore and comprehend. Surprisingly, only a very small subset of these visualizations are accompanied by text-based search mechanisms. While textbased search has often been used in conjunction with large datasets for analysts, our results suggest that its inclusion in everyday visualizations, even those with relatively small amounts of data, may encourage engagement and support user information seeking goals that are difficult with other forms of interaction. Through experiments with five visualizations, we find that in most visualizations, a majority of users will use text-based search features if present, and that search can shape people’s experience and behavior with visualizations. Results of the experiments also indicate the average participant who used text-based search engaged with individual data items for longer, and explored different parts of the data. The results of these experiments have practical implications for design, and more broadly serve as a case study in how interactive data visualizations can be augmented to support diverse information seeking goals.

"
Towards Design Principles for Visual Analytics in Operations Contexts,https://dl.acm.org/authorize?N655418,"
Operations engineering teams interact with complex data systems to make technical decisions that ensure the operational efficacy of their missions. To support these decision-making tasks, which may require elastic prioritization of goals dependent on changing conditions, custom analytics tools are often developed. We were asked to develop such a tool by a team at the NASA Jet Propulsion Laboratory, where rover telecom operators make decisions based on models predicting how much data rovers can transfer from the surface of Mars. Through research, design, implementation, and informal evaluation of our new tool, we developed principles to inform the design of visual analytics systems in operations contexts. We offer these principles as a step towards understanding the complex task of designing these systems. The principles we present are applicable to designers and developers tasked with building analytics systems in domains that face complex operations challenges such as scheduling, routing, and logistics. Tasked with improving the workflow for Mars Exploration Rover telecommunications operators, we transformed a complex, distributed process into a singular interface that supports efficient and flexible decision-making. Reflecting on our design process, we discovered that many of our findings could be distilled into principles broadly applicable across similar domains. In this paper, we have presented seven design principles to inform the design of visual analytics systems in operations contexts. We hope that this contribution inspires continued examination of this space, enhancing the experience of individuals and teams engaged in this constant yet supple work practice.

"
Displaying Invisible Objects: Why People Rarely Re-read E-books,https://dl.acm.org/authorize?N655419,"
This study of paper and e-books investigates how specific affordances of physical and digital objects relate to people's valuations and uses of those objects over time. We found that while the visibility of paper books amplified the meaningfulness of organizational and display actions taken with regards to those objects, the systems that supported interactions with e-books instead tended to make such actions less meaningful. We argue that these systems also discouraged re-uses of e-books for most participants -- the important exceptions being several participants who used the book-focused social networking site Goodreads. This paper details how the affordances and limitations that resulted from the material constructions of paper and e-books impacted participants' uses of and feelings towards those objects, and examines the implications of using a supplementary online system for displaying digital objects. Through an examination of people’s practices of usage for paper and e-books, this paper has demonstrated how visibility of paper books made maintenance practices meaningful and supported object re-use, both of which had important implications for identity-related uses of objects. We also established an understanding of how returning to favorite objects over time (through re-reading, in this case) contributed to participants’ changing understandings of their own identities, and how this practice was not typically undertaken with e-books – unless their owners engaged in supplementary display practices. Finally, we explored the implications of these supplementary display practices, highlighting in particular how they separated display from ownership. We close by noting that research from HCI on human interactions with physical and digital objects, in establishing an understanding of how the visibility of physical objects (in contrast to digital objects) reinforces people’s relationships of ownership to those objects, has contributed not only to knowledge within this field but to the field of material culture studies as well.
"
A Visual Interaction Cue Framework from Video Game Environments for Augmented Reality,https://dl.acm.org/authorize?N655410,"
Based on an analysis of 49 popular contemporary video games, we develop a descriptive framework of visual interaction cues in video games. These cues are used to inform players what can be interacted with, where to look, and where to go within the game world. These cues vary along three dimensions: the purpose of the cue, the visual design of the cue, and the circumstances under which the cue is shown. We demonstrate that this framework can also be used to describe interaction cues for augmented reality applications. Beyond this, we show how the framework can be used to generatively derive new design ideas for visual interaction cues in augmented reality experiences. Video game designers have developed and honed a visual language for interaction cues. Our interaction cue framework illuminates the roles of purpose, visual design and interaction design for these cues. Further, we find that it can describe interaction cues from AR experiences, and we show how designers can use this framework to generate new designs for interaction cues in AR. Designers of AR experiences as well as those building AR platforms (e.g. [7]) will be able to build from this work to develop a parallel visual language of interaction cues for AR.
"
HARK No More: On the Preregistration of CHI Experiments,https://dl.acm.org/authorize?N655411,"
Experimental preregistration is required for publication in many scientific disciplines and venues. When experimental intentions are preregistered, reviewers and readers can be confident that experimental evidence in support of reported hypotheses is not the result of HARKing, which stands for Hypothesising After the Results are Known. We review the motivation and outcomes of experimental preregistration across a variety of disciplines, as well as previous work commenting on the role of evaluation in HCI research. We then discuss how experimental preregistration could be adapted to the distinctive characteristics of Human-Computer Interaction empirical research, to the betterment of the discipline. Many scientific disciplines have concluded that experimental preregistration is an essential part of their discipline. It overcomes or reduces many substantial problems that are demonstrable in their literature, including HARKing, publication bias, and the file drawer effect. When registries are appropriately equipped, experimental preregistration also facilitates replication and meta-analyses. The nature of HCI empirical research raises some challenges for preregistration, particularly its strong and appropriate reliance on iterative exploratory design and evaluation. However, preregistration can offer advantages even for exploratory work. Regardless of exploratory studies, much of the research knowledge within HCI is derived from formal experiments that make use of NHST, and for these studies our discipline is every bit as susceptible to problems of absent preregistration as any other; the potential benefits are equivalent too. Ideally preregistration would promote empirical publications being judged on the quality of the research, including the potential interest of the hypothesis, not simply whether they give the ‘right’ results. HCI research can sharpen its methods and rigour by introducing experimental preregistration.
"
Designing Coherent Gesture Sets for Multi-scale Navigation on Tabletops,https://dl.acm.org/authorize?N655412,"
Multi-scale navigation interfaces were originally designed to enable single users to explore large visual information spaces on desktop workstations. These interfaces can also be quite useful on tabletops. However, their adaptation to co-located multi-user contexts is not straightforward. The literature describes different interfaces, that only offer a limited subset of navigation actions. In this paper, we first identify a comprehensive set of actions to effectively support multi-scale navigation. We report on a guessability study in which we elicited user-defined gestures for triggering these actions, showing that there is no natural design solution, but that users heavily rely on the now-ubiquitous slide, pinch and turn gestures. We then propose two interface designs based on this set of three basic gestures: one involves two-hand variations on these gestures, the other combines them with widgets. A comparative study suggests that users can easily learn both, and that the gesture-based, visually-minimalist design is a viable option, that saves display space for other controls. This paper presents and structures a space of sixteen multiscale navigation actions for collaborative work on tabletops. A guessability study supports the hypothesis that users expect an interaction vocabulary based on a core set of three basic gestures (SLIDE, PINCH and TURN), which have now become a standard for touch-based interfaces, popularized by applications such as Google Maps. We propose and evaluate two possible designs for triggering this set of sixteen desirable actions without resorting to complex gestures. Interestingly, even if our gesture-based design provides less visual guidance than our widget-based design, participants in our study were able to recall how to trigger actions equally well with one or the other, and this even when coming back more than a day after having learnt how to use them. This suggests that interface designers could include a quick start guide for gesture-based navigation controls, in order to get a minimalist, more pleasant design and potentially save display space for other controls. By giving users the possibility to manipulate text elements (labels, annotations, etc.) independently from the rest of the representation, we hint at multi-layer manipulation in touchbased multi-scale interfaces. We plan to work on a design solution that scales with more than two layers. This probably requires considering a hybrid approach mixing widgets, gestures and tangibles in order to avoid using complex gestures and generating too much visual clutter. We also plan to study multi-scale navigation in multi-surface environments that feature several large screens (tabletops and walls), and small devices (tablets and smartphones). Our distributed architecture and Web-based implementation already support the instantiation of detailed views on separate devices, facilitating the implementation of tangible personal views similar to those in, e.g., [30]. Besides the technical aspects, these complex setups raise many interesting research questions in terms of interaction design, such as how to facilitate the transition between the context and the personal views, or how to have a personal view linked to several contexts.

"
HindSight: Enhancing Spatial Awareness by Sonifying Detected Objects in Real-Time 360-Degree Video,https://dl.acm.org/authorize?N655413,"
Our perception of our surrounding environment is limited by the constraints of human biology. The field of augmented perception asks how our sensory capabilities can be usefully extended through computational means. We argue that spatial awareness can be enhanced by exploiting recent advances in computer vision which make high-accuracy, real-time object detection feasible in everyday settings. We introduce HindSight, a wearable system that increases spatial awareness by detecting relevant objects in live 360-degree video and sonifying their position and class through bone conduction headphones. HindSight uses a deep neural network to locate and attribute semantic information to objects surrounding a user through a head-worn panoramic camera. It then uses bone conduction headphones, which preserve natural auditory acuity, to transmit audio notifications for detected objects of interest. We develop an application using HindSight to warn cyclists of approaching vehicles outside their field of view and evaluate it in an exploratory study with 15 users. Participants reported increases in perceived safety and awareness of approaching vehicles when using HindSight. We introduced HindSight, a wearable system that increases spatial awareness by detecting relevant objects in live, egocentric 360-degree video and sonifying their attributes through bone conduction headphones. HindSight draws upon advances in computer vision and work in delivering continuous feedback for physical tasks to identify points of interest in a user’s surroundings and notify the user when necessary to redirect their attention. Our analysis suggests that at current detection performance, bicyclists can be notified in time to react to dangers when vehicles travel up 8.6 m/s faster than the cyclists. This margin may be sufficient for many, but not all urban cycling situations. Progress in camera technology and object classification can further improve on this threshold. In our exploratory study, we find HindSight increased users’ reported comfort, awareness, reaction time, and safety, and identify potential avenues for future work, such as reducing the recall rate of object detection and designing broader audio experiences for users. While our prototype is somewhat limited by the need to wear a laptop with a powerful GPU, multiple hardware companies are currently developing chips can run deep neural networks in real time, which would make a truly portable solution feasible. Beyond the domain of cycling, we believe that combining an enhanced awareness of visual periphery with the rich semantic understanding of objects and scenes from computer vision techniques has the potential to enable an entire new class of applications that improve on unaided human capabilities.
"
Uncertainty Displays Using Quantile Dotplots or CDFs Improve Transit Decision-Making,https://dl.acm.org/authorize?N655414,"
Everyday predictive systems typically present point predictions, making it hard for people to account for uncertainty when making decisions. Evaluations of uncertainty displays for transit prediction have assessed people's ability to extract probabilities, but not the quality of their decisions. In a controlled, incentivized experiment, we had subjects decide when to catch a bus using displays with textual uncertainty, uncertainty visualizations, or no-uncertainty (control). Frequency-based visualizations previously shown to allow people to better extract probabilities (quantile dotplots) yielded better decisions. Decisions with quantile dotplots with 50 outcomes were(1) better on average, having expected payoffs 97% of optimal(95% CI: [95%,98%]), 5 percentage points more than control (95% CI: [2,8]); and (2) more consistent, having within-subject standard deviation of 3 percentage points (95% CI:[2,4]), 4 percentage points less than control (95% CI: [2,6]).Cumulative distribution function plots performed nearly as well, and both outperformed textual uncertainty, which was sensitive to the probability interval communicated. We discuss implications for real time transit predictions and possible generalization to other domains. In this paper, we demonstrate that including uncertainty displays in realtime transit decision-making can produce higher quality decisions. Using a method adapted from experimental economics, we successfully tracked serial decision making to evaluate what type of uncertainty displays produced the highest quality decisions. We found that cumulative distribution function (CDF) plots and low-density quantile dotplots produce more accurate and consistent decisions compared to other uncertainty visualizations, textual displays, and displays with no uncertainty. We also found that when using uncertainty displays, decision quality can improve over time. The types of displays that we have found to be the best for supporting decision-making in the transit have also been shown to be more accurate at estimating probability intervals more generally, suggesting that our results may generalize to similar situations where the same type of uncertainty information would help inform decision-making.

"
What to Put on the User: Sensing Technologies for Studies and Physiology Aware Systems,https://dl.acm.org/authorize?N655425,"
Fitness trackers not just provide easy means to acquire physiological data in real-world environments due to affordable sensing technologies, they further offer opportunities for physiology-aware applications and studies in HCI; however, their performance is not well understood. In this paper, we report findings on the quality of 3 sensing technologies: PPG-based wrist trackers (Apple Watch, Microsoft Band 2), an ECG-belt (Polar H7) and reference device with stick-on ECG electrodes (Nexus 10). We collected physiological (heart rate, electrodermal activity, skin temperature) and subjective data from 21 participants performing combinations of physical activity and stressful tasks. Our empirical research indicates that wrist devices provide a good sensing performance in stationary settings. However, they lack accuracy when participants are mobile or if tasks require physical activity. Based on our findings, we suggest a textitDesign Space for Wearables in Research Settings and reflected on the appropriateness of the investigated technologies in research contexts. By this work, we first contribute a comparison between PPG, wrist devices (Apple Watch, Microsoft Band 2) against an ECG chest strap (Polar H7 chest belt) and a laboratory measurement instrument with stick-on ECG technology (Nexus 10 kit) under different physical and stressful conditions. To evaluate the reliability of the named sensing technologies, we investigated the differences in physiological data measured by the devices (Hypotheses 1a and H1b) confirming that PPGwearables tend to be less accurate in movement and the data gets less suitable for sensitive research settings. We further checked the influence of stress on physiological data under stationary and physical activity (Hypothesis 2) which could be only partly confirmed owed to the lack of accuracy in the devices. As another contribution, we could show that perceived stress and arousal (tense and wake) correlate with the physiological data suggesting a strong relation between physiological and subjectively felt stress, whereas there no correlations for valence and dominance observed (Hypotheses 3). Based on our findings, we lastly contribute a Design Space for Wearables Used in Research Settings addressing five dimension covering important criteria for choosing an appropriate measurement tool for research purposes. In future work, we plan to investigate noise reduction by using the accelerometer data, which is readily available in most consumer devices. Therefore, we will compare more wearables involving new products using improved sensors and data extraction algorithms. Also most of these wearables are not scientifically validated for their accuracy and validity. Novel consumer devices even target well-being aspects and stress such as the Garmin Vivosmart 310, which claims to use HRV to calculate a proprietary stress score throughout the day. In terms of stress and emotion detection, we plan to have a closer look at stress detection through wearables in the wild as there are already approaches based on mobile sensing data [10, 40]. The combination of those approaches with wearable physiological data could lead to more accurate predictions and models [20]. By this work we believe to have presented a first step towards assessing sensing technologies in wearables for their reliability and accuracy, as well as having provided fruitful insights for other researchers when it comes to decide which measurement tool to use in a study.
"
Lost in Migration: Information Management and Community Building in an Online Health Community,https://dl.acm.org/authorize?N655426,"
The ever-growing volume of information within online health communities (OHCs) presents an urgent need for new solutions that improve the efficiency of information organization and retrieval for their members. To meet this need, OHCs may choose to adopt off-the-shelf platforms that provide novel features for information management, but were not specifically designed to meet these communities' needs. The questions remain, however, as to the impact of these new platforms on social dynamics within OHCs and their well-being. To examine these questions, we qualitatively studied a migration of a popular OHC, focusing on diabetes self-management, between two off-the-shelf social computing platforms. Despite improving information management, the migration served as a catalyst to reveal the importance of features for identity management and closed circle communication that were not apparent to either the management or the membership of the community. We describe the study and draw implications for research and design for OHCs. In this work we studied changes in the attitudes, perception, and social dynamics of an OHC after migration to a new technical platform. The study further highlighted the need to balance informational and socio-emotional support features in the SCP for OHCs. Further, it pointed to the difficulty and the importance of recognizing the true importance of social behaviors in the community and their connections to SCP features, and provided descriptions of these connections. Finally, the study suggested the need for new approaches to the design of OHC platforms that allow for easy and flexible configuration of features based on prioritization of the value-driven behaviors types, and monitoring tools for discovering new potential connections between those behaviors and SCP features.
"
On Visual Granularity: Collocated Sales Meeting Interactions in the Machine Industry,https://dl.acm.org/authorize?N655427,"
Visual representations are being used in typical sales meetings of the machine industry to exchange information and support social interactions. In these meetings, sales representatives design for granularity by taking into account verbal and visual details of communication. Our article builds on increasingly occurring collocated interactions in sales meetings investigating the social relevance of mobile devices in face-to-face settings. The article aims to understand the supporting and disturbing role of visual granularity in sales meetings and develops design implications for interaction designers. We conducted an ethnographic study of sales meetings in material handling and paper machine industries, including Conversation Analysis (CA) of video recordings, and involving groups of professional analysts that are seldom used in HCI. Our findings draw evidence from sales meetings and design processes on successful and unsuccessful use of granularity in visual representations. Finally, we propose seven design guidelines for visual granularity striving to understand buyers' perceptions and visual qualities. Sales representatives of the machine industry increasingly use mobile devices in collocated settings, taking into account how granularity in spoken and visual form and details shape the conversation during meetings. The article focused on visual granularity either supporting or disturbing sales conversation. We conducted two years of collaboration with two machine industries obtaining different types of data from sales settings. These included multiple Conversation Analysis (CA) sessions of videoed excerpts by groups of researchers from the fields of interaction design, sociology, linguistics and organisational management. Findings elaborate insights on granularity in understanding specific messengers and recipients, contexts, allocating senses on visual details and original ownership of information in sales conversation. Seven design implications firstly revolve around seeking information about devices and buyers from sales reps, customer databases and meetings. Secondly, these result as qualities of designing glanceable visualisations, being relevant to buyers with considerations of time used in designing, complexity and adaptability. The study contributes to the HCI community in terms of visualising granularity in contexts involving extensive information flow and using groups of professional conversations analysts. In future research, we concentrate on gaze detection and sound to enrich the understanding on connections of utterances and visual details.
"
Distance and Attraction: Gravity Models for Geographic Content Production,https://dl.acm.org/authorize?N655428,"
Volunteered Geographic Information (VGI), such as contributions to OpenStreetMap and geotagged Wikipedia articles, is often assumed to be produced locally. However, recent work has found that peer-produced VGI is frequently contributed by non-locals. We evaluate this approach across hundreds of content types from Wikipedia, OpenStreetMap, and eBird, and show that these models can describe more than 90% of ""VGI flows"" for some content types. Our findings advance geographic HCI theory, suggesting some spatial mechanisms underpinning VGI production. We also discuss design implications that can help (a) human and algorithmic consumers of VGI evaluate the perspectives it contains and (b) address geographic coverage variations in these platforms (e.g. via more effective volunteer recruitment strategies). In this paper, we showed that VGI contributions can be modeled effectively using spatial interaction techniques, and gravity models in particular. We also explored the implications of these findings for our understanding of VGI, for stakeholders currently managing large VGI communities, and for the development of future VGI platforms.
"
MatchSticks: Woodworking through Improvisational Digital Fabrication,https://dl.acm.org/authorize?N655429,"
Digital fabrication tools have broadened participation in making and enabled new methods of rapid physical prototyping across diverse materials. We present a novel smart tool designed to complement one of the first materials employed by humans - wood - and celebrate the fabrication practice of joinery. Our tool, MatchSticks, is a digital fabrication system tailored for joinery. Combining a portable CNC machine, touchscreen user interface, and parametric joint library, MatchSticks enables makers of varying skill to rapidly explore and create artifacts from wood. Our system embodies tacit woodworking knowledge and distills the distributed workflow of CNC tools into a hand tool; it operates on materials existing machines find difficult, produces assemblies much larger than its workspace, and supports the parallel creation of geometries. We describe the workflow and technical details of our system, present example artifacts produced by our tool, and report results from our user study. In this paper, we introduced MatchSticks, a digital fabrication system for joinery. Our tool introduces complementary capabilities to the domain of digital and hand tools used in woodworking while further exploring the area of interactive smart tools. We introduced novel hardware that is capable of expressing a large vocabulary of joints as well as software for design and fabrication that is similarly tuned to the domain of joinery. Our system enables the fabrication of geometries and assemblies that are (1) difficult to create using other tools, (2) larger than the tool itself, and (3) parallelizable in production. By eliminating the need to interact with a separate computer for the design of these joints, we explore how users can much more easily express their design intent directly to the CNC fabrication tool. This directness both empowers novices to incorporate joinery into their practice, while providing experts with an adaptive palette of joinery designs. We validate capabilities of our tool through the production of example artifacts, and a user study of a tutorial workflow.
"
Visuo-Haptic Illusions for Improving the Perceived Performance of Shape Displays,https://dl.acm.org/authorize?N655420,"
In this work, we utilize visuo-haptic illusions to improve the perceived performance of encountered-type haptic devices, specifically shape displays, in virtual reality. Shape displays are matrices of actuated pins that travel vertically to render physical shapes; however, they have limitations such as low resolution, small display size, and low pin speed. To address these limitations, we employ illusions such as redirection, scaling, and retargeting that take advantage of the visual dominance effect, the idea that vision often dominates when senses conflict. Our evaluation of these techniques suggests that redirecting sloped lines with angles less than 40 degrees onto a horizontal line is an effective technique for increasing the perceived resolution of the display. Scaling up the virtual object onto the shape display by a factor less than 1.8x can also increase the perceived resolution. Finally, using vertical redirection a perceived 3x speed increase can be achieved. Shape displays can provide realistic haptic feedback in virtual reality, however the low resolution, small display size, and the low pin speed of these devices limit their usage. Visuo-haptic illusions such as scaling, retargeting, and redirection can be utilized to address these limitations and improve the perceived performance of shape displays. We suggest two methods, angle redirection and upscaling, for improving the perceived resolution of shape displays. We also propose two approaches, retargeting and downscaling, for increasing the perceived size of the display. Finally, we increase the perceived speed of the pins by retargeting and vertical redirection. The results of our evaluation suggest that redirecting sloped lines with angles less than 40◦ onto a horizontal line, and scaling up to 1.8x are effective techniques for increasing the perceived resolution of the display. Moreover, a 3x perceived speed increase can be achieved by vertical redirection and modifying the C/D ratio.

"
A Schnittmuster for Crafting Context-Sensitive Toolkits,https://dl.acm.org/authorize?N655421,"
DIY-making can be an expensive pastime if makers are relying on ready-made toolkits, specialised materials and off-shelf components. Many prefabricated commercial kits seek to lower the learning barrier of making and to help beginners to successfully take their first steps in engineering. However, as soon as the novices become a little more advanced, these toolkits often do not fit the specific requirements of personal maker projects anymore. We introduce the idea of a Schnittmuster (or a meta-toolkit) as a novel approach to toolkit design that seeks to address these creativity-limiting factors as well as practical entrance hurdles. To demonstrate the adaptive power of the Schnittmuster concept, we discuss an exemplar in the context of capacitive touch sensing (FlexE-Touch). Implemented under the constraints of materials, user skill sets and making environments, we illustrate how the Schnittmuster facilitated four cheap and flexible toolkit instantiations for crafting custom touch sensor electrodes. In this paper, we presented a Schnittmuster approach to designing custom toolkits and illustrated it using the example of FlexE-Touch tailored for crafting capacitive touch sensor electrodes. A Schnittmuster is highly flexible and if used by empathic facilitators can react to a range of otherwise limiting project constraints. If funding is the matter, the costs can be lowered by using cheaper materials. If time and the availability of contributors are limited, it can adapt the manufacturing process by distributing the tasks. If learning about the underlying electronic concepts is the main goal, the activity can provide steps for guided recreation while maintaining space for personalisation and self-paced meaning-making. With the Schnittmuster, we contribute a novel critical perspective on and an alternative approach to designing maker toolkits that can assist facilitators of DIY electronics activities for novice makers.
"
Repurposing Emoji for Personalised Communication: Why 🍕 means “I love you”,https://dl.acm.org/authorize?N655422,"
The use of emoji in digital communication can convey a wealth of emotions and concepts that otherwise would take many words to express. Emoji have become a popular form of communication, with researchers claiming emoji represent a type of ""ubiquitous language"" that can span different languages. In this paper however, we explore how emoji are also used in highly personalised and purposefully secretive ways. We show that emoji are repurposed for something other than their ""intended"" use between close partners, family members and friends. We present the range of reasons why certain emoji get chosen, including the concept of ""emoji affordance"" and explore why repurposing occurs. Normally used for speed, some emoji are instead used to convey intimate and personal sentiments that, for many reasons, their users cannot express in words. We discuss how this form of repurposing must be considered in tasks such as emoji-based sentiment analysis. In this paper we have highlighted how people using computer and mobile mediated communication have come to repurpose certain emoji. By using the ambiguity in meaning that emoji naturally have, users can create a shared personal meaning between themselves and another person, or small group, creating a new cultural understanding of particular emoji. This in turn can help people feel closer to one another. Previous work has tended to explore emoji understanding on a large cultural level, whereas this work looks at emoji understanding at a micro scale. Results of this paper will contribute to future work into emoji sentiment analysis, as the work highlights that emoji do not always correspond to their intended, nor culturally accepted meanings. At times emoji are chosen at random to mean a specific concept, and equally some emoji are chosen purposefully because they convey the complete opposite of the intended sentiment. This implies caution is required when using machine learning techniques to understand the meaning and use of emoji. As with other papers in this area [13,21], we also conclude that a universal rendering of emoji may be required to standardise how they appear across platforms. It is well understood that emoji facial expressions can be misconstrued, for example, but this paper also highlights that discrepancies in rendering can also affect a whole range of emoji that people choose to use due to the way they are drawn. The current method of rendering emoji in a range of different ways may be preventing users from using the implicit affordance of emoji. Additionally, the redrawing of emoji must be carefully considered. Just as users reacted badly to Apple redrawing the Peach emoji, smaller communities of other users may be affected by redrawing of any number of emoji currently in use because of how they look, and not what they represent.

"
More Stars or More Reviews?,https://dl.acm.org/authorize?N655423,"
The large majority of reputation systems use features such as star ratings and reviews to give users a reputation in online peer-to-peer markets. Both have been shown to be effective for signaling trustworthiness. However, the exact extent to which these features can change perceptions of users' trustworthiness remains an open question. Using data from an online experiment conducted on Airbnb users, we investigate which of the two types of reputation information --average star rating or the number of reviews --is more important for signaling a user's trustworthiness. We find that the relative effectiveness of ratings and reviews differ depending on whether reputation has a strong or a weak differentiation power. Our findings show that reputation effects are contingent on and susceptible to the context created by the alternative choices presented to users, highlighting how reputation information is displayed can drastically alter their efficacy for engendering trust. We set out to accomplish two things with our current study. First, we wanted to test for the exact effects of the two core components of the conventional reputation system –average star ratings and review counts –on perceptions of trustworthiness. We tried to achieve this by conducting an investment game on a large population of Airbnb users. Second, we wanted to understand how design choices can affect assessment of trustworthiness by investigating users’ responses to reputation contexts. We achieved this by constructing and randomly assigning subjects to two reputation conditions – “reputational contexts"" –whereby we varied the distributional characteristics of the reputations of profiles we displayed to subjects. We found that the effect of a profile going from having an average rating of 4 stars to 5 stars is roughly equivalent to having a review count increase of 10 or more –from having 1-3 reviews to having 11+ reviews. Yet we also found that when a reputation system has enough differentiating power, the number of review carries more weight than the average ratings. Indeed, what is interesting about these results is that the same exact reputation information can have varying efficacy for signaling trustworthy behavior depending on what alternatives that users have. Our work thus contributes to the existing work on reputation systems by quantifying exactly how two conventional feature of these systems affect users’ perceptions of others’ trustworthiness and identifying how their effects vary depending on various design choices.
"
Self-Reflection and Personal Physicalization Construction,https://dl.acm.org/authorize?N655424,"
Self-reflection is a central goal of personal informatics systems, and constructing visualizations from physical tokens has been found to help people reflect on data. However, so far, constructive physicalization has only been studied in lab environments with provided datasets. Our qualitative study investigates the construction of personal physicalizations in people's domestic environments over 2-4 weeks. It contributes an understanding of (1) the process of creating personal physicalizations, (2) the types of personal insights facilitated, (3) the integration of self-reflection in the physicalization process, and (4) its benefits and challenges for self-reflection. We found that in constructive personal physicalization, data collection, construction and self-reflections are deeply intertwined. This extends previous models of visualization creation and data-driven self-reflection. We outline how benefits such as reflection through manual construction, personalization, and presence in everyday life can be transferred to a wider set of digital and physical systems. Our study provides a first exploration of constructive physicalization in a personal real-world context. We found that the process facilitated deep reflections on the data, personal context, actions and values. We discussed how these reflections are deeply embedded in and supported by the manual construction. The process allowed people to personalize representations and integrate these within their personal environment to spark moments of reflection and conversations. Our findings point to exciting directions for design and future research in the context of both physicalization creation and tools for self-reflection. Our results further highlight the potential of physicalization construction in a personal context, including the fluid integration with people’s routines, benefits for capturing qualitative and subjective aspects, supporting shared experiences, as well as the potential of using constructive visualizations as a generative tool for planning and creative exploration.
"
G2G: The Design and Evaluation of a Shared Calendar and Messaging System for Grandparents and Grandchildren,https://dl.acm.org/authorize?N655435,"
Distance separated grandparents and grandchildren often face challenges in staying connected. To explore this topic, we designed G2G, a shared calendar and video messaging system to connect young children (ages 5-10) with their grandparents over distance. Our design focused on providing grandparents and grandchildren with an awareness of each other's lives to support conversations and design elements to help reduce the need for parent scaffolding. A field study with two grandparent-grandchild pairs over two months showed that systems designed around structured communication can help young children develop a routine around staying in touch with their remote grandparents. Autonomy in maintaining awareness can help children to be engaged more easily. This suggests that designs focusing on connecting young children to their grandparents over distance should be flexible yet structured and designing to reduce parental scaffolding can lead to positive effects and strengthened relationships.
"
Playing Close to Home: Interaction and Emerging Play in Outdoor Play Installations,https://dl.acm.org/authorize?N655436,"
Outdoor play is becoming an increasingly marginalised activity in the urban landscape. Even in HCI, research on interactive solutions for outdoor play has largely been limited to special areas and in particular playgrounds. But children play everywhere, and especially play close to home is central in children's play activities. In this article we draw upon knowledge about designing for children's play in interaction design as well as in landscape architecture, to study how interactive play installations can be integrated in outdoor environments of a residential area. We present a field study in which digitally enhanced play installations were installed, in dialogue with the landscape, in between the buildings of a residential area. We focus on how emerging play activities made use of the installations as well as of the surrounding landscape in expected as well as unexpected ways. Based on the observations, we discuss how residential play is special, and how this affects how to design for it. The design for play close to home meets with specific challenges, related to the ways in which children have access, to and learn to know, such installations. Through our study, we have highlighted how both the physical aspects of the space and the social practices of a residential area work to influence the play behavior that emerges around such installations. The installations in this trial were temporary, which made it difficult to fully arrive at an integrated design of technology and landscape. The DigiFys project is currently approaching its final phase, in which we will be making permanent installations in close to home places. Through the close collaboration with municipality representatives, we have been given the opportunity to include interactive installations in the rebuilding of two recreational areas in the immediate vicinity of housing areas. These installations will allow us to design for, and study, long term engagement, as well as force us to address organizational issues related to the upkeep and maintenance of equipment over time. Creating permanent installations provides the opportunity to not only adapt the installations to an existing landscape, but allows us to also in part shape that landscape. The permanency of these installations wiil also be the ultimate challenge for and final test of the longevity of the play activities that emerge around this type of installations.

"
"CFar: A Tool to Increase Communication, Productivity, and Review Quality in Collaborative Code Reviews",https://dl.acm.org/authorize?N655437,"
Collaborative code review has become an integral part of the collaborative design process in the domain of software development. However, there are well-documented challenges and limitations to collaborative code review---for instance, high-quality code reviews may require significant time and effort for the programmers, whereas faster, lower-quality reviews may miss code defects. To address these challenges, we introduce CFar, a novel tool design for extending collaborative code review systems with an automated code reviewer whose feedback is based on program-analysis technologies. To validate this design, we implemented CFar as a production-quality tool and conducted a mixed-method empirical evaluation of the tool usage at Microsoft. Through the field deployment of our tool and a laboratory study of professional programmers using the tool, we produced several key findings showing that CFar enhances communication, productivity, and review quality in human--human collaborative code review. In this paper, we introduced the novel CFar tool design for extending collaborative code review systems with an automated code reviewer that uses program analyses to enhance communication, productivity, and review quality in human–human collaborative code review. Our mixed-method empirical evaluation of CFar produced several key findings: • RQ1 (communication): 45% of programmers who used CFar for their work indicated that the tool increased communication, and over 60% indicated that it enhanced collaboration. (Only 7% and 14% of participants, respectively, disagreed with these benefits.) • RQ2 (productivity): 38% of programmers who used CFar indicated that it increased their productivity (versus only 19% who disagreed), and multiple participants indicated that a key reason was that it freed them from dealing with shallow defects. • RQ3 (code quality): 48% of programmers who used CFar indicated that it helped increase code quality (versus one who disagreed), and several expressed that a key reason was that it identified defects that a human reviewer would miss. • RQ4 (user opinions): All but one programmer who used CFar found it useful, and 69% expressed that they liked the tool (versus only one programmer who did not). We hope that CFar and our findings represent a substantial step toward more efficient and more effective collaborative code review systems. A promising direction for future work is to explore novel ways in which a human may interact with an automated code reviewer. For instance, to debug an analysis comment, a review participant might require more information than that provided by the analysis warning. In such cases, the automated reviewer could display, upon request, the code parts that are to blame for the warning and even suggested fixes. Our tool design and implementation elicited considerable optimism from programmers at Microsoft, which revealed an even larger opportunity for tools to improve code reviewing. Tools like CFar could continue to help programmers, as was the case for one of our participants.

"
Announcing Pregnancy Loss on Facebook: A Decision-Making Framework for Stigmatized Disclosures on Identified Social Network Sites,https://dl.acm.org/authorize?N655438,"
Pregnancy loss is a common experience that is often not disclosed in spite of potential disclosure benefits such as social support. To understand how and why people disclose pregnancy loss online, we interviewed 27 women in the U.S. who are social media users and had recently experienced pregnancy loss. We developed a decision-making framework explaining pregnancy loss disclosures on identified social network sites (SNS) such as Facebook. We introduce network-level reciprocal disclosure, a theory of how disclosure reciprocity, usually applied to understand dyadic exchanges, can operate at the level of a social network to inform decision-making about stigmatized disclosures in identified SNSs. We find that 1) anonymous disclosures on other sites help facilitate disclosure on identified sites (e.g., Facebook), and 2) awareness campaigns enable sharing about pregnancy loss for many who would not disclose otherwise. Finally, we discuss conceptual and design implications. CAUTION: This paper includes quotes about pregnancy loss. This work contributes a framework that explains disclosures of pregnancy loss on identified SNSs. This framework includes six types of decision factors: self-related, audiencerelated, societal, platform and affordance-related, networklevel, and temporal. While pregnancy loss was the focus in this paper, we suggest that this framework could be applicable to other sensitive disclosures on identified SNSs where people connect with others they know and use their physical world identities. We encourage researchers to evaluate our framework in other contexts. Within our framework, we introduce a theoretical construct, networklevel reciprocal disclosure, as a concept that can motivate social computing system designs to better promote sensitive disclosures and social support exchange. Further, we found that prior anonymous online participation facilitated disclosures of pregnancy loss on Facebook, that one-to-many disclosures on Facebook are appreciated because by doing so people in distress can avoid many painful one-to-one disclosures, and that social media awareness campaigns prompt disclosures motivated by network-level, societal, and temporal factors. Taken together, awareness campaigns, the efficiency of one-to-many disclosures, and opportunities for anonymous lower-risk disclosures elsewhere contribute to women’s decisions to disclose pregnancy loss experiences on identified SNSs, which, through the mechanism of network-level reciprocation, creates an increasingly disclosure-friendly context for those who come after.
"
Conceptualizing Disagreement in Qualitative Coding,https://dl.acm.org/authorize?N655439,"
Collaborative qualitative coding often involves coders assign- ing different labels to the same instance, leading to ambiguity. We refer to such an instance of ambiguity as disagreement in coding. Analyzing reasons for such a disagreement is essential-- both for purposes of bolstering user understanding gained from coding and reinterpreting the data collaboratively, and for negotiating user-assigned labels for building effective machine learning models. We propose a conceptual definition of collective disagreement using diversity and divergence within the coding distributions. This perspective of disagreement translates to diverse coding contexts and groups of coders irrespective of discipline. We introduce two tree-based ranking metrics as standardized ways of comparing disagreements in how data instances have been coded. We empirically validate that, of the two tree-based metrics, coders' perceptions of dis- agreement match more closely with the n-ary tree metric than with the post-traversal tree metric. Human intuition for comparing across different states of disagreement can be severely challenged with increasing amounts of data to be coded and limited available resources for coding it. This is further complicated by coder bias when dealing with the complexity of disagreement [26]. However, a state of (dis)agreement is independent of the labels used for coding data instances. The paper presents a conceptual understanding of collaborative disagreement that remains indifferent to the coding context and groups of coders irrespective of their discipline. We use this conceptual formulation to offer tree-based ranking metrics that allow coders to order different states of coding disagreements to discern ambiguity. Our proposed approach of dealing with disagreement treats all the labels uniformly, and remains unchanged with new coding schemes provided the number of unique labels is preserved. This agnostic property offers qualitative coders an opportunity to easily analyze disagreement, resolve minor disputes, single out irregular instances, and help improve coding of the data. With such properties, the metrics successfully represent ambiguous instances such that they match the coder’s perceptions of disagreement.
"
Intermodulation: Improvisation and Collaborative Art Practice for HCI,https://dl.acm.org/authorize?N655430,"
This paper integrates theory, ethnography, and collaborative artwork to explore improvisational activity as both topic and tool of multidisciplinary HCI inquiry. Building on theories of improvisation drawn from art, music, HCI and social science, and two ethnographic studies based on interviews, participant observation and collaborative art practice, we seek to elucidate the improvisational nature of practice in both art and ordinary action, including human-computer interaction. We identify five key features of improvisational action -- reflexivity, transgression, tension, listening, and interdependence -- and show how these can deepen and extend both linear and open-ended methodologies in HCI and design. We conclude by highlighting collaborative engagement based on 'intermodulation' as a tool of multidisciplinary inquiry for HCI research and design. This paper explores the value and possibility of improvisation and collective art practice as a mode of HCI research and inquiry. As the theories and cases reviewed above make clear, the core engine that enables improvisation is neither a predetermined plan nor sheer extemporaneousness (or put simply, just ‘making stuff up’.). Instead, it is enabled through processes of dialogue and intermodulation that continually arbitrate between freedom and structure, similarity and otherness, tension and relaxation, and self-conviction and external validity. From this complex and precarious interaction, the subject ‘I’ can be stimulated to emerge from his or her existing narrowness in the world, and develop creativity and knowledge in a mutually reciprocal way with others. This paper identifies five key features that enable such learning opportunities, and explain how these can deepen and extend notions of creativity and collaboration in HCI and design. Our work has attempted to provide one useful model of multidisciplinary research wherein musicians, artists, and HCI researchers learn to come together – “to bend for the common result” – to achieve outcomes, insights and ideas unavailable to each individually.

"
OptiMo: Optimization-Guided Motion Editing for Keyframe Character Animation,https://dl.acm.org/authorize?N655431,"
The mission of animators is to create nuanced, high-quality character motions. To achieve this, the careful editing of animation curves---curves that determine how a series of keyframed poses are interpolated over time---is an important task. Manual editing affords full and precise control, but requires tedious and nonintuitive trials and errors. Numerical optimization can automate such exploration; however, automatic solutions cannot always be perfect, and it is difficult for animators to control optimization owing to its black-box behavior. In this paper, we present a new framework called optimization-guided motion editing, which is aimed at maintaining a sense of full control while utilizing the power of optimization. We have designed interactions and developed a set of mathematical formulations to enable them. We discuss the framework's potential by demonstrating several usage scenarios with our proof-of-concept system, named OptiMo. We presented a new framework called optimization-guided motion editing, in which numerical optimization is utilized as a tool for animators to effectively edit character motions. To enable this, we set the three design goals: editability of the optimized motion, controllability of the optimization behavior, and transparency of the optimization process. Based on these goals, we presented a set of interactions and their mathematical formulations, and developed a proof-of-concept system, named OptiMo. We showed how this system could support animators by demonstrating several usage scenarios. To validate our approach, we interviewed domain experts using our system, and obtained comments for each feature of the system as well as its potential usages in the professional production.

"
Medley: A Library of Embeddables to Explore Rich Material Properties for 3D Printed Objects,https://dl.acm.org/authorize?N655432,"
In our everyday life, we interact with and benefit from objects with a wide range of material properties. In contrast, personal fabrication machines (e.g., desktop 3D printers) currently only support a much smaller set of materials. Our goal is to close the gap between current limitations and the future of multi-material printing by enabling people to explore the reuse of material from everyday objects into their custom designs. To achieve this, we develop a library of embeddables--everyday objects that can be cut, worked and embedded into 3D printable designs. We describe a design space that characterizes the geometric and material properties of embeddables. We then develop Medley---a design tool whereby users can import a 3D model, search for embeddables with desired material properties, and interactively edit and integrate their geometry to fit into the original design. Medley also supports the final fabrication and embedding process, including instructions for carving or cutting the objects, and generating optimal paths for inserting embeddables. To validate the expressiveness of our library, we showcase numerous examples augmented by embeddables that go beyond the objects' original printed materials. To approximate a future of multi-material printing, we have constructed a library (and the tool support) of embeddables– everyday objects that can be cut, worked and embedded into 3D printed designs to augment their material properties. We hope this work can promote future research on end-user material literacy, as well as inspiring designers and makers to explore the material aspects of personal fabrication.

"
GeoCoin: Supporting Ideation and Collaborative Design with Smart Contracts,https://dl.acm.org/authorize?N655433,"
Design and HCI researchers are increasingly working with complex digital infrastructures, such as cryptocurrencies, distributed ledgers and smart contracts. These technologies will have a profound impact on digital systems and their audiences. However, given their emergent nature and technical complexity, involving non-specialists in the design of applications that employ these technologies is challenging. In this paper, we discuss these challenges and present GeoCoin, a location-based platform for embodied learning and speculative ideating with smart contracts. In collaborative workshops with GeoCoin, participants engaged with location-based smart contracts, using the platform to explore digital 'debit' and 'credit' zones in the city. These exercises led to the design of diverse distributed-ledger applications, for time-limited financial unions, participatory budgeting, and humanitarian aid. These results contribute to the HCI community by demonstrating how an experiential prototype can support understanding of the complexities behind new digital infrastructures and facilitate participant engagement in ideation and design processes. In this paper, we have introduced GeoCoin, an example of unfinished software aimed at creating a shared environment for speculative ideating and collaborative designing with locationbased smart contracts. We contribute to the HCI community by developing the concept of unfinished software. In particular, we have used it to: • Make smart contracts experientially accessible to nonspecialists. We have discussed how this experiential prototype can support non-specialists in understanding complex technological systems such as smart contracts. • Explore values and concerns of smart contracts for smart cities. We have discussed insights from a series of GeoCoin workshops with a number of audiences ranging from arts organisations, artists, designers to academics, industry and blockchain experts. • Allow participants to ideate with smart contracts. We have presented a series of design ideas emerging from the workshops which shows how GeoCoin facilitated participant engagement in ideation and design processes, through the concept of open, unfinished software. GeoCoin.site is freely available to HCI researchers as a web platform for creating open geo-located smart contract experiences. More generally, we demonstrate how employing unfinished software platforms can engage non-specialist audiences in location-based value exchange and peer-to-peer systems in a way that is meaningful and empowering to them.

"
Evaluating Attack and Defense Strategies for Smartphone PIN Shoulder Surfing,https://dl.acm.org/authorize?N655434,"
We evaluate the efficacy of shoulder surfing defenses for PIN-based authentication systems. We find tilting the device away from the observer, a widely adopted defense strategy, provides limited protection. We also evaluate a recently proposed defense incorporating an ""invisible pressure component"" into PIN entry. Contrary to earlier claims, our results show this provides little defense against malicious insider attacks. Observations during the study uncover successful attacker strategies for reconstructing a victim's PIN when faced with a tilt defense. Our evaluations identify common misconceptions regarding shoulder surfing defenses, and highlight the need to educate users on how to safeguard their credentials from these attacks. We conducted experiments using 30 subjects to understand shoulder surfing defenses and attack strategies on smartphone PINs. Our experiments show that it is quite easy to correctly guess PINs with two observations, on average. Furthermore, attackers were surprisingly effective in shoulder surfing PINs from across the room. We also subject the most commonly used defense, tilting the device screen away from the observer, to shoulder surfing attacks and show its limited efficacy. We show that while tilting the device screen away from the attacker with an angle of 70° or higher prevents complete guessing of PINs, smart attackers look for other clues (such as the proximity of the finger to the corner of the smartphone) to partially guess the PIN. Finally, we conduct experiments and gather empirical evidence to show that ForcePIN has an inherent timing side channel, which renders it completely ineffective against shoulder surfing attacks. Our work calls attention to educating users about the threat of shoulder surfing and common misconceptions. First, it shows that smartphone users need to be careful even when the observer is located across the room. Second, while tilting the device screen may hide its contents, it does not prevent shoulder surfing attacks on PINs. Therefore, it is important to educate smartphone users on the inefficacy of this defense.
"
VR-OOM: Virtual Reality On-rOad driving siMulation,https://dl.acm.org/authorize?N655445,"
Researchers and designers of in-vehicle interactions and interfaces currently have to choose between performing evaluation and human factors experiments in laboratory driving simulators or on-road experiments. To enjoy the benefit of customizable course design in controlled experiments with the immediacy and rich sensations of on-road driving, we have developed a new method and tools to enable VR driving simulation in a vehicle as it travels on a road. In this paper, we describe how the cost-effective and flexible implementation of this platform allows for rapid prototyping. A preliminary pilot test (N = 6), centered on an autonomous driving scenario, yields promising results, illustrating proof of concept and indicating that a basic implementation of the system can invoke genuine responses from test participants. VR-OOM is the first on-road VR driving simulator. The driving simulation environment features the controlled events and scenarios of traditional driving simulators with the physical sensations, immediacy and presence of actual driving. Our initial validation tests indicate that it can serve functionally as a driving simulation environment for autonomous driving scenarios to test driver situation awareness and intervention. Further validation studies are ongoing to ensure that this novel driving simulator can help bridge the gap between safe testing of human response and effective prediction of human performance. By using the system description and protocol described in this paper, researchers can have access to a highly immersive driving simulation environment for relatively low cost and effort. We hope this broadens the pool of people who will design future interactions and interfaces for automobiles, and encourage broader empirical testing to understand human response in the road ahead.
"
KickAR: Exploring Game Balancing Through Boosts and Handicaps in Augmented Reality Table Football,https://dl.acm.org/authorize?N655446,"
When player skill levels are not matched, games provide an unsatisfying player experience. Player balancing is used across many digital game genres to address this, but has not been studied for co-located augmented reality (AR) tabletop games, where using boosts and handicaps can adjust for different player skill levels. In the setting of an AR table football game, we studied the importance of game balancing being triggered by the game system or the players, and whether player skill should be required to trigger game balancing. We implemented projected icons to prominently display game balancing mechanics in the AR table football game. In a within-subjects study (N=24), we found players prefer skill-based control over game balancing and that different triggers are perceived as having different fairness. Further, the study showed that even game balancing that is perceived as unfair can provide enjoyable game experiences. Based on our findings, we provide suggestions for player balancing in AR tabletop games. Co-located social games involving physical movement facilitate enjoyment and engagement. Due to their frequent occurrence in diverse social settings, game balancing for skill mismatch is particularly important for these games. In this paper, we presented KickAR, a custom-made ARenhanced foosball table, to explore game balancing with boosts and handicaps. With this game table, we explored our research question on different triggers for this kind of game balancing, and their effects on player experience. In a within-subjects study, we compared system-based control to player-based control over the game balancing triggers, as well as the importance of designing for different types of player control: skill-based as opposed to non-game-related. In summary, our contribution consists of three main findings regarding the use of game balancing for AR table games: • Players clearly prefer skill-based control over the game balancing triggers, rather than control by the game system or player control that is not tied to skill. • Perceived fairness of game balancing does not necessarily impact the player experience. CHI 2018 Paper CHI 2018, April 21–26, 2018, Montréal, QC, Canada Paper 166 Page 9 • The appearance of game balancing challenges alone already negatively affects players’ perceived competence, regardless of game interference. With these results in mind, AR table game designers can improve player experience and increase enjoyment in games that feature a disparity in skill levels. While these findings are particularly important for table games, future work will explore to what degree these findings generalize to team-vsteam gameplay, and video games with players that are not co-located.

"
Digital Joinery For Hybrid Carpentry,https://dl.acm.org/authorize?N655447,"
The craft of carpentry relies on joinery: the connections between pieces of wood to create multipart structures. In traditional woodworking, joints are limited to the manual chisel skills of the craftsperson, or to capabilities of the machines, which favorite 90° or 180° angle joints with no more than two elements. We contribute an interactive design process in which joints are generated digitally to allow for unrestricted beam connectors, then produced from Nylon-12 using selective laser sintering (SLS) 3D printing. We present our Generative Joinery Design Tool and demonstrate our system on a selection of stools. The paper exemplifies the potential of Digital Joinery to enhance carpentry by incorporating a hybrid and interactive level of design sophistication and affordances that are very hard to achieve with traditional skills and tools. In this paper we presented Digital Joinery, a woodworking design paradigm aiming to liberate carpentry from traditional construction limitations. We contribute a new joinery design software tool that supports new types of connectors, to let multiple pieces of lumber meet at unconventional angles, manifesting a new type of aesthetics and structure for furniture. We demonstrated our tool in a collection of three stools, each of which illustrates a different aspect of new joints for stool making. Our work aims at reinforcing the bridge that has already started to pave the way for makers, designers, and researchers to merge computational practices with craft [26,29]. In doing so, we continue the lines of prior work [36] aiming to keep computational design open and allow for some sort of manual freedom to encourage makers to explore the full potential of the hybrid medium. In addition to the technical work presented here, we discussed our work with professional carpenters and designers. The designers seemed a bit more open-minded about the affordances of such technology, envisioning a new type of human-computer interaction via the furniture design process. Yet, as our work aims to assist craftpersons as well as designers, we believe that hybrid design tools may be most fruitful in the hands of hybrid makers. Otherwise, a broader introduction is needed to expose traditional craftpersons to digital design and fabrication prior to training them to use Digital Joinery. Considering future work, digital joinery can be extended to serve as a connecting agent between other ready-made artifacts (not just wood). Moreover, we would like to extend the deployment of Digital Joinery to a diverse community of makers wishing for hybrid design prosperity. We envision future craftpersons equipped with manual and digital machines, mastering handwork and computational work. While this community of professional hybrid makers CHI 2018 Honourable Mention CHI 2018, April 21–26, 2018, Montréal, QC, Canada Paper 167 Page 9 already exists, more tools and technology for hybrid practice is required, and our work aims at this need. Finally, and on a slightly different track, we would like to learn how Digital Joinery could personalize mass furniture manufacturing. For instance, given a warehouse with a huge amount of lumber in a finite set of dimensions and types, how can consumers interact with parametric and generative tools to customize their furniture? How can we optimize the use for this collection of raw material, yet maximize the design freedom of the end users? For example, let us assume a warehouse includes 4 cm x 4 cm wooden beams in lengths of 80 cm and 50 cm. When a customer requests a chair that will support a load of 100 kg, or a table in a specific dimension, the software will be able to generate an optimal or semi-optimal (using a minimal amount of wood) solution enabled by 3D-printed joints and an optimizing design algorithm. In addition, the user can specify a finished style and other details. This vision require extensive research on (1) finite-set catalog selection mechanisms integrated into generative design software; and (2) extended generative design procedures that optimize full furniture architecture for production, considering wood costs, 3D printing costs, and the system’s abilities. This will be the topic of our future research, and hopefully will interest other researchers in the HCI and CG communities.

"
What Makes an Automated Vehicle a Good Driver?,https://dl.acm.org/authorize?N655448,"
An automated vehicle needs to learn how human road users experience the intentions of other drivers and understand how they communicate with each other in order to avoid misunderstandings and prevent giving a negative external image during interactions. The aim of the present study is to identify a cooperative lane change indication which other drivers understand unambiguously and prefer when it comes to lane change announcements in a dense traffic situation on the highway. A fixed-base driving simulator study is conducted with N = 66 participants in Germany in a car-following scenario. Participants rated, from the lag driver's perspective, different lane change announcements of another driver which varied in lateral movements (i.e., duration, lateral offset). Main findings indicate that a medium offset and moderate duration of lateral movement is experienced as most cooperative. The results are crucial for the development of lane change strategies for automated vehicles. The study results extend the available literature in the following ways: Current literature on modelling human lane change strategies, called gap acceptance models [2,8,14,19,23], only distinguish between courtesy and forced merging models. This implies that either the other vehicle is willing to allow a lane change or the driver has to enforce his/her desire but neglect the fact that a driver can actively announce a lane change desire without appearing reckless or too passive. Therefore, the results of the study contribute to enhancing the current literature on human interaction behaviour in dense traffic situations. Secondly, it represents a first approach when it comes to modelling the interaction between automated vehicles and human drivers in order to be unambiguously understood.
"
"Feel My Pain: Design and Evaluation of Painpad, a Tangible Device for Supporting Inpatient Self-Logging of Pain",https://dl.acm.org/authorize?N655449,"
Monitoring patients' pain is a critical issue for clinical caregivers, particularly among staff responsible for providing analgesic relief. However, collecting regularly scheduled pain readings from patients can be difficult and time-consuming for clinicians. In this paper we present Painpad, a tangible device that was developed to allow patients to engage in self-logging of their pain. We report findings from two hospital-based field studies in which Painpad was deployed to a total of 78 inpatients recovering from ambulatory surgery. We find that Painpad results in improved frequency and compliance with pain logging, and that self-logged scores may be more faithful to patients' experienced pain than corresponding scores reported to nurses. We also show that older adults may prefer tangible interfaces over tablet-based alternatives for reporting their pain, and we contribute design lessons for pain logging devices intended for use in hospital settings. In this paper we presented Painpad, a device that enables selflogging of pain by hospitalised patients. Painpad was wellreceived by participants, and was shown to facilitate improved compliance compared to equivalent nurse scoring. Allowing patients to self-score their pain circumvents the need for nurses to engage in burdensome data collection, and lowers the data management and processing time for clinicians [6, 13] by autopopulating each patient’s data into a secure database for use in research and service improvement. In future work, we plan to explore other input techniques for inpatient pain logging. The present work compared a device with physical buttons and two tablet alternatives, but one could conceivably create a device that incorporates a physical slider and compare this to a keypad alternative. Likewise, unlock journaling [44] represents an alternative, low-effort method for collecting pain scores on mobile phones and tablets. Beyond this, it is worth noting that the pain data collected in our study did not factor into clinical decision-making, yet could easily do so in future. We plan to allow nurses to monitor the pain curve of multiple patients from the convenience of the nurse’s station, combining self-logged data with live pain monitoring interfaces to support decision making. Similarly, logging pain scores alongside analgesic dosing times might allow clinicians to identify correlations and make better decisions based on real-time data collected about their patients.
"
Challenges and Opportunities for Technology-Supported Activity Reporting in the Workplace,https://dl.acm.org/authorize?N655440,"
Effective communication of activities and progress in the workplace is crucial for the success of many modern organizations. In this paper, we extend current research on workplace communication and uncover opportunities for technology to support effective work activity reporting. We report on three studies: With a survey of 68 knowledge workers followed by 14 in-depth interviews, we investigated the perceived benefits of different types of progress reports and an array of challenges at three stages: Collection, Composition, and Delivery. We show an important interplay between written and face-to-face reporting, and highlight the importance of tailoring a report to its audience. We then present results from an analysis of 722 reports composed by 361 U.S.-based knowledge workers, looking at the influence of the audience on a report's language. We conclude by discussing opportunities for future technologies to assist both employees and managers in collecting, interpreting, and reporting progress in the workplace. We presented results from a mixed-methods investigation of activity reporting in the workplace. Our survey and interviews highlighted challenges with recalling, composing and delivering reports, and uncovered the prevalence of communicating progress across multiple channels. Our corpus of real-world weekly reports highlighted the ways in which people tailored reports differently based on presumed audience. As activity reporting continues to be an important means of achieving workplace awareness, our work provides insight into promising areas in which technology can support the power dynamic between workers and managers. We highlight opportunities for researchers and designers to further streamline the process of recording activity for different audiences, across multiple places and channels, so that the outcomes can benefit all stakeholders.
"
Supporting Meaningful Personal Fitness: the Tracker Goal Evolution Model,https://dl.acm.org/authorize?N655441,"
While the number of users sporting fitness trackers is constantly increasing, little is understood about how tracking goals can evolve over time. As recent studies have shown that the long-term health effects of trackers are limited, we need to readdress how trackers engage users. We conducted semi-structured interviews and an online survey to explore how users change their tracking goals. Based on our results, we created the Tracker Goal Evolution Model. The model describes how tracker goals can evolve from internal user needs through qualitative goals to quantitative goals that can be used with trackers. It also includes trust and reflection as key contextual factors contributing to meaningful transitions between goals. We postulate showing how tracker goals relate to other personal fitness goals as key for long-term engagement with trackers. Our model is useful for designers of future trackers as a tool to create evolving and meaningful tracking goals. In this paper, we introduced the Tracker Goal Evolution Model. Our model describes the user practices around fitness tracker goals on three levels: Hedonic and eudaimonic needs, Qualitative goals and Quantitative goals. These three levels are connected with two transitions: manifestation and translation. We based our model on a series of semi-structured interviews and an online survey. Our model enables charting an individual’s goals in order to build more engaging and evolving tracking experiences. The model can be used as a complement to models of personal informatics as it offers a new perspective on how a user’s goals evolve through the journey through personal tracking. We hope that our work will help researchers and designers address new challenges with personal tracking and build trackers that offer long-term benefits. Our model aims to contribute to a new generation of trackers that offer engaging experiences and tangible benefits. In future work, we hope to investigate how it can help generate alternative designs for fitness trackers and ways to communicate its levels and transitions to the user in a meaningful way.

"
Everybody's Hacking: Participation and the Mainstreaming of Hackathons,https://dl.acm.org/authorize?N655442,"
Hackathons have become a popular tool for bringing people together to imagine new possibilities for technology. Despite originating in technology communities, hackathons have now been widely adopted by a broad range of organisations. This mainstreaming of hackathons means they encompass a very different range of attendees and activities than they once did, to the extent that some events billed as hackathons may involve no coding at all. Given this shift away from production of code, they might instead be seen as an increasingly popular participatory design activity, from which designers and researchers in HCI can learn. Through fieldwork at six hackathons that targeted non-technical communities, we identify the types of activities and contributions that emerge through these events and the barriers and tensions that might exist. In doing so, we contribute a greater understanding of hackathons as a growing phenomenon and as a potential tool for participatory research. In this paper, we have explored the ongoing mainstreaming of hackathons from a largely technical pursuit into participatory events that engage with a much wider audience. We have shown the motivations for organisers in running these events, the ways in which people are able to participate without coding, and the tensions that arise between the hackathon format and the desire for wider participation. We believe that the popularity experienced by these events poses many opportunities for participatory research, both in terms of repurposing hackathons themselves and in informing the configuration of other participatory research activities. Future work will need to focus on how new types of activity can be configured while also avoiding some of the acknowledged inclusion pitfalls of hackathons. However, despite these challenges, there is clear potential for engaging people with the design of technologies in new ways.
"
Pointing at a Distance with Everyday Smart Devices,https://dl.acm.org/authorize?N655443,"
Large displays are becoming commonplace at work, at home, or in public areas. However, interaction at a distance -- anything greater than arms-length -- remains cumbersome, restricts simultaneous use, and requires specific hardware augmentations of the display: touch layers, cameras, or dedicated input devices. Yet a rapidly increasing number of people carry smartphones and smartwatches, devices with rich input capabilities that can easily be used as input devices to control interactive systems. We contribute (1) the results of a survey on possession and use of smart devices, and (2) the results of a controlled experiment comparing seven distal pointing techniques on phone or watch, one- and two-handed, and using different input channels and mappings. Our results favor using a smartphone as a trackpad, but also explore performance tradeoffs that can inform the choice and design of distal pointing techniques for different contexts of use. Our work investigates the use of handheld devices, the smartphone and the smartwatch, as remote pointing devices to interact with large displays. We first explore the design space for high-level dimensions and describe the implementation of 7 feasible techniques. Through a controlled pointing study, we suggest the use of the smartphone as a relative trackpad, utilizing landscape orientation and two hands for best performance. We then explore design challenges and offer alternative recommendations for dimensionally-constrained contexts such as when one or both hands are unavailable for use. Most importantly, our work demonstrates the sensor and touch capabilities of modern smart devices allow for rich, flexible, and alwaysavailable interactions, which can be dynamically adjusted for real-world contexts without expensive and rigid augmentations to the environment or the large display.

"
The Story in the Notebook: Exploratory Data Science using a Literate Programming Tool,https://dl.acm.org/authorize?N655444,"
Literate programming tools are used by millions of programmers today, and are intended to facilitate presenting data analyses in the form of a narrative. We interviewed 21 data scientists to study coding behaviors in a literate programming environment and how data scientists kept track of variants they explored. For participants who tried to keep a detailed history of their experimentation, both informal and formal versioning attempts led to problems, such as reduced notebook readability. During iteration, participants actively curated their notebooks into narratives, although primarily through cell structure rather than markdown explanations. Next, we surveyed 45 data scientists and asked them to envision how they might use their past history in an future version control system. Based on these results, we give design guidance for future literate programming tools, such as providing history search based on how programmers recall their explorations, through contextual details including images and parameters. Data scientists from a broad range of domains and skill levels are doing impactful work through code. In this study of literate programming we found that programmers do create narrative structure during their exploration, although often by manipulating cell structure rather than using much explanatory markdown. Creating a narrative also intersects and conflicts with other objectives, such as participants who prototyped and debugged code by expanding-reducing cell structure, or participants who kept a clutter of old iterations in their notebooks to retain a history of their work. We hope our results will inspire future designs for ways to interact with notebook cells for browsing history, debugging, and other tasks which may improve the effectiveness of literate programming for supporting data science.

"
What Did I Really Vote For?,https://dl.acm.org/authorize?N656555,"
E-voting has been embraced by a number of countries, delivering benefits in terms of efficiency and accessibility. End-to-end verifiable e-voting schemes facilitate verification of the integrity of individual votes during the election process. In particular, methods for cast-as-intended verification enable voters to confirm that their cast votes have not been manipulated by the voting client. A well-known technique for effecting cast-as-intended verification is the Benaloh Challenge. The usability of this challenge is crucial because voters have to be actively engaged in the verification process. In this paper, we report on a usability evaluation of three different approaches of the Benaloh Challenge in the remote e-voting context. We performed a comparative user study with 95 participants. We conclude with a recommendation for which approaches should be provided to afford verification in real-world elections and suggest usability improvements. Based on our usability evaluation, we recommend the mobile approach for deployment during elections. Many of the problems that participants reported in the questionnaire could be mitigated by improved implementations. However, the mobile approach’s use is limited to supplementary device owners (e.g. a Smartphone), of which there were only 78% in Germany, according to the latest survey [46]. Therefore, for greater inclusivity we recommend offering the automatic approach as a fall-back. The automatic approach can serve as an alternative for those who do not own Smartphones. It has to be acknowledged, however, that the verification data is no longer displayed to the voter in the automatic and mobile approaches. As already proposed by Karayumak et al. [32], the verification data should still be available in an auxiliary expert mode. Expert voters can then conduct manual verification, perhaps by using their own programmed verifier.
"
Tangible Drops: A Visio-Tactile Display Using Actuated Liquid-Metal Droplets,https://dl.acm.org/authorize?N656556,"
We present Tangible Drops, a visio-tactile display that for the first time provides physical visualization and tactile feedback using a planar liquid interface. It presents digital information interactively by tracing dynamic patterns on horizontal flat surfaces using liquid metal drops on a programmable electrode array. It provides tactile feedback with directional information in the 2D vector plane using linear locomotion and/or vibration of the liquid metal drops. We demonstrate move, oscillate, merge, split and dispense-from-reservoir functions of the liquid metal drops by consuming low power (450 mW per electrode) and low voltage (8--15 V). We report on results of our empirical study with 12 participants on tactile feedback using 8 mm diameter drops, which indicate that Tangible Drops can convey tactile sensations such as changing speed, varying direction and controlled oscillation with no visual feedback. We present the design space and demonstrate the applications of Tangible Drops, and conclude by suggesting potential future applications for the technique. We have presented Tangible Drops, which can produce visual animations on a flat surface using locomotion of liquid metal droplets. The enabling technology for the novel user experience that it provides is tactile feedback with 2D planar direction information using a liquid material. It demonstrates, for the first time, the locomotion of liquid metal drops from electrode to electrode on an open surface without constraining channels or cells. It also demonstrates for the first time a combination of locomotion and vibration of a material to provide tactile feedback. While this work is still in its prototype stage, we have shown it to be a viable new method of controlling liquid metal alloys for visio-tactile displays, and have therefore laid the foundations for what we hope to be a rich and full area of future work in the area.

"
FingerT9: Leveraging Thumb-to-finger Interaction for Same-side-hand Text Entry on Smartwatches,https://dl.acm.org/authorize?N656557,"
We introduce FingerT9, leveraging the action of thumb-to-finger touching on the finger segments, to support same-side-hand (SSH) text entry on smartwatches. This is achieved by mapping a T9 keyboard layout to the finger segments. Our solution avoids the problems of fat finger and screen occlusion, and enables text entry using the same-side hand which wears the watch. In the pilot study, we determined the layout mapping preferred by the users. We conducted an experiment to compare the text-entry performances of FingerT9, the tilt-based SSH input, and the direct-touch non-SSH input. The results showed that the participants performed significantly faster and more accurately with FingerT9 than the tilt-based method. There was no significant difference between FingerT9 and direct-touch methods in terms of efficiency and error rate. We then conducted the second experiment to study the learning curve on SSH text entry methods: FingerT9 and the tilt-based input. FingerT9 gave significantly better long-term improvement. In addition, eyes-free text entry (i.e., looking at the screen output but not the keyboard layout mapped on the finger segments) was made possible once the participants were familiar with the keyboard layout. We introduced FingerT9, a novel SSH text-entry approach for smartwatch, combining traditional T9 keyboard and thumb-to-finger interaction. We implemented an experimental prototype with the thin-film capacitive sensors attached on the finger segments. The within-subject controlled experiment showed that FingerT9 has significant faster typing speed and lower error rate than the SSH tilt-based input, and has lower error compared with the traditional direct-touch input. Experiment II showed that FingerT9 has significant improvement than the tilt-based input over time and users could remember the FingerT9 layout. The two experiments revealed that FingerT9 performed better than tilt-based input in text entry speed, error rate, efficiency, and learnability. These advantages of FingerT9 over the tilt-based method could be due to the simplified typing procedure (i.e. eliminating the step of letter selection) and the reduced physical efforts. In the future, we would like to optimize the keyboard layout, and improve the prototype of thumb-to-finger touch sensing ability by attempting possible finger sensing approaches through tracking finger movement with high resolution Electrical Impedance Tomography (EIT). We will investigate SSH smartwatch text entry in more depth with text entry in specific context (such as, walking, standing, and hand holding something), study the performance of SSH text entry with dominant and non-dominant hands and its social acceptance to see how practical SSH smartwatch text entry is in everyday life. Besides, we are interested in exploring FingerT9 for other language text entry, such as Chinese, Japanese, and Korean, and investigate how it could be applied for eyes-free typing.

"
Time-Turner: Designing for Reflection and Remembrance of Moments in the Home,https://dl.acm.org/authorize?N656558,"
Families preserve memories of their special and everyday experiences, though it can be hard to capture all these moments in everyday life. We explore the concept of automated forms of capturing family life and presenting them through situated, tangible everyday artifacts in the home. We designed Time-Turner, an always-on video recording system along with a set of three drink coasters that allow family members to easily search, filter and replay videos to connect to their past. We engaged households in speculative enactments and interviews to explore the design space. Our findings point to the value of witnessing real rather than staged moments and the ways in which the affordances of everyday artifacts can allow media to be 'lived with' as a part of everyday life. Yet our design also revealed tensions around sharing and changing perceptions across time and generations. This points to design challenges around safeguarding this media and capturing 'reality' as opposed to curated content. We designed Time-Turner to critically explore how digital memories might be captured through an always-on video recording system and then embedded in everyday objects in the home for supporting family practices around reflecting and remembering the past. Our findings revealed the opportunities, possibilities, and the tensions of this new paradigm of capturing and playing back family life. We provide new insights on how always-on video systems in the home can open opportunities for households to construct value with video archives as well as the social tensions that could emerge. Future work should explore non-Western ways of accessing time and include a range of family members (kids, grandparents). Our work provides an example of new kinds of connected devices that can automate aspects of our lives while helping to fit within the social fabric of domestic life and home environments.
"
Webcam Covering as Planned Behavior,https://dl.acm.org/authorize?N656559,"
Most of today's laptops come with an integrated webcam placed above the screen to enable video conferencing. Due to the risk of webcam spying attacks, some laptop users seem to be concerned about their privacy and seek protection by covering the webcam. This paper is the first to investigate personal characteristics and beliefs of users with and without webcam covers by applying the Theory of Planned Behavior. We record the privacy behavior of 180 users, develop a path model, and analyze it by applying Partial Least Squares. The analysis indicates that privacy concerns do not significantly influence users' decision to use a webcam cover. Rather, this behavior is influenced by users' attitudes, social environment, and perceived control over protecting privacy. Developers should take this as a lesson to design privacy enhancing technologies which are convenient, verifiably effective and endorsed by peers. This study is the first to investigate laptop users’ webcam covering behavior by applying the Theory of Planned Behavior. Our field data collection at public places enabled us to unobtrusively observe this privacy protection behavior as a special kind of human-machine interaction. About a third of the study participants used a webcam cover. We use PLS analysis to fit a latent factor path model to understand the drivers that lead users to adopt or abstain from covering their webcam. Users perform this behavior largely independent of their stated level of privacy concerns, but rather because of specific beliefs regarding webcam covers. We find that users are heterogeneous in these beliefs. As a consequence, developers should take our results as a lesson on the importance of designing privacy enhancing technologies which are perceived as convenient, effective and socially acceptable.

"
WrisText: One-handed Text Entry on Smartwatch using Wrist Gestures,https://dl.acm.org/authorize?N656550,"
We present WrisText - a one-handed text entry technique for smartwatches using the joystick-like motion of the wrist. A user enters text by whirling the wrist of the watch hand, towards six directions which each represent a key in a circular keyboard, and where the letters are distributed in an alphabetical order. The design of WrisText was an iterative process, where we first conducted a study to investigate optimal key size, and found that keys needed to be 55º or wider to achieve over 90% striking accuracy. We then computed an optimal keyboard layout, considering a joint optimization problem of striking accuracy, striking comfort, word disambiguation. We evaluated the performance of WrisText through a five-day study with 10 participants in two text entry scenarios: hand-up and hand-down. On average, participants achieved a text entry speed of 9.9 WPM across all sessions, and were able to type as fast as 15.2 WPM by the end of the last day. In this paper, we proposed, designed, and studied a onehanded text entry technique on smartwatches. The technique allows users to enter text using the same hand wearing the smartwatch, by whirling the wrist in six directions to select letters in a circular keyboard. We designed the layout of the keyboard in an iterative approach, where we first studied the optimal size of the keyboard keys, and found that keys needed to be 55º or wider to achieve over 90% striking accuracy. We then optimized the keyboard layout by considering factors, including keyboard learnability, striking accuracy, striking comfort, and word disambiguation. This led to a final design which was evaluated in a 5-day study with 10 participants. The result indicates that participants could achieve an average text entry speed of 9.9 WPM across all the sessions, and were able to type as fast as 15.2 WPM in the last day. We believe smartwatches will become the major platform for mobile text entry, and our technique may serve as important groundwork for future work on new text entry techniques for wearable devices.

"
Off-Line Sensing: Memorizing Interactions in Passive 3D-Printed Objects,https://dl.acm.org/authorize?N656551,"
Embedding sensors into objects allow them to recognize various interactions. However, sensing usually requires active electronics that are often costly, need time to be assembled, and constantly draw power. Thus, we propose off-line sensing: passive 3D-printed sensors that detect one-time interactions, such as accelerating or flipping, but neither require active electronics nor power at the time of the interaction. They memorize a pre-defined interaction via an embedded structure filled with a conductive medium (e.g., a liquid). Whether a sensor was exposed to the interaction can be read-out via a capacitive touchscreen. Sensors are printed in a single pass on a consumer-level 3D printer. Through a series of experiments, we show the feasibility of off-line sensing. This paper has presented off-line sensing: a set of sensors that memorize pre-defined interaction via embedded 3D-printed structures filled with a conductive medium. Using off-line sensing, we contribute a variety of off-line sensors that sense load, pressure, acceleration, tilt, flip over, and temperature. Whether an object was exposed to the interaction can be readout via placing it on a standard capacitive touchscreen. Using a consumer-level fabrication pipeline, off-line sensors can be easily created, printed and used. Future work should address more advanced (e.g., continuous) interactions and further applications, where it is beneficial to deploy off-line sensors.

"
Social Computing-Driven Activism in Youth Empowerment Organizations: Challenges and Opportunities,https://dl.acm.org/authorize?N656552,"
Throughout the world, organizations empower youth to participate in civic engagement to impact social change, and adult-youth collaborations are instrumental to the success of such initiatives. However, little is known about how technology supports this activism work, despite the fact that tools such as Social Networking Applications (SNAs) are increasingly being leveraged in such contexts. We report results from a qualitative study of SNA use within a youth empowerment organization. Using the analytical lens of object-oriented publics, our findings reveal opportunities and challenges that youth and staff face when they use SNAs. We describe the illegibility of youth outreach efforts on SNAs, and how this illegibility complicated staff attempts to hold youth accountable. We also characterize how youth and staff differed in what they felt were socially appropriate uses of SNA features, and tensions that arose in the co-use of these tools. We conclude with implications for the design of collaborative technologies that support youth-led activism in organizational contexts. Through a qualitative study, we investigated the practices of an organization engaged in youth-led community organizing. Our findings shed light on the ways in which SNAs mediated the work that youth and adults did, as well as their relationships with one another. We further discussed challenges that arose as staff strove to hold youth accountable for their outreach work. Additional tensions resulted from the gap between adults' perception of how youth (should) use SNAs for their work and the actual challenges that youth face. Using our findings, we present recommendations for how new ICTs could be designed to support youth-adult collaborations and the shifting attachments to issues amongst youth and their peers.

"
AdaM: Adapting Multi-User Interfaces for Collaborative Environments in Real-Time,https://dl.acm.org/authorize?N656553,"
Developing cross-device multi-user interfaces (UIs) is a challenging problem. There are numerous ways in which content and interactivity can be distributed. However, good solutions must consider multiple users, their roles, their preferences and access rights, as well as device capabilities. Manual and rule-based solutions are tedious to create and do not scale to larger problems nor do they adapt to dynamic changes, such as users leaving or joining an activity. In this paper, we cast the problem of UI distribution as an assignment problem and propose to solve it using combinatorial optimization. We present a mixed integer programming formulation which allows real-time applications in dynamically changing collaborative settings. It optimizes the allocation of UI elements based on device capabilities, user roles, preferences, and access rights. We present a proof-of-concept designer-in-the-loop tool, allowing for quick solution exploration. Finally, we compare our approach to traditional paper prototyping in a lab study. In this paper we have demonstrated a scalable approach to the automatic assignment of UI elements to users and devices in cross-device user interfaces, during multi-user events. By posing this problem as an assignment problem, we were able to create an algorithm which adapts to dynamic changes due to altering configurations of users, their roles, their preferences and access rights, as well as advertised device capabilities. Underpinning AdaM, is a MILP solver which given an objective function decides the assignment of elements to multiple devices and users. Measures for both quality, completeness along with constraints, help to guide the optimization toward satisfactory solutions, which are represented by suitable assignments of UI elements. Following this, the layout problem is performed by responsive design practices common in web design, as shown in our application scenarios. The AdaM application platform itself is web-based and enables collaborative prototyping and rapid iterations of AdaM applications. In addition, our simulator environment allows us to instantiate a wide range of simulated devices. We report on scenarios with up to 1000 users and 2200 devices along with a user study involving six participants, who are asked to assign and adapt UI-element configurations. Our qualitative results indicate that AdaM can reduce both designer and user effort in attaining ideal DUI configurations. The results are promising and suggest further exploration is warranted into the automatic UI element assignment approach introduced here. The mathematical formulation introduced here may be extended to incorporate other issues present in collaborative multi-user interfaces including, extended device parameterization, social acceptability factors, user attention, proxemic dimensions, display switching, display contiguity, field of view, spatio-temporal interaction flow, inter-device consistency, sequential and parallel device use along with synchronous and asynchronous device arrangements.

"
SymbiosisSketch: Combining 2D & 3D Sketching for Designing Detailed 3D Objects in Situ,https://dl.acm.org/authorize?N656554,"
We present SymbiosisSketch, a hybrid sketching system that combines drawing in air (3D) and on a drawing surface (2D) to create detailed 3D designs of arbitrary scale in an augmented reality (AR) setting. SymbiosisSketch leverages the complementary affordances of 3D (immersive, unconstrained, life-sized) and 2D (precise, constrained, ergonomic) interactions for in situ 3D conceptual design. A defining aspect of our system is the ongoing creation of surfaces from unorganized collections of 3D curves. These surfaces serve a dual purpose: as 3D canvases to map strokes drawn on a 2D tablet, and as shape proxies to occlude the physical environment and hidden curves in a 3D sketch. SymbiosisSketch users draw interchangeably on a 2D tablet or in 3D within an ergonomically comfortable canonical volume, mapped to arbitrary scale in AR. Our evaluation study shows this hybrid technique to be easy to use in situ and effective in transcending the creative potential of either traditional sketching or drawing in air. SymbiosisSketch is a hybrid system aiming to blend design knowledge and expertise from the traditional 2D sketching domain to the new and exciting world of direct 3D sketching in AR/VR. By devising a novel method for defining constrained drawing canvases in 3D and building tools to support interaction with physical objects, we have built a system for 3D conceptual design in situ. Our user evaluation confirmed that our toolset is useful, effective, and is able to support a variety of design tasks for users with diverse artistic backgrounds. We hope that this work guides future research in visual communication, furthering our ability to efficiently transform mental design concepts into digital models.

"
Video Game Selection Procedures For Experimental Research,https://dl.acm.org/authorize?N656565,"
Videogames are complex stimuli, and selecting games that consistently induce a desired player experience (PX) in an experimental setting can be challenging. The number of relatively high-quality games being released each year continues to increase, which makes deriving a shortlist of plausible candidate games from this pool increasingly problematic. Despite this, guidance for structuring and reporting on the game selection process remains limited. This paper therefore proposes two approaches to game selection: the first leverages online videogame databases and existing PX research, and is structured with respect to widely-applicable videogame metadata. The second process applies established game design theory to serve researchers when insufficient connections between desired PX outcomes and recognisable game elements exist. Both methods are accompanied by example reports of their application. The present work aims to assist experimental researchers in selecting videogames likely to meet their needs, while encouraging more rigorous standards of reporting in the field. This paper has proposed two approaches to videogame selection in experimental PX research. The first, based on videogame metadata, can quickly and accurately remove large numbers of games from consideration, and is bestsuited to studies with strong theoretical connections to existing PX research. The second approach, based on the MDA framework, provides a means for researchers to derive a list of game elements likely to support a desired experience – and a corresponding set of appropriate games – when existing literature is less readily applicable. Example applications of both methods demonstrate that reporting the selection process clarifies the relationship between candidate games and research goals. Rigour in designing and reporting videogame research is crucial for study replication and the credibility of the field; both would make funding PX research a more viable prospect.
"
Animated Edge Textures in Node-Link Diagrams: a Design Space and Initial Evaluation,https://dl.acm.org/authorize?N656566,"
Network edge data attributes are usually encoded using color, opacity, stroke thickness and stroke pattern, or some combination thereof. In addition to these static variables, it is also possible to animate dynamic particles flowing along the edges. This opens a larger design space of animated edge textures, featuring additional visual encodings that have potential not only in terms of visual mapping capacity but also playfulness and aesthetics. Such animated edge textures have been used in several commercial and design-oriented visualizations, but to our knowledge almost always in a relatively ad hoc manner. We introduce a design space and Web-based framework for generating animated edge textures, and report on an initial evaluation of particle properties - particle speed, pattern and frequency - in terms of visual perception.

"
Silicone Devices: A Scalable DIY Approach for Fabricating Self-Contained Multi-Layered Soft Circuits using Microfluidics,https://dl.acm.org/authorize?N656567,"
We present a scalable Do-It-Yourself (DIY) fabrication workflow for prototyping highly stretchable yet robust devices using a CO2 laser cutter, which we call Silicone Devices. Silicone Devices are self-contained and thus embed components for input, output, processing, and power. Our approach scales to arbitrary complex devices as it supports techniques to make multi-layered stretchable circuits and buried VIAs. Additionally, high-frequency signals are supported as our circuits consist of liquid metal and are therefore highly conductive and durable. To enable makers and interaction designers to prototype a wide variety of Silicone Devices, we also contribute a stretchable sensor toolkit, consisting of touch, proximity, sliding, pressure, and strain sensors. We demonstrate the versatility and novel opportunities of our technique by prototyping various samples and exploring their use cases. Strain tests report on the reliability of our circuits and preliminary user feedback reports on the user-experience of our workflow by non-engineers. In this paper, we presented a scalable and accessible DIY fabrication approach for making Silicone Devices. These devices are highly stretchable, and seamlessly embed all components for sensing, processing, and powering the circuit. Our DIY approach scales to arbitrarly complex devices as it supports multi-layered circuits which are interconnected using stretchable buried VIAs. Circuit traces use Galinstan as conductors in tiny micro-fluidic channels. Galinstan has superior conductive properties and maintains it conductive performance while stretching. Hence, Silicone Devices always self heal after stretching. To enable makers and interaction designers to prototype a wide variety of Silicone Devices, we also demonstrated how to make basic stretchable input sensors, including a highly reliable strain sensor, using our approach. We demonstrated the versatility and novel opportunities of our technique by prototyping a diverse set of samples and exploring their use cases. Finally, strain tests report on the reliability of our circuits and preliminary user feedback reports on the user-experience of our workflow by non-engineers.

"
RFIBricks: Interactive Building Blocks Based on RFID,https://dl.acm.org/authorize?N656568,"
We present RFIBricks, an interactive building block system based on ultrahigh frequency radio-frequency identification (RFID) sensing. The system enables geometry resolution based on a simple yet highly generalizable mechanism: an RFID contact switch, which is made by cutting each RFID tag into two parts, namely antenna and chip. A magnetic connector is then coupled with each part. When the antenna and chip connect, an interaction event with an ID is transmitted to the reader. On the basis of our design of RFID contact switch patterns, we present a system of interactive physical building blocks that resolves the stacking order and orientation when one block is stacked upon another, determines a three-dimensional (3D) geometry built on a two-dimensional base plate, and detects user inputs by incorporating electromechanical sensors. Because it is calibration-free and does not require batteries in each block, it facilitates straightforward maintenance when deployed at scale. Compared with other approaches, this RFID-based system resolves several critical challenges in human-computer interaction, such as 1) determining the identity and the built 3D geometry of passive building blocks, 2) enabling stackable token+constraint interaction on a tabletop, and 3) tracking in-hand assembly. This paper presents RFIBricks, a lightweight and reliable interactive building block system based on UHF RFID sensing. This simple yet effective physical design transforms an RFID tag into an RFID contact switch and therefore enables the localization of the tag in 3D using a 2D grid of RFID contact switches. The proposed sensing algorithm effectively enables resolution of the 3D position and orientations of building blocks as well as the assembled geometry. The interactivity can be further enhanced by connecting the RFID contact switches with other electromechanical sensors to support more vibrant tangible and embodied interaction. The system evaluation addresses the limitations and design opportunities of our prototype system, which lays a foundation for further investigation of interactive tangible interaction platforms involving physical building blocks.

"
Let Me Be Implicit: Using Motive Disposition Theory to Predict and Explain Behaviour in Digital Games,https://dl.acm.org/authorize?N656569,"
We introduce explicit and implicit motives (i.e., achievement, affiliation, power, autonomy) into player experience research and situate them in existing theories of player motivation, personality, playstyle, and experience. Additionally, we conducted an experiment with 109 players in a social play situation and show that: 1. As expected, there are several correlations of playstyle, personality, and motivation with explicit motives, but few with implicit motives; 2. The implicit affiliation motive predicts in-game social behaviour; and 3. The implicit affiliation motive adds significant variance to explain regression models of in-game social behaviours even when we control for social aspects of personality, the explicit affiliation motive, self-esteem, and social player traits. Our results support that implicit motives explain additional variance because they access needs that are experienced affectively and pre-consciously, and not through cognitive interpretation necessary for explicit expression and communication, as is the case in any approaches that use self-report. Understanding the choices that players make and the behaviours that they exhibit in games are two of the central goals of player experience research. In this paper, we introduce Motive Disposition Theory as a new lens for understanding players, contrast the use of explicit and implicit motives with existing player experience methods, and demonstrate that the implicit affiliation motive can predict behaviour in a social play setting, even when controlling for explicitly-accessed social constructs of play style, personality, and motivation. Our results demonstrate the utility of implicit motives in player experience research as they access affective preferences, not conscious ones. We discuss how explicit motives explain the choices that we consciously make, but implicit motives describe the aspects of our experiences that bring us pleasure – both notions that are relevant to play. Motive Disposition Theory – and implicit motives in particular – bring a valuable new perspective and set of assessment tools that can help us better understand players, the choices that they make, and what drives their behaviour in digital games.
"
Single or Multiple Conversational Agents?: An Interactional Coherence Comparison,https://dl.acm.org/authorize?N656560,"
Chatbots focusing on a narrow domain of expertise are in great rise. As several tasks require multiple expertise, a designer may integrate multiple chatbots in the background or include them as interlocutors in a conversation. We investigated both scenarios by means of a Wizard of Oz experiment, in which participants talked to chatbots about visiting a destination. We analyzed the conversation content, users' speech, and reported impressions. We found no significant difference between single- and multi-chatbots scenarios. However, even with equivalent conversation structures, users reported more confusion in multi-chatbots interactions and adopted strategies to organize turn-taking. Our findings indicate that implementing a meta-chatbot may not be necessary, since similar conversation structures occur when interacting to multiple chatbots, but different interactional aspects must be considered for each scenario. This paper compared users’ behaviors when interacting with single or multiple chatbots to gather information for decision-making. Human participants reported different communication experience, but the conversation structure remained similar between conditions. More specifically, users reported feeling more confused in multi-chatbots interactions, where they tried to adopt strategies to organize turn-taking. Also, we found that the conversational agents’ proactive behaviors disrupt sequential coherence, but also help users to explore a knowledge database; hence, proactive behavior should be developed with a well-defined protocol that reduces disruptive messages. Our results suggest that when designing a conversational agent to help users make decisions, there is no evident reason to split the knowledge amongst more than one agent. Therefore, it is possible to leverage already available conversational agents and combine their knowledge to build a richer database. In this case, designers can choose between: a meta-chatbot user interface that orchestrates multiple agents in the background and overrides the multi-party aspect of interaction; or a multi-chatbot user interface with a well-defined interactional protocol that reduces stimuli overload. In both cases, designers should consider the importance of providing clues on how to explore chatbots’ knowledge, but also avoid sequential coherence disruption caused by unimportant messages.
"
Understanding the Use and Impact of the Zero-Rated Free Basics Platform in South Africa,https://dl.acm.org/authorize?N656561,"
Companies are offering zero-rated, or data-charge free Internet services to help bring unconnected users online where Internet access is less affordable. However, it is unclear whether these services achieve this goal or how they shape Internet use. To inform evidence-based policy around and the design of zero-rated services, we show in this paper how mobile users are making use of Facebook's controversial Free Basics platform. We present findings from interviews of 35 Free Basics users in South Africa: current low-income users and non-regular student users. Our findings suggest that Free Basics does shape Internet usage, for instance, users spend more time online because of 'free' apps. Second, Free Basics saves users money but adoption of the platform depends on access to other 'free' Internet options. Finally, most users are confused about how zero-rated services work and what 'free' means. Based on our findings, we make recommendations for future work. Zero-rated services are increasingly being used to connect low-income and unconnected users around the globe to the Internet. Our findings suggest that first, Free Basics does shape users’ Internet use and their choices of which online services to use. Users can get online more frequently and are driven to use ‘free’ services especially when they have fewer resources to get online. Second, the impact of zero-rated services is highest on the lowest income users but can be a supplemental help to more well resourced users who need to get online. Finally, users find the concept of zero-rating confusing which complicates the process of managing mobile Internet costs. We suggest that zero-rated platforms give users agency to influence what is included in these platforms and a voice about the impact of these services on them. We also suggest that alternative models of zero-rating be examined for comparative impact assessment. Lastly, we suggest more interface design work is needed to help users form an improved mental model of zero-rated services.

"
Cooperating to Compete: the Mutuality of Cooperation and Competition in Boardgame Play,https://dl.acm.org/authorize?N656562,"
This paper examines the complex relationship between competition and cooperation in boardgame play. We understand boardgaming as distributed cognition, where people work together in a shared activity to accomplish the game. Although players typically compete against each other, this competition is only possible through ongoing cooperation to negotiate, enact and maintain the rules of play. In this paper, we report on a study of people playing modern boardgames. We analyse how knowledge of the game's state is distributed amongst the players and the game components, and examine the different forms of cooperation and collaboration that occur during play. Further, we show how players use the material elements of the game to support articulation work and to improve their awareness and understanding of the game's state. Our goal is to examine the coordinative practices that the players use during play and explicate the ways in which these enable competition. In this paper, we have explored the complex relationship between cooperation and competition in boardgame play. Using the lens of distributed cognition, we have demonstrated that forms of cooperation occur not only in overtly cooperative games but also in competitive games, and that they are found in both teaching situations and in the play of those with considerable experience in the game. Cooperation is necessary to enable the artificial conflicts within the game, and takes many forms – it is not limited simply to articulation activities. Essentially, cooperation occurs not only between the players themselves; the game components play an important role. Boardgame play is a form of cooperative work that is situated, embodied, distributed and articulated. As we have shown, the play of a game encompasses complex relationships between the players and the material game components. At times, this extends to allowing the players to renegotiate rules, for example in allowing a player to change their selected action (Vignette 5). Not only are the components used to enact the game, they are also used by the players in epistemic tasks of sorting and ordering (V6). These tasks help players to remember key information as well as to plan for their future actions, allowing them to offload key cognitive work onto the components, and to observe the state of the game (V2, V3, V6). In some cases, the tasks are prescribed by the game rules to control how the components are used to manage information; in others, they are formulated and introduced by the players themselves to simplify or clarify gameplay, either in the form of a negotiated rule or as an imposed convention. An example of this is V4; Julie’s insistence that Max place his chosen card above his tableau. Knowledge of the game’s state is distributed not only amongst the players but also amongst the material components of the game, and moves between and amongst them throughout the game. This appropriation of the pieces for offloading cognitive tasks has important ramifications for designers of boardgames as well as of the growing number of digital implementations of boardgames and boardgame-like games. Currently, digital games typically show a player’s money and resources as numbers, rather than as objects that can be manipulated. Making this manipulation visible, as Sally does in V4, ensures that the transaction is evident to her opponents and helps players to understand the implications of certain actions [63]; but automatic implementation in a computer game prevents deliberate and accidental cheating. Given that players seek to offload cognitive tasks and plans onto the game components, designers of digital games should develop flexible interfaces [12] that allow players to group and reorient representations of the game components within the digital interface, “reorganiz[ing] the distributed cognitive system” [24] as they do in a material boardgame. This preferences the player’s own use of the components to implement the game over the designer’s view of how the components should be implemented and recognises the rich trajectories along which information moves between and amongst players and components. In digital games, as in their material counterparts, the player should be allowed to make sense of their components by grouping, sorting and ordering them to support their own cognitive processes, mirroring the way this activity is observed to occur in material play (V6). Digital representations often remove the possibility for cognitive off-loading of information through these core epistemic tasks by neglecting the affordances of the material resource tokens. Through careful attention to the play environment, we have shown how both players and observers make use of the various use-histories and other sources of information available to them through the game environment: their own and others’ played pieces, discarded pieces, cards in hand and other tokens, as well as the game rules themselves. Players plan their actions not only during their own turn but during other players’ actions as well, when they maintain active engagement with the game (V1), and even before the game starts (V6). Lastly, we have demonstrated that in playing a game, players collaborate in a variety of ways, not only in adhering to the prescribed and negotiated rules but also in making sense of them within the play environment: enacting and interpreting them, supporting one another in this process and even in playing with the theme of the game (V6). When errors occur, they are resolved and corrected by the players; with no outside agency beyond the (collaboratively interpreted) rules to dictate the consequences, these rules exceptions must be negotiated [6]. Further, players must cooperate to maintain an operational focus on the game rather than on the social activities that accompany it; collaboration is a necessary, ongoing, and inherent part of the work of playing the game. Throughout the game, whether the rules are cooperative or oppositional, competition and cooperation go hand in hand. As Flavia commented after the game of Concordia (V6), “We're not helping each other to win. We're helping each other to play, to make someone win.”

"
A Matter of Control or Safety?: Examining Parental Use of Technical Monitoring Apps on Teens' Mobile Devices,https://dl.acm.org/authorize?N656563,"
Adoption rates of parental control applications (""apps"") for teens' mobile devices are low, but little is known about the characteristics of parents (or teens) who use these apps. We conducted a web-based survey of 215 parents and their teens (ages 13-17) using two separate logistic regression models (parent and teen) to examine the factors that predicted parental use of technical monitoring apps on their teens' mobile devices. Both parent and teen models confirmed that low autonomy granting (e.g., authoritarian) parents were the most likely to use parental control apps. The teen model revealed additional nuance, indicating that teens who were victimized online and had peer problems were more likely to be monitored by their parents. Overall, increased parental control was associated with more (not fewer) online risks. We discuss the implications of these findings and provide design recommendations for mobile apps that promote online safety through engaged, instead of restrictive, parenting. This study is the first to investigate factors that contribute to the use of parental control apps. We found that these “control” apps are, indeed, appropriately named, as low autonomy granting or controlling parenting was one of the key factors that predicted adoption, but was also associated with higher levels of peer problems and online victimizations. Thus, we conclude that parental control does not equate to teen safety, and that autonomy-supportive, involved, yet strict parenting, whether through technology or not, is likely the best approach for online parenting.

"
CommunityCrit: Inviting the Public to Improve and Evaluate Urban Design Ideas through Micro-Activities,https://dl.acm.org/authorize?N656564,"
While urban design affects the public, most people do not have the time or expertise to participate in the process. Many online tools solicit public input, yet typically limit interaction to collecting complaints or early-stage ideas. This paper explores how to engage the public in more complex stages of urban design without requiring a significant time commitment. After observing workshops, we designed a system called CommunityCrit that offers micro-activities to engage communities in elaborating and evaluating urban design ideas. Through a four-week deployment, in partnership with a local planning group seeking to redesign a street intersection, CommunityCrit yielded 352 contributions (around 10 minutes per participant). The planning group reported that CommunityCrit provided insights on public perspectives and raised awareness for their project, but noted the importance of setting expectations for the process. People appreciated that the system provided a window into the planning process, empowered them to contribute, and supported diverse levels of skills and availability. This paper introduces a novel system for engaging the public in early-stage urban design processes. In collaboration with a planning group, we observed public workshops and developed a system to engage the public in ideation, elaboration, and evaluation. A key design consideration was how to yield valid contributions given that most people have limited time and bandwidth. We designed a flexible workflow that allows the public to choose the ideas and activities that interest them and offers five activities per session. Through a four-week deployment, CommunityCrit gathered 352 contributions from 39 people and produced new ideas that were comparable to those generated during the face-to-face workshops. CommunityCrit was successful in engaging people to elaborate on and evaluate ideas, rather than merely voicing issues or submitting ideas. However, our work explored a single urban design project within a fairly early stage in the process. Our outreach was limited to few weeks. We studied an urban design use case for one particular intersection. More work is required to understand how our approach can be adapted to other contexts and broader scales. We have identified several key areas for future work. Our research team played an active role in recruiting participants for the CommunityCrit deployment. In order to scale up participation the future, we will need better recruitment strategies and incentive mechanisms. As the contributions increase, further research will be needed to meaningfully organize results. The interface could be designed to support different ways to view contributions (e.g. newest, most/least popular, most/least comments, etc.) and visualize large numbers of ideas with list views or network graphs. Both organizers and community members will need tools to make sense of the collected input, including how to represent different stakeholder groups and how to extend the approach into more convergent stages of the urban design process, such as enabling deliberation and consensus building (e.g. [5, 28, 84]). For instance, recent research points to the benefits of identifying and visualizing points of disagreement to help with consensus building process [64], and to providing a clear causal link between public input and its impact on decisions and outcomes [80]. Finally, future work can focus on creating an authoring interface to allow planners to adapt the tool for other contexts. We also plan to explore the benefits of using adaptive algorithms to personalize activities based on people’s interests, skills, and other contextual details, such as where they live and their relationship with the project.

"
The Perils of Confounding Factors: How Fitts' Law Experiments can Lead to False Conclusions,https://dl.acm.org/authorize?N656575,"
The design of Fitts' historical reciprocal tapping experiment gravely confounds index of difficulty ID with target distance D: Summary statistics for the candidate Fitts model and a competing model may appear identical, and the validity of Fitts' model for some tasks can be legitimately questioned. We show that the contamination of ID by either target distance D or width W is due to the common practices of pooling and averaging data belonging to different distance-width (D,W) pairs for the same ID, and taking a geometric progression for values of D and W. We analyze a case study of the validation of Fitts' law in eye-gaze movements, where an unfortunate experimental design has misled researchers into believing that eye-gaze movements are not ballistic. We then provide simple guidelines to prevent confounds: Practitioners should carefully design the experimental conditions of (D,W), fully distinguish data acquired for different conditions, and put less emphasis on r² scores. We also recommend investigating the use of stochastic sampling for D and W. In the experimental testing of any mathematical model, we may distinguish two steps: 1. Sampling the factor space, e.g. Fitts’ traditional (D,W) space or Guiard’s form×scale space, thus defining a set of experimental conditions for data collection; 2. Processing the data by applying operations that yield a score. In Fitts’ law studies, this traditionally involves computing means of movement times and r 2 values between ID and MT. 6 Incidentally, this is precisely how Fitts summarized his data in his historical study [10, Fig. 4] CHI 2018 Paper CHI 2018, April 21–26, 2018, Montréal, QC, Canada Paper 196 Page 8 We have shown that a Fitts-like sampling of the (D,W) space solely associated with the computation of r 2 between ID and MT creates strong confounds between D and ID. We attributed this to the geometric progression of D and W. A simple workaround would seem to be to avoid such a sampling. However, using a constructive approach, we devised a sampling strategy that strongly confounds ID with any simple function of D and W. Avoiding Fitts-like designs is thus insufficient to avoid strong confounds. Based on these new results, we analyzed an apparent contradiction between the results of two eye-gaze pointing experiments using. We resolved the contradiction by noting that Carpenter’s formula is a widely accepted model for eye-gaze data and by showing that in one of the experiments, Fitts’ model was indistinguishable from Carpenter’s model due to the use of a Fitts-like design. Finally, we provide guidelines to avoid strong confounds between factors. We believe that Fitts’ law studies place too much emphasis on high r 2 values. It is crucial to introduce other considerations when validating a model, such as the flexibility of the evaluated model, the variability of the dataset and the possibility of competing models. Working with block averages or, worse, averages computed for equal values of ID, such as MT, dramatically decreases the number of points to be fitted, thereby mechanically increasing r 2 values. An interesting and simple way to prevent strong confounds is to use stochastic sampling of the (D,W) space. Stochastic sampling is a promising perspective, especially when considering the replication of studies, as a different but equivalent design can be ensured with each replication. However, more conceptual work is needed to support the idea of using random conditions in a controlled experiment.
"
Utilizing Narrative Grounding to Design Storytelling Gamesfor Creative Foreign Language Production,https://dl.acm.org/authorize?N656576,"
Foreign language students must learn to use language creatively to overcome knowledge gaps and keep readers or listeners interested. However, few tools exist to support practicing this skill. Therefore, we set out to explore design of storytelling games for practicing creative language use. Through an iterative design process, we identified narrative grounding (establishing common ground for collaborative narrative) as key to student engagement and learning. However, designing games for narrative grounding while keeping the game flexible enough to easily accommodate teacher goals is challenging. Considering this challenge, we designed a collaborative storytelling game where students help scaffold the narrative and teachers can easily integrate language goals with ""language cards"". In an in-classroom evaluation with 36 students, we show the importance of narrative grounding for learning. Qualitative evidence also suggests narrative grounding makes the game more engaging for players. We conclude with discussion of design implications for digital language learning tools. We presented the design process and findings from a storytelling game to help classroom teachers to encourage practicing creative foreign language production. Through our iterative design process, we explored challenges in designing storytelling games for the classroom, which we hope can inspire other design work in this area. We found that designing for narrative grounding can help us address many of the core challenges in storytelling games, and we used this to guide our final design and study. Quantitative results from our study as well as players’ feedback indicated that with more narrative grounding, students feel more engaged and can better improve creative language production. Finally, we offer recommendations to help designers encourage active listening with team play, utilize story elements to help scaffold learning and inspire creativity, and balance teacher-generated constraints with student-generated constraints to keep the game adaptable for ever-changing pedagogical goals. We hope that these recommendations can improve the design of future language learning systems.
"
Accessible Maps for the Blind: Comparing 3D Printed Models with Tactile Graphics,https://dl.acm.org/authorize?N656577,"
Tactile maps are widely used in Orientation and Mobility (O&M) training for people with blindness and severe vision impairment. Commodity 3D printers now offer an alternative way to present accessible graphics, however it is unclear if 3D models offer advantages over tactile equivalents for 2D graphics such as maps. In a controlled study with 16 touch readers, we found that 3D models were preferred, enabled the use of more easily understood icons, facilitated better short term recall and allowed relative height of map elements to be more easily understood. Analysis of hand movements revealed the use of novel strategies for systematic scanning of the 3D model and gaining an overview of the map. Finally, we explored how 3D printed maps can be augmented with interactive audio labels, replacing less practical braille labels. Our findings suggest that 3D printed maps do indeed offer advantages for O&M training. We investigated whether 3D printed maps offer benefits over the current use of tactile maps in O&M training. While it is clear that 3D models offer benefits when representing inherently three-dimensional objects, it was not clear whether they also offered benefits for maps. In a controlled user-study with 16 severely vision impaired adults we found a strong user preference for 3D plans. While it is possible that that selection and response bias influenced this preference for the new technology, a mitigating bias towards the more familiar tactile representation was also expected. Performance measures revealed that the 3D maps were easier to understand than tactile equivalents due to the use of 3D iconic symbols and allowed relative height of map elements to be more easily understood. There was also some advantage in short term recall but not for longer term recall. We conjecture that the fundamental reason why 3D prints were generally preferred to tactile graphics is a perceptual mismatch between touch and the vision-based conventions used in tactile graphics such as aerial or side views and depth cues like occlusion to show relative height. The use of 3D prints reduces the need for these conventions and so we infer reduces the cognitive load required to understand the graphic. While our studies provide partial support for this we plan to explore this interesting question further. We have also investigated techniques for augmenting 3D printed maps with interactive audio-labels. Interactive audiolabels provide several benefits over braille labels, including a clearer tactile experience and enabling people who cannot read braille to understand the graphic. They also allow information to be provided at different levels of detail and for labels to be updated without the need for reprinting. We explored how low-cost “maker community” electronics can be used to provide robust interactive audio labels on 3D printed maps. Such maps could be provided in public spaces such as train stations or shopping centres to improve accessibility. Our studies have significant implications for O&M training, suggesting that 3D prints augment or replace the current use of tactile maps. Based on our studies we have developed some initial guidelines for the design of accessible 3D prints and for teaching touch readers to read them. One limitation of our main study was that participants were touch readers with experience reading tactile graphics. A replication of the study with participants with recent vision loss is needed to determine whether similar benefits are found.

"
Reactile: Programming Swarm User Interfaces through Direct Physical Manipulation,https://dl.acm.org/authorize?N656578,"
We explore a new approach to programming swarm user interfaces (Swarm UI) by leveraging direct physical manipulation. Existing Swarm UI applications are written using a robot programming framework: users work on a computer screen and think in terms of low-level controls. In contrast, our approach allows programmers to work in physical space by directly manipulating objects and think in terms of high-level interface design. Inspired by current UI programming practices, we introduce a four-step workflow-create elements, abstract attributes, specify behaviors, and propagate changes-for Swarm UI programming. We propose a set of direct physical manipulation techniques to support each step in this workflow. To demonstrate these concepts, we developed Reactile, a Swarm UI programming environment that actuates a swarm of small magnets and displays spatial information of program states using a DLP projector. Two user studies-an in-class survey with 148 students and a lab interview with eight participants-confirm that our approach is intuitive and understandable for programming Swarm Uis. Although the proposed interactions were generally appreciated, the technical limitations of our hardware prototype sometimes limited the usability and capability of our approach. For example, the refresh rate in the current implementation depends on the number of objects and it becomes non-negligible as the number of markers increases. While this can be addressed through different implementations (e.g., using a transistor for each coil instead of multiplexing), this prevented users from receiving immediate feedback, making it difficult to predict a program’s behavior (P1, P7). Another hardware limitation is the information resolution. Although an individual marker is small in size, the minimum distance between two markers (30 mm) prevents them from forming a high-density shape. We also tested smaller and weaker magnets with N35 and 8 mm diameter, which requires only 12-15 mm distance. However, there is a trade-off with the weight of the marker. For example, in the above case, if we attach a 3D printed cap (1.1 g), it can become difficult to travel. However, we expect the minimum distance can be decreased using a galvanized steel case [23]. While these limitations are not about our proposed interaction techniques, having appropriate hardware can be crucial for better programming experiences. The hardware design option explored by this work is based on a swarm of simple, passive magnets actuated by a board. Another option for future work is to explore other types of robots such as wheel-based robots. One concern is that these robots may introduce cost increases and additional technical challenges in coordinating them as a swarm. However, we are seeing a growing body of research projects [22] and commercial efforts to address this concern [28], which can eventually lead to significant cost reduction and make Swarm UIs practical in the near future. This work contributes to that possible future by proposing a programming paradigm people can use to create a range of novel Swarm UI applications. In terms of the capability of our programming paradigm, an interesting discussion is how to extend our approach to additional dimensions. For example, P3 mentioned that additional attributes such as color, texture, and z height can be helpful for creating more expressive applications such as data visualizations, accessibility, and shape-displays. However, it is not trivial to extend our approach to these modalities. For future work, we will explore how our approach can be generalized to these other application domains.

"
Increasing User Attention with a Comic-based Policy,https://dl.acm.org/authorize?N657902,"
End user license agreements, terms of service agreements and privacy policies all suffer from many of the same problems: people rarely read them and yet still agree to whatever is contained within them. There are many usability challenges with these policies: they are often lengthy, with jargon filled language that is difficult to quickly comprehend. However, these notices are the primary tool for users to understand the privacy implications of their digital activities and make informed decisions on which websites and software they use. Prior research has explored alternative designs for such notices, using more visual and structured interfaces for conveying information. We expand upon these results by exploring a comic-based interface, examining whether it can engage users to pay more attention to a terms of service agreement. Our results indicate that the comic version did hold user attention for longer than text-based alternatives, encouraging deeper investigation into comic-based interfaces. Our results so far imply that taking the approach of displaying important information to a user using a more graphical design rather than lengthy jargon can improve attention and the average time spent reading ToS agreements. This initial comic design was very basic, with lots of text. While not unrealistic, it does not take full advantage of the affordances of comics to engage and communicate to users. We believe simpler and more graphical comics with reduced text will further benefit users. However, organizations would need guidelines and assistance for creating such summaries. They may also need incentives to do so. Future research will need to investigate what motivates the service providers to provide more usable policies to their users, and guidelines that are useful and actionable for them to create such interfaces. Finally, our ultimate goal is to help users make more informed decisions. As yet, we have found no evidence that user understanding is improved with the comic and we did not examine actual decision making in our study. We hope this study provides additional evidence that novel privacy notice designs warrant additional investigation, and encourage future studies to examine how to support privacy decision making.

"
Deployments of the table-non-table: A Reflection on the Relation Between Theory and Things in the Practice of Design Research,https://dl.acm.org/authorize?N657903,"
Design-oriented research in HCI has increasingly migrated towards theoretical perspectives to understand the implications of newly crafted technology in everyday life. However, in this context, the relations between theory and understanding the things we make are not always clear, especially the degree to which the nature of research artifacts is revealed through or determined by theory. We examine a series of field deployment studies we conducted with our research artifact table-non-table over the course of four and a half years that we came to see as a postphenomenological inquiry. Importantly, our interpretations of this artifact, methodological concerns, and theoretical groundings evolved over time. We account for and critically reflect on these shifts in the relationship between theory and our design artifact. We detail how theory was enacted and embodied in our design research practice and offer insights into the complex relations between theory and things in design-oriented HCI research. This work contributes an in-depth account of how, through empirical studies, a theoretical grounding can be enacted and embodied to reveal new insights on a design artifact that, in turn, can shape how studies of it are conducted and analyzed. Our reflexive analysis provides HCI researchers insights into the tensions, complexities, and challenges in the necessary interplay and entanglements between theory, empirical fieldwork, and design artifacts in an RtD context. We described how postphenomenology offered salient insights into this process and productively shaped our capacity to theoretically and empirically articulate key qualities of the table-non-table—an artifact explicitly diverting from utilitarian assumptions. We proposed opportunities for future design research inquiries into the complex, nuanced, dynamic nature of human-technology relations in everyday life.
"
Investigating How Smartphone Movement is Affected by Body Posture,https://dl.acm.org/authorize?N657904,"
We present an investigation into how hand usage is affected by different body postures (Sitting at a table, Lying down and Standing) when interacting with smartphones. We theorize a list of factors (smartphone support, body support and muscle usage) and explore their influence the tilt and rotation of the smartphone. From this we draw a list of hypotheses that we investigate in a quantitative study. We varied the body postures and grips (Symmetric bimanual, Asymmetric bimanual finger, Asymmetric bimanual thumb and Single-handed) studying the effects through a dual pointing task. Our results showed that the body posture Lying down had the most movement, followed by Sitting at a table and finally Standing. We additionally generate reports of motions performed using different grips. Our work extends previous research conducted with multiple grips in a sitting position by including other body postures, it is anticipated that UI designers will use our results to inform the development of mobile user interfaces. We have furthered the research of Eardley et al [9], demonstrating how hand movements are affected by grip and smartphone size. We extended this work by investigating grip and body posture (Standing, Sitting at a table and Lying down) and provided valuable metrics of hand movements for UI designers. We believe that designers can benefit from understanding the variances in smartphone rotations in order to create touchscreen interactions that adapt to the context of use.
"
"""I can do everything but see!""  -- How People with Vision Impairments Negotiate their Abilities in Social Contexts",https://dl.acm.org/authorize?N657915,"
This research takes an orientation to visual impairment (VI) that does not regard it as fixed or determined alone in or through the body. Instead, we consider (dis)ability as produced through interactions with the environment and configured by the people and technology within it. Specifically, we explore how abilities become negotiated through video ethnography with six VI athletes and spectators during the Rio 2016 Paralympics. We use generated in-depth examples to identify how technology can be a meaningful part of ability negotiations, emphasizing how these embed into the social interactions and lives of people with VI. In contrast to treating technology as a solution to a 'sensory deficit', we understand it to support the triangulation process of sense-making through provision of appropriate additional information. Further, we suggest that technology should not try and replace human assistance, but instead enable people with VI to better identify and interact with other people in-situ. Our research took an orientation to disability that regards it as something that is not fixed or manifested alone through the body, but created through a person’s social and material interactions with the world. We explored, through rich video ethnography, how athletes and spectators with VI negotiated their abilities in various contexts during the Rio Paralympics. Our findings presented in-depth examples that show how our participants triangulated information resources to understand their environments, posing this as a way to focus design in this space. We showed how providing additional information to on-going sense-making should not solely be considered as a mechanistic process, but as one that is deeply embedded in the social context in which interactions occur. We drew out how negotiating ability is shaped by perceived social norms, social opportunities for connection, and not least, assistance from other people, who can act as a vital information resource. Discussing these insights, we identified opportunities for technology design to become a meaningful part in processes of ability negotiation, and, through this, to assist in extending the abilities of people with vision impairments.
"
vrSocial: Toward Immersive Therapeutic VR Systems for Children with Autism,https://dl.acm.org/authorize?N657916,"
Social communication frequently includes nuanced nonverbal communication cues, including eye contact, gestures, facial expressions, body language, and tone of voice. This type of communication is central to face-to-face interaction, but can be challenging for children and adults with autism. Innovative technologies can provide support by augmenting human-delivered cuing and automated prompting. Specifically, immersive virtual reality (VR) offers an option to generalize social skill interventions by concretizing nonverbal information in real-time social interactions. In this work, we explore the design and evaluation of three nonverbal communication applications in immersive VR. The results of this work indicate that delivering real-time visualizations of proximity, speaker volume, and duration of one's speech is feasible in immersive VR and effective for real-time support for proximity regulation for children with autism. We conclude with design considerations for therapeutic VR systems. Immersive VR has tremendous potential as an accessible space for face-to-face social interaction for those with nonverbal communication challenges. Beyond reducing sensory input and designing visualization applications in VR, making accessible social spaces includes leveraging the potential to access mainstream infrastructure to support a broader base of users [23]. Additionally, we need to acknowledge and support the creation of safe spaces that can be shared beyond a therapy session [31]. Technologicallymediated interactions entail going beyond limited-access assistive technology systems to specifically designing for neuro-shared space that promote “compassionate interaction” [34]. If children with autism are at higher risk for social isolation and choose to use screens to escape an overwhelming world, is it not time for them to join in a shared safe place? Might this be a call to the next generation of immersive VR applications to reach beyond demonstrating efficacy as a therapeutic tool to strive for greater accessibility to the social world. Future work could focus on end-user toolkits to customize an immersive experience.
"
DeepWriting: Making Digital Ink Editable via Deep Generative Modeling,https://dl.acm.org/authorize?N657917,"
Digital ink promises to combine the flexibility and aesthetics of handwriting and the ability to process, search and edit digital text. Character recognition converts handwritten text into a digital representation, albeit at the cost of losing personalized appearance due to the technical difficulties of separating the interwoven components of content and style. In this paper, we propose a novel generative neural network architecture that is capable of disentangling style from content and thus making digital ink editable. Our model can synthesize arbitrary text, while giving users control over the visual appearance (style). For example, allowing for style transfer without changing the content, editing of digital ink at the word level and other application scenarios such as spell-checking and correction of handwritten text. We furthermore contribute a new dataset of handwritten text with fine-grained annotations at the character level and report results from an initial user evaluation. We have proposed a novel approach for making digital ink editable, which we call conditional variational recurrent neural networks (C-VRNN ). At the core of our method lies a deep NN architecture that disentangles content from style. The key idea underlying our approach is to treat style and content as two separate latent random variables with distributions learned during training. At sampling time one can then draw from the content and style component to edit either style, content or both, or one can generate entirely new samples. These learned statistical distributions make traditional approaches to NN training intractable, due to the lack of access to the true distributions. Moreover, to produce realistic samples of digital ink the model needs to perform auxiliary tasks, such as controlling the spacing in between words, character segmentation and recognition. Furthermore, we have build a variety of proof-of-concept applications, including conditional synthesis and editing of digital ink at the word level. Initial user feedback, while preliminary, indicates that users are largely positive about the capability to edit digital ink - in one’s own handwriting or in the style of another author. To enable the community to build on our work we release our implementation as open-source. Finally, we have contributed a compound dataset, consolidating existing and newly collected handwritten text into a single corpus, annotated to the character level. Data and code are publicly available 1 . While our model can create both disconnected and connected (cursive) styles, its performance is currently better for the former, simpler case. This also applies to most state-of-theart character recognition techniques, and we leave extending our method to fully support cursive script for future work. Further, we are planning to integrate our currently auxiliary character recognition network into the proposed architecture. One interesting direction in this respect would be the inclusion of a full language model. Finally, and in part inspired by initial feedback, we believe that the underlying technology bears a lot of potential for research in other application domains dealing with time-series, such as motion data (e.g., animation, graphics) or sketches and drawings (e.g., arts and education).

"
Media of Things: Supporting the Production of Metadata Rich Media Through IoT Sensing,https://dl.acm.org/authorize?N657918,"
Rich metadata is becoming a key part of the broadcast production pipeline. This information can be used to deliver compelling new consumption experiences which are personalized, location-aware, interactive and multi-screen. However, media producers are struggling to generate the metadata required for such experiences, using inefficient post-production solutions which are limited in how much of the original context they can capture. In response, we present Media of Things (MoT), a tool for on-location media productions. MoT enables practical and flexible generation of sensor based point-of-capture metadata. We demonstrate how embedded ubiquitous sensing technologies such as the Internet of Things can be leveraged to produce context rich, time sequenced metadata in a production studio. We reflect on how this workflow can be integrated within the constraints of broadcast production and the possibilities that emerge from access to rich data at the beginning of the production lifecycle to produce well described media for reconfigurable consumption. In this paper, we present Media of Things, our implementation of a point-of-capture metadata capture system which integrates into the existing production workflow. By supporting the capture of raw, time-based sensor data from multiple sources in real-time, MoT provides a solid base for creating rich Object-based Media based experiences in the future. We highlight the symmetry in using IoT based technologies with ‘invisible’ sensors which can record actions and object use on a film set which responds to the need to capture such contextual and descriptive information from broadcasts in nearly-live scenarios and post production. Through the deployment of Media of Things in a real production scenario, we explore how metadata capture can become part of the film production workflow. Our primary contribution is the validation of sensor-based metadata capture as a sustainable and flexible process that can be included reasonably within the existing constraints of the media production pipeline. Specifically, we recommend that the ‘Sensor Role’ should be recognized as a distinct and valued member of the production team, in line with the roles allocated to ‘Sound’ and ‘Lighting’ professionals, facilitating a smooth transition into the professional workflow. When planning for MoT style productions however we caution that the selection of sensors is a nuanced and often difficult tradeoff between granularity of data and operational requirements for configuration. This is an area that we acknowledge would benefit from more work, trialing different sensors for the collection of data. Although our deployment was within the situated and constrained environment of a cookery show, the structured and standardized nature of film production means that MoT has been used against the inherent situational factors which define such workflows. Combined with the flexibility offered by our sensor agnostic approach, we envision that MoT could be used as part of the normal production workflow for any production scenario, when appropriate sensing is applied. In summary, we encourage the community to consider how production tools such as MoT which leverage existing trends in IoT can be integrated into existing production pipelines now, to more quickly enable the rich, immersive and indeed exciting experiences that Object-based Media can deliver.
"
Enhancing Online Problems Through Instructor-Centered Tools for Randomized Experiments,https://dl.acm.org/authorize?N657919,"
Digital educational resources could enable the use of randomized experiments to answer pedagogical questions that instructors care about, taking academic research out of the laboratory and into the classroom. We take an instructor-centered approach to designing tools for experimentation that lower the barriers for instructors to conduct experiments. We explore this approach through DynamicProblem, a proof-of-concept system for experimentation on components of digital problems, which provides interfaces for authoring of experiments on explanations, hints, feedback messages, and learning tips. To rapidly turn data from experiments into practical improvements, the system uses an interpretable machine learning algorithm to analyze students' ratings of which conditions are helpful, and present conditions to future students in proportion to the evidence they are higher rated. We evaluated the system by collaboratively deploying experiments in the courses of three mathematics instructors. They reported benefits in reflecting on their pedagogy, and having a new method for improving online problems for future students. To help realize the promise of experimentation to help instructors enhance digital educational resources, this paper presented design guidelines for more instructor-centered tools for experimentation. Our goal was to reduce the programming knowledge and time that instructors need to conduct experiments, and to help instructors more rapidly use data to enhance the learning experience of the next student. We instantiated a solution in the proof-of-concept system DynamicProblem, which lowered the barriers for instructors to conduct experiments on elaboration messages in online problems, such as explanations, hints, and learning tips. The system used a multiarmed bandit algorithm to analyze and use data about students’ ratings, in order to present the higher rated conditions to future students, automatically enhancing the problems. Case study deployments with three instructors suggested the system helped them reflect on how to improve pedagogy, and provided a data-driven pathway for enhancing their online problems. Just as decades of work have established how learning researchers can effectively use experiments, a great deal of future work is needed to understand how instructors can effectively use experiments. A hosted version of the DynamicProblem system can be used in courses by following the instructions at www.josephjaywilliams.com/dynamicproblem, and the code base is available on Github by following the URL http://tiny.cc/githubdynamicproblem. We hope that this paper provides a foundation for future investigations of instructorcentered experimentation.
"
Exploring the Role of Conversational Cues in Guided Task Support with Virtual Assistants,https://dl.acm.org/authorize?N657910,"
Voice-based conversational assistants are growing in popularity on ubiquitous mobile and stationary devices. Cortana, as well as Google Home, Amazon Echo, and others, can provide support for various tasks from managing reminders to booking a hotel. However, with few exceptions, user input is limited to explicit queries or commands. In this work, we explore the role of implicit conversational cues in guided task completion scenarios. In a Wizard of Oz study, we found that, for the task of cooking a recipe, nearly one-quarter of all user-assistant exchanges were initiated from implicit conversational cues rather than from plain questions. Given that these implicit cues occur in such high frequency, we conclude by presenting a set of design implications for the design of guided task experiences in contemporary conversational assistants. Current voice-based conversational assistants mostly abide by the <trigger word, question, answer> paradigm, which constrains user interactions, and a number of implicit conversational cues are missed as a result. In this work we have considered a set of common implicit verbal cues exhibited by users of a simulated conversational assistant for the task of cooking a culinary recipe. We have described these cues and their intents in detail and have provided a set of design implications for designing task-oriented conversational systems.
"
Simulator Sickness in Augmented Reality Training Using the Microsoft HoloLens,https://dl.acm.org/authorize?N657911,"
Augmented Reality is on the rise with consumer-grade smart glasses becoming available in recent years. Those interested in deploying these head-mounted displays need to understand better the effect technology has on the end user. One key aspect potentially hindering the use is motion sickness, a known problem inherited from virtual reality, which so far remains under-explored. In this paper we address this problem by conducting an experiment with 142 subjects in three different industries: aviation, medical, and space. We evaluate whether the Microsoft HoloLens, an augmented reality head-mounted display, causes simulator sickness and how different symptom groups contribute to it (nausea, oculomotor and disorientation). Our findings suggest that the Microsoft HoloLens causes across all participants only negligible symptoms of simulator sickness. Most consumers who use it will face no symptoms while only few experience minimal discomfort in the training environments we tested it in. In this work, we empirically investigated simulator sickness in augmented reality training applications using Microsoft HoloLens. We found insignificant effects of simulator sickness. The main factor contributing to it is eyestrain, a symptom in the oculomotor group. We concluded that mixed reality interaction (spanning both the real and physical world) is less likely to cause simulator sickness than interaction with virtual elements alone. Simulator sickness should be understood as a complex phenomenon, which differs from our everyday conception of motion sickness, often simply perceived as nausea. Based on our results, eyestrain can be seen as the most common and prominent symptom caused by using the HoloLens. The symptoms observed appeared less frequent and milder than in comparable virtual reality simulators. Nonetheless, they cause discomfort to and affect technology acceptance of those selected few suffering of them. Innovation on how to alleviate these symptoms would certainly be beneficial to facilitate increase uptake.

"
Experiencing the Body as Play,https://dl.acm.org/authorize?N657912,"
Games research in HCI is continually interested in the human body. However, recent work suggests that the field has only begun to understand how to design bodily games. We propose that the games research field is advancing from playing with digital content using a keyboard, to using bodies to play with digital content, towards a future where we experience our bodies as digital play. To guide designers interested in supporting players to experience their bodies as play, we present two phenomenological perspectives on the human body (Körper and Leib) and articulate a suite of design tactics using our own and other people's work. We hope with this paper, we are able to help designers embrace the point that we both ""have"" a body and ""are"" a body, thereby aiding the facilitation of the many benefits of engaging the human body through games and play, and ultimately contributing to a more humanized technological future. Interaction design and, in particular, game design has an ongoing interest in the intersection between interactive technology and the human body, fuelled by technological advancements. Yet, recent scholarly work suggests that the field has only just begun to fully understand the various perspectives through which designers can see the human body. To advance the field, we introduced two perspectives on the human body (Körper and Leib) and articulated implications for design. We discussed these perspectives by looking at a set of bodily game and play systems from our own and other’s work. We see our work not as a complete investigation into the Körper and Leib in game design, but rather as a springboard for future investigations. In summary, our work aims to contribute to the emerging intersection between the human body and interactive games and play. We believe that for a successful combination of technology and the human body, we need to move beyond seeing the human body as solely a “thing”, instead we need to embrace that we both have a body and are a body. If we do so, we believe it is possible to experience our body as play. We hope with our work we are aiding in facilitating the many benefits of engaging the human body through games and play, ultimately contributing to a more humanized technological future.
"
Upstanding by Design: Bystander Intervention in Cyberbullying,https://dl.acm.org/authorize?N657913,"
Although bystander intervention can mitigate the negative effects of cyberbullying, few bystanders ever attempt to intervene. In this study, we explored the effects of interface design on bystander intervention using a simulated custom-made social media platform. Participants took part in a three-day, in-situ experiment, in which they were exposed to several cyberbullying incidents. Depending on the experimental condition, they received different information about the audience size and viewing notifications intended to increase a sense of personal responsibility in bystanders. Results indicated that bystanders were more likely to intervene indirectly than directly, and information about the audience size and viewership increased the likelihood of flagging cyberbullying posts through serial mediation of public surveillance, accountability, and personal responsibility. The study has implications for understanding bystander effect in cyberbullying, and how to develop design solutions to encourage bystander intervention in social media. This research presents a new approach to increasing bystander intervention when cyberbullying occurs. We used research on the bystander effect and BIM to inform design interventions on a custom-made social media platform to increase upstanding behavior. To do this, we altered the site’s user interface by adding markers of public surveillance to increase bystanders’ personal responsibility and likelihood of intervention. Although most bystanders did not intervene throughout the three-day study, we found that participants who received information on audience size and view notifications were more likely to intervene because they internalized personal responsibility prompted by increases in accountability and public surveillance. This suggests that upstanding by design could be a viable solution that can help bystanders to become upstanders by encouraging their sense of personal responsibility for a cyberbullying situation.

"
Examining the Demand for Spam: Who Clicks?,https://dl.acm.org/authorize?N657914,"
Despite significant advances in automated spam detection, some spam content manages to evade detection and engage users. While the spam supply chain is well understood through previous research, there is little understanding of spam consumers. We focus on the demand side of the spam equation examining what drives users to click on spam via a large-scale analysis of de-identified, aggregated Facebook log data (n=600,000). We find (1) that the volume of spam and clicking norms in a users' network are significantly related to individual consumption behavior; (2) that more active users are less likely to click, suggesting that experience and internet skill (weakly correlated with activity level) may create more savvy consumers; and (3) we confirm previous findings about the gender effect in spam consumption, but find this effect largely corresponds to spam topics. Our findings provide practical insights to reduce demand for spam content, thereby affecting spam profitability. In this work we contribute one of the largest scale studies of spam clicking behavior to date, and the first study to examine spam consumption behavior on social media. We find that more active users are less likely to consume spam, and find a weak positive correlation between activity level and Internet and Facebook skills – suggesting that higher skill may, in part, help users avoid spam. We also find that the volume of spam in a users’ social network as well as clicking norms in that network influence their behavior. We identify a new, nuanced relationship between resharing, content-viewer relationships and click behavior. We find that resharing increases clicking on spam from friends, but decreases clicking on spam posted by friends-of-friends or Pages, suggesting that resharing serves as different heuristic roles depending on the type of content. CHI 2018 Paper CHI 2018, April 21–26, 2018, Montréal, QC, Canada Paper 212 Page 7 Additionally, we echo prior results finding that women are more likely to click on spam [26, 49], but find based on the results of open-coding of 250 pieces of spam content, that this relationship between gender and clicking is likely due to differences in the type of spam seen on Facebook, rather than anything intrinsically gender-related. Our results suggest that spam topics are likely a significant feature driving click behavior. In summary, we provide some of the first insights on users’ consumption of spam on social media and find that spam clicking behavior on Facebook is affected by factors unique from those that affect clicking on general content. Based on these results, we suggest new directions for the design and targeting of spam interventions to disrupt the spam value chain and keep users secure.
"
ColorMod: Recoloring 3D Printed Objects using Photochromic Inks,https://dl.acm.org/authorize?N657925,"
Recent research has shown how to change the color of existing objects using photochromic materials. These materials can switch their appearance from transparent to colored when exposed to light of a certain wavelength. The color remains even when the object is removed from the light source. The process is fully reversible allowing users to recolor the object as many times as they want. So far, these systems have been limited to single color changes, i.e. changes from transparent to colored. In this paper, we present ColorMod, a method to extend this approach to multi-color changes (e.g., red-to-yellow). We accomplish this using a multi-color pattern with one color per voxel across the surface of the object. When recoloring the object, our system locally activates only those voxels that have the desired color and turns all other voxels off. We describe ColorMod's hardware/software system and its user interface that comes with a conversion tool for 3D printing as well as a painting tool that matches physical voxels with the desired appearance. We also contribute our own material formula for a 3D-printable photochromic ink. We presented ColorMod, a method for changing the color of a 3D-printed object even after fabrication. We demonstrated how by printing different photochromic colors in a dense multi-color voxel pattern across an object’s surface, we can recolor the object by selectively turning specific color voxels on and off. We explained the different factors influencing how precisely and how fast we can activate individual voxels and also provided a description of the material formula for our custom 3D printable photochromic ink. For future work, we plan to make the setup more compact to allow for mobile use. Our long-term goal is to allow users to recolor objects while on the go using a smart phone with an integrated micro-projector and UV light.

"
Multidimensional Risk Communication: Public Discourse on Risks during an Emerging Epidemic,https://dl.acm.org/authorize?N657926,"
Crisis informatics has examined how institutions and individuals seek, communicate, and curate information in response to crises. The public's communication and perception of risks on social media remain understudied. In this study, we report a qualitative analysis of public perceptions of risks and risk management measures on Reddit during the Zika crisis, an emerging epidemic associated with high uncertainty regarding pathology, epidemiology, and broad consequences. We found two types of perceived risks: ones directly caused by the Zika virus, and ones potentially introduced by authorities' risk management measures. Risk perceptions unfolded along multiple dimensions beyond the imminent and personal level. Reddit users discussed in a speculative way to foresee various risks in the long run or at larger geographical scales. We discuss the multidimensionality and speculative nature of risk perception on social media, and derive implications for crisis informatics research and public health research and practice. In this paper, we presented a qualitative content analysis of risk communication and perception in the Zika outbreak. We highlighted the multidimensionality and speculative nature of risk perception. We discussed the implications of the multidimensional perspective for crisis informatics. We proposed to rethink authorities’ existing model of risk communication, in favor of a participatory approach. More research can be done on risk communication on social media to understand the public’s concerns, reasoning processes, and interests, to map out a fuller set of risks associated with different crisis types, and to reveal the role of risk communication in impacting citizens and institutions’ informational practices during crises.
"
A Face Recognition Application for People with Visual Impairments: Understanding Use Beyond the Lab,https://dl.acm.org/authorize?N657927,"
Recognizing others is a major challenge for people with visual impairments (VIPs) and can hinder engagement in social activities. We present Accessibility Bot, a research prototype bot on Facebook Messenger, that leverages state-of-the-art computer vision and a user's friends' tagged photos on Facebook to help people with visual impairments recognize their friends. Accessibility Bot provides users information about identity and facial expressions and attributes of friends captured by their phone's camera. To guide our design, we interviewed eight VIPs to understand their challenges and needs in social activities. After designing and implementing the bot, we conducted a diary study with six VIPs to study its use in everyday life. While most participants found the Bot helpful, their experience was undermined by perceived low recognition accuracy, difficulty aiming a camera, and lack of knowledge about the phone's status. We discuss these real-world challenges, identify suitable use cases for Accessibility Bot, and distill design implications for future face recognition applications. We designed Accessibility Bot, a research prototype Bot on Facebook Messenger, which recognizes friends of people with visual impairments by leveraging Facebook’s high performance face recognition algorithms and the large set of tagged photos on Facebook. We evaluated Accessibility Bot outside of a laboratory environment through a weeklong diary study to understand participants’ daily experiences with the Bot in different social situations. We found that Accessibility Bot was helpful for most participants. However, people’s experience was undermined by the low perceived accuracy, difficulty with aiming the camera, and other usability issues that do not typically arise in lab studies. We discussed these real-world challenges and provided design implications for future face recognition applications for users with visual impairments.
"
Evaluation Beyond Usability: Validating Sustainable HCI Research,https://dl.acm.org/authorize?N657928,"
The evaluation of research artefacts is an important step to validate research contributions. Sub-disciplines of HCI often pursue primary goals other than usability, such as Sustainable HCI (SHCI), HCI for development, or health and wellbeing. For such disciplines, established evaluation methods are not always appropriate or sufficient, and new conventions for identifying, discussing, and justifying suitable evaluation methods need to be established. In this paper, we revisit the purpose and goals of evaluation in HCI and SHCI, and elicit five key elements that can provide guidance to identifying evaluation methods for SHCI research. Our essay is meant as a starting point for discussing current and improving future evaluation practice in SHCI; we also believe it holds value for other subdisciplines in HCI that encounter similar challenges while evaluating their research. In this paper, we sought to explore ways to address the evaluation problem when the primary goal is not usability. We have looked at the history of evaluation in HCI and SHCI and discussed ways to assess the validity of sustainability in research projects. As a starting point for and to facilitate the debate within the community we highlighted five ingredients that, based on past research on evaluation in HCI and other fields, are critical for identifying an appropriate evaluation method. We emphasize that our recipe is not meant as a framework for evaluation itself, but as a process to guide researchers towards identifying the most suitable evaluation for their research; and to give researchers a common set of terms for justifying and debating evaluation. In our understanding, this work is only a first step towards solving the evaluation problem, and while we believe that it is a helpful step not only for SHCI but also for other areas in HCI, more work is required. We hope that the community builds on this work and that it spurs the debate about identifying new ways to evaluate research artefacts beyond usability.
"
Beyond Translation: Design and Evaluation of an Emotional and Contextual Knowledge Interface for Foreign Language Social Media Posts,https://dl.acm.org/authorize?N657929,"
Although many social media sites now provide machine translation (MT) for foreign language posts, translation of a post may not suffice to support understanding of, and engagement with, that post. We present SenseTrans, a tool that provides emotional and contextual annotations generated by natural language analysis in addition to machine translation. We evaluated SenseTrans in a laboratory experiment in which native English speakers browsed five Facebook profiles in foreign languages. One group used the SenseTrans interface while the other group used MT alone. Participants using SenseTrans reported significantly greater understanding of the posts, and greater willingness to engage with the posts. However, no additional cognitive load was associated with using an interface that provided more information. These results provide promising support for the idea of using computational tools to annotate communication to support multilingual sense making and interaction on social media. We present SenseTrans, a tool that provides emotional and contextual information generated by natural language analysis in addition to machine translation to support people’s sense making process for foreign language social media posts. We evaluated SenseTrans in a laboratory experiment in which native English speakers browsed five Facebook profiles in foreign languages. Participants using SenseTrans reported significantly greater understanding of the posts and more willingness to engage with the posts, but no additional cognitive load.cThese results provide promising support for the idea of using NLP and other computational tools to annotate communication to help people better understand and interact with others in social media across language barriers, and potentially in many other contexts as well.
"
Projective Windows: Bringing Windows in Space to the Fingertip,https://dl.acm.org/authorize?N657920,"
In augmented and virtual reality (AR and VR), there may be many 3D planar windows with 2D texts, images, and videos on them. However, managing the position, orientation, and scale of such a window in an immersive 3D workspace can be difficult. Projective Windows strategically uses the absolute and apparent sizes of the window at various stages of the interaction to enable the grabbing, moving, scaling, and releasing of the window in one continuous hand gesture. With it, the user can quickly and intuitively manage and interact with windows in space without any controller hardware or dedicated widget. Through an evaluation, we demonstrate that our technique is performant and preferable, and that projective geometry plays an important role in the design of spatial user interfaces. We proposed Projective Windows, a technique for arranging 3D planar windows in AR and VR workspaces, which is, thanks to its strategic use of the absolute and apparent sizes of a window at various stages of the interaction, minimal, direct, and intuitive, requiring only a single continuous flow of hand gesture and no other controllers and UI widgets. We evaluated our technique quantitatively and qualitatively, and found that it performed significantly better against a baseline technique for two key metrics: task completion time (22%) and operations count (40%). We furthermore demonstrated the relevance and usefulness of exploiting projective geometry in designing spatial UIs. Some speculate that AR and VR devices might come to replace all screen-based devices in the future [17]. For Projective Windows to facilitate seamless interactions with 2D content in future immersive 3D experiences, more work is needed on the systematic use of ad-hoc surfaces, such as the palm or a handheld plane prop, and surrounding geometries, such as wall edges or ceilings. In addition, it should better integrate with other relevant 3D windows techniques [5, 9, 10, 14].

"
Scenariot: Spatially Mapping Smart Things Within Augmented Reality Scenes,https://dl.acm.org/authorize?N657921,"
The emerging simultaneous localizing and mapping (SLAM) based tracking technique allows the mobile AR device spatial awareness of the physical world. Still, smart things are not fully supported with the spatial awareness in AR. Therefore, we present Scenariot, a method that enables instant discovery and localization of the surrounding smart things while also spatially registering them with a SLAM based mobile AR system. By exploiting the spatial relationships between mobile AR systems and smart things, Scenariot fosters in-situ interactions with connected devices. We embed Ultra-Wide Band (UWB) RF units into the AR device and the controllers of the smart things, which allows for measuring the distances between them. With a one-time initial calibration, users localize multiple IoT devices and map them within the AR scenes. Through a series of experiments and evaluations, we validate the localization accuracy as well as the performance of the enabled spatial aware interactions. Further, we demonstrate various use cases through Scenariot. Our paper builds towards the broad goal of empowering users with the ability to quickly discover and intuitively interact with the connected smart things within the surrounding environment. We propose Scenariot to discover and localize the surrounding smart things as well as spatially register them with a SLAM based mobile AR device. By leveraging the spatial registration, in-situ AR interaction with the IoT devices is enabled. Through our experiments and user studies, we verified our method is capable of providing object level localization accuracy ∼ 0.4m with multiple devices distributed in a cluttered scene (∼ 10×10m). Therefore, we believe this work can bring spatial awareness to the IoT devices within an AR scene and further inspire advanced interaction designs.

"
AlterWear: Battery-Free Wearable Displays for Opportunistic Interactions,https://dl.acm.org/authorize?N657922,"
As the landscape of wearable devices continues to expand, power remains a major issue for adoption, usability, and miniaturization. Users are faced with an increasing number of personal devices to manage, charge, and care for. In this work, we argue that power constraints limit the design space of wearable devices. We present AlterWear: an architecture for new wearable devices that implement a batteryless design using electromagnetic induction via NFC and bistable e-ink displays. Although these displays are active only when in proximity to an NFC-enabled device, this unique combination of hardware enables both quick, dynamic and long-term interactions with persistent visual displays. We demonstrate new wearables enabled through AlterWear with dynamic, fashion-forward, and expressive displays across several form factors, and evaluate them in a user study. By forgoing the need for onboard power, AlterWear expands the ecosystem of functional and fashionable wearable technologies. In this paper, we presented AlterWear: an architecture for battery-free wearable displays supporting opportunistic interactions. We hope that AlterWear will inspire and enable new wearable devices that expand the ecosystem of functional and fashionable wearable technologies.

"
The Role of Gamification in Participatory Environmental Sensing: A Study In the Wild,https://dl.acm.org/authorize?N657923,"
Participatory sensing (PS) and citizen science hold promises for a genuinely interactive and inclusive citizen engagement in meaningful and sustained collection of data about social and environmental phenomena. Yet the underlying motivations for public engagement in PS remain still unclear particularly regarding the role of gamification, for which HCI research findings are often inconclusive. This paper reports the findings of an experimental study specifically designed to further understand the effects of gamification on citizen engagement. Our study involved the development and implementation of two versions (gamified and non-gamified) of a mobile application designed to capture lake ice coverage data in the sub-arctic region. Emerging findings indicate a statistically significant effect of gamification on participants' engagement levels in PS. The motivation, approach and results of our study are outlined and implications of the findings for future PS design are reflected. In this work, we present the results from an experimental user study that explored the impact of gamification on engagement and user experience. We designed, deployed and evaluated two environmental sensing applications (one gamified and one non-gamified) via a 20 days experiment with volunteers. We found that gamification affected the participants’ engagement in a positive way (producing more submissions), without improving nor compromising their user experience. This led us to think that in order to produce human engagement, altering interfaces is not enough. This supports the review results of Nacke et al. [44] that adding simple visual manifestation of gamification elements or deterministic mechanics to the interface is not enough without considering other aspects of engagement. Deterding et al. propose [10] that gamified motivation design should move from stimulus-effect determinism to providing fun, engaging and playful experiences. Our participants mentioned ""proving data for a good cause"", ""I like going out to nature"" and ""it is useful not only for lakes, but for people as well"" as motivating factors. This leads us to conclude that the gamification design for environmental sensing had a positive effect on participants’ engagement. Even when the underlying technology is still evolving, participatory sensing has already shown its great potential, not only as a tool for citizens’ collecting data but also as a vehicle for engaging a large public community in solving social and environmental challenges. These systems have the potential to close the gaps among researchers, environmental experts, decision-makers, and the people, while collecting data and building a whole new level of services (from the people, for the people). However, the success of participatory sensing relies heavily on continuous citizen participation and the computational capacity to extract patterns from the data being collected.
"
How Information Sharing about Care Recipients by Family Caregivers Impacts Family Communication,https://dl.acm.org/authorize?N657924,"
Previous research has shown that tracking technologies have the potential to help family caregivers optimize their coping strategies and improve their relationships with care recipients. In this paper, we explore how sharing the tracked data (i.e., caregiving journals and patient's conditions) with other family caregivers affects home care and family communication. Although previous works suggested that family caregivers may benefit from reading the records of others, sharing patients' private information might fuel negative feelings of surveillance and violation of trust for care recipients. To address this research question, we added a sharing feature to the previously developed tracking tool and deployed it for six weeks in the homes of 15 family caregivers who were caring for a depressed family member. Our findings show how the sharing feature attracted the attention of care recipients and helped the family caregivers discuss sensitive issues with care recipients. This paper showed how information sharing among family caregivers affected homecare and family communication. We developed a tracking tool that allows family caregivers to record their caregiving activities and care recipient’s conditions and share them with other family caregivers. A deployment study of our tool revealed that the sharing feature not only helped the caregivers improve their coping strategies but also positively affected the care recipients. Our most significant finding is that the tool provided the caregivers and the care recipients with indirect ways to discuss sensitive home issues. Our findings describe how the sharing feature helped them avoid direct communication/conflict while still allowing them to indirectly discuss problems. The sharing feature also provided care recipients indirect ways to learn and gain emotional support from other caregivers, which sometimes motivated them to engage in positive behaviors. We believe these findings will contribute to the field of HCI by suggesting new ways for designing health-tracking technologies and information sharing.
"
DataInk: Direct and Creative Data-Oriented Drawing,https://dl.acm.org/authorize?N657935,"
Creating whimsical, personal data visualizations remains a challenge due to a lack of tools that enable for creative visual expression while providing support to bind graphical content to data. Many data analysis and visualization creation tools target the quick generation of visual representations, but lack the functionality necessary for graphics design. Toolkits and charting libraries offer more expressive power, but require expert programming skills to achieve custom designs. In contrast, sketching affords fluid experimentation with visual shapes and layouts in a free-form manner, but requires one to manually draw every single data point. We aim to bridge the gap between these extremes. We propose DataInk, a system supports the creation of expressive data visualizations with rigorous direct manipulation via direct pen and touch input. Leveraging our commonly held skills, coupled with a novel graphical user interface, DataInk enables direct, fluid, and flexible authoring of creative data visualizations. We propose DataInk, a system for authoring whimsical and personalized visual representations of data. DataInk aims at supporting the creative process while affording design expression to craft visually-rich graphics via freeform sketching and rigorous execution by maintaining a tight coupling between visuals and data via direct manipulation. The design decisions of supporting direct manipulation are embodied as a simple set of pen-and-touch interaction techniques and a fluid graphical user interface, which enable both rich expressiveness and effortless execution of data visualizations. A user study with eight designers and non-experts suggests that this approach is promising and demonstrate that DataInk allows users to unleash their creativity to experiment with differently visual designs and create a diverse set of glyph-based visualizations.

"
Remediating a Design Tool: Implications of Digitizing Sticky Notes,https://dl.acm.org/authorize?N657936,"
Sticky notes are ubiquitous in design processes because of their tangibility and ease of use. Yet, they have well-known limitations in professional design processes, as documentation and distribution are cumbersome at best. This paper compares the use of sticky notes in ideation with a remediated digital sticky notes setup. The paper contributes with a nuanced understanding of what happens when remediating a physical design tool into digital space, by emphasizing focus shifts and breakdowns caused by the technology, but also benefits and promises inherent in the digital media. Despite users' preference for creating physical notes, handling digital notes on boards was easier and the potential of proper documentation make the digital setup a possible alternative. While the analogy in our remediation supported a transfer of learned handling, the users' experiences across technological setups impact their use and understanding, yielding new concerns regarding cross-device transfer and collaboration. This paper presented a qualitative, comparative study of physical sticky note use to a remediated digital sticky notes setup. The results were analyzed using the activity theoretical HumanArtifact Model pointing at expected, and unexpected, differences and similarities between the setups, when it comes to physical adaptation and learned handling. Despite users’ general preference for creating physical sticky notes, the digital technologies have reached a point where easier handling of digital sticky notes on interactive screens and the potential of proper design documentation makes it an alternative to their physical counterparts. Addressing the digital sticky notes as remediation, we conclude that while the analogy in our remediation supported a transfer of learned handling, the users’ experiences from other technological setups impact their use and understanding, leading to new concerns regarding crossdevice transfer and collaboration, and fewer for tangibility.

"
Slacktivists or Activists?: Identity Work in the Virtual Disability March,https://dl.acm.org/authorize?N657937,"
Protests are important social forms of activism, but can be inaccessible to people with disabilities. Online activism, like the 2017 Disability March, has provided alternative venues for involvement in accessible protesting and social movements. In this study, we use identity theory as a lens to understand why and how disabled activists engaged in an online movement, and its impact on their self-concepts. We interviewed 18 disabled activists about their experiences with online protesting during the Disability March. Respondents' identities (as both disabled individuals and as activists) led them to organize or join the March, evolved alongside the group's actions, and were reprioritized or strained as a result of their involvement. Our findings describe the values and limitations of this activism to our respondents, highlight the tensions they perceived about their activist identities, and present opportunities to support further accessibility and identity changes by integrating technology into their activist experiences. This paper presents how an online social movement unfolded and influenced participants’ identities as disabled activists. People with disabilities must overcome multiple accessibility barriers to be involved with traditional protests and activism. These barriers were described as the main reason for the successful growth of this online movement by our respondents. Using identity theory as a theoretical lens, we have highlighted how marchers conducted identity work during the movement and experienced self-change. However, the marchers still questioned the legitimacy of their actions and activist identities. Based on our findings, we believe the legitimacy of virtual activism must be reconsidered by able-bodied activists. We propose ways to support participation of disabled people in online activism.
"
Applying Computational Analysis to Textual Data from the Wild: A Feminist Perspective,https://dl.acm.org/authorize?N657938,"
With technologies that afford much larger-scale data collection than previously imagined, new ways of processing and interpreting qualitative textual data are required. HCI researchers use a range of methods for interpreting the 'full range of human experience' from qualitative data, however, such approaches are not always scalable. Feminist geography seeks to explore how diverse and varied accounts of place can be understood and represented, whilst avoiding reductive classification systems. In this paper, we assess the extent to which unsupervised topic models can support such a research agenda. Drawing on literature from Feminist and Critical GIS, we present a case study analysis of a Volunteered Geographic Information dataset of reviews about breastfeeding in public spaces. We demonstrate that topic modelling can offer novel insights and nuanced interpretations of complex concepts such as privacy and be integrated into a critically reflexive feminist data analysis approach that captures and represents diverse experiences of place. In this paper we presented a combination of computational and human approaches for textual analysis, suitable for largescale experiential data. We have demonstrated the underlying methods and challenges behind exploring and analysing a reallife VGI dataset and positioning this experiential, qualitative dataset (FeedFinder reviews) in dialogue with a structural, quantitative dataset (IMD). We position this work alongside other efforts to develop a Feminist HCI methodology, as it begins to unpick the challenges and opportunities of dialectic information gathering, exploring how a heterogeneity of data sources and interpretations can be employed [5] and combined with computational approaches. An essential a priori step in achieving this is clearly to do with data collection. In order to empirically examine the ‘knowledges among very different and power-differentiated communities’ [17], we must first establish access to accounts of situated knowledge. As demonstrated by the work of Balaam et al. [2] and FeedFinder, this is possible. We demonstrated that topic models can identify novel and insightful patterns in such data, providing an empirically grounded entry-point for researchers. This approach facilitated the experiential accounts of place to be brought to bear upon the purely quantitatively driven IMD data. This novel combination of existing structural data and experiential VGI data enables us to systematically interpret the impact of materialsocio-economic factors on people’s experiences and concerns when breastfeeding in public. In particular, a key finding of this case study re-frames what breastfeeding parents may mean by privacy, and the material-factors, such as a separate room, cubicle, curtain or lockable-door, that populate the various individual regions of concern. Furthermore, we empirically show, that these concerns are systematically different depending on the contextual factors of deprivation in the area as captured by the IMD. As such, this paper contributes a reflexive process in which objective interpretation of data which preserves multiple subjectivities (as afforded through topic models) is positioned in dialogue with a more qualitative reading of the data, and through which user-generated data is used to ask questions of official data and vice versa.
"
Design Opportunities for AAC and Children with Severe Speech and Physical Impairments,https://dl.acm.org/authorize?N657939,"
Augmentative and alternative communication (AAC) technologies can support children with severe speech and physical impairments (SSPI) to express themselves. Yet, these seemingly 'enabling' technologies are often abandoned by this target group, suggesting a need to understand how they are used in communication. Little research has considered the interaction between people, interaction design and the material dimension of AAC. To address this, we report on a qualitative video study that examines the situated communication of five children using AAC in a special school. Our findings offer a new perspective on reconceptualising AAC design and use revealing four areas for future design: (1) incorporating an embodied view of communication, (2) designing to emphasise children's competence and agency, (3) regulating the presence, prominence and value of AAC, and (4) supporting a wider range of communicative functions that help address children's needs. This paper reported a qualitative 14-week field study at a special school. Our research aim was to examine how communication manifests in five children with SSPI who use AAC in school, and the mediating role of AAC design. Videos of communication incidents involving children and technology were collected. Inductive video analysis was then carried out applying a multimodal and social semiotic approach. Our analysis approached communication from three lenses: children’s choice of modes (a child view), their interactions with each other and technology (an interactional view), and the ordering of people and technology (a structural view). This enabled us to identify the kinds of communication achieved through and around AAC and to unpack how the design of AAC impacts on this communication. Our paper contributes to the field of interaction design and AAC research with four design opportunities: incorporating an embodied view of communication; designing to emphasise children’s competence and agency, regulating the presence, prominence and value of AAC, and; supporting children in maintaining self-initiated communication. One methodological limitations we faced was the limited capture of repeated incidents of AAC-mediated communication in naturally occurring interactions. Additional research is needed from more diverse contexts to enrich our findings, in line with what Stebbins calls concatenation i.e. incremental development of theory [39]. This could include considering the role of conversation partners and the impact of varied language displays on communication. Our study should not be interpreted as providing clear solutions to this complex problem space, but rather identifying new avenues for a future design agenda that brings interaction designers closer to the concerns of young children with SSPI who use technology. In particular, we hope that future design work will move beyond the transmission of information framing of technology to design for situated, embodied and coconstructed communication.
"
Facebook in Venezuela: Understanding Solidarity Economies in Low-Trust Environments,https://dl.acm.org/authorize?N657930,"
Since 2014, Venezuela has experienced severe economic crisis, including scarcity of basic necessities such as food and medicine. This has resulted in over-priced goods, scams, and other forms of economic abuse. We present an investigation of Venezuelans' efforts to form an alternative, Solidarity Economy (SE) through Facebook Groups. In these groups, individuals can barter for items at fair prices. We highlight group practices and design features of Facebook Groups which support solidarity or anti-solidarity behaviors. We conclude by leveraging design principles for online communities presented by Kollock to present strategies to design more effective SEs in environments of low trust. We have presented a review of Venezuelan Facebook bartering groups’ efforts to create an alternative, SE to counteract an economy encumbered by bachaqueros and low trust. In doing so, we have highlighted areas of Facebook Groups which support solidarity and anti-solidarity behaviors, and in leveraging Kollock we present strategies which can be implemented to better support the SE movement in Venezuela and other low-trust environments in the future. When Mark Zuckerberg first created a social site for college students at elite universities in 2004, he could not have anticipated that just over a decade later, his platform would play a significant role in basic survival for individuals in a prolonged economic crisis thousands of miles away in South America. It is a tribute to human ingenuity and resilience that software tools can be re-purposed in this way. Analysis of the strengths and weaknesses of Facebook Groups for the purpose of bartering in extreme scarcity suggests a host of simple modifications that could make them work significantly better. There may be a viable economic model to motivate this software development, if Facebook wishes to compete with marketplace-oriented sites, or it could be undertaken as a humanitarian effort. More broadly, providing users with more powerful, general-purpose, customizable tools can help empower people–in this crisis and crises of the future–to craft solutions for themselves.
"
Digital Payment and Its Discontents: Street Shops and the Indian Government's Push for Cashless Transactions,https://dl.acm.org/authorize?N657931,"
In November 2016, the Government of India banned the vast majority of the nation's banknotes in a move referred to as 'demonetization', with the stated goals of fighting corruption, terrorism, and eventually expanding digital transactions. In this study of 200 shop-keepers in Mumbai and Bengaluru, we found that cash shortage increased digital payment adoption but that digital payments fell after new banknotes became available. Digital payment adoption depended on the nature and scope of transactions, type of product sold, as well as personal factors specific to business owners such as comfort and familiarity with other digital technologies and online transactions. Using theoretical work on market and information behavior, we examined environmental pushes for technology adoption against prevalent transactional practices, trust, and control. We propose that the move toward digital payments must be framed within a larger undertaking of technology-driven modernity that drives these initiatives, rather than just the efficiency or productivity gains digital payments present. Our analysis of how shopkeepers coped with demonetization gives descriptive insight into the nature of technology adoption in state-mandated decisions. We found that digital payment adoption increased immediately following the crisis but fell after new banknotes became available. The participants in our study who continued to use digital payments did so based on the nature and scope of transactions, type of product they sold, and personal factors such as comfort and familiarity with other digital technologies and transactions. We found that when technology adoption pushes against existing practices it is resisted; such adoption should be examined within the larger context in which it is taking place because it is not solely about the user experience or platform. The work on HCI4D has long looked at technology adoption either from the perspective of people’s needs and abilities or from that of socio-economic drivers. This research shows how a political agenda, enacted through a technological intervention, can be a lens into people’s acceptance or rejection of artifacts. We found that markets and entrenched practices are important considerations and that networks have effects on whether people try new things. We found that existing comfort with digital technology is an important indicator of new technology adoption, which further suggests that the information-poor and those with limited access to technology are less likely to be users. This, in turn, raises questions about the idea of technology and modernity and the scope of new technologies to change lives. The critical failing of the Modi government was that while it made a number of sentimental appeals for technology adoption, it provided little compelling evidence that the cash economy was bad for people. On the contrary, respondents suggested the reverse was true.
"
Moving Target Selection: A Cue Integration Model,https://dl.acm.org/authorize?N657932,"
This paper investigates a common task requiring temporal precision: the selection of a rapidly moving target on display by invoking an input event when it is within some selection window. Previous work has explored the relationship between accuracy and precision in this task, but the role of visual cues available to users has remained unexplained. To expand modeling of timing performance to multimodal settings, common in gaming and music, our model builds on the principle of probabilistic cue integration. Maximum likelihood estimation (MLE) is used to model how different types of cues are integrated into a reliable estimate of the temporal task. The model deals with temporal structure (repetition, rhythm) and the perceivable movement of the target on display. It accurately predicts error rate in a range of realistic tasks. Applications include the optimization of difficulty in game-level design. This paper has presented a theory and mathematical model for predicting error rates in moving target selection tasks. The model is derived from a known cognitive theory called cue integration theory. To explain how users form an integrated percept of a rapidly moving target, the theory builds on a probabilistic account of sensorimotor performance. It estimates an upper bound of performance achievable by a perfect decoder. Across three studies, we have shown that this optimality assumption fits empirical data of trained users surprisingly well. This makes the model potentially useful in the design of gaming and music applications, where it can be used to design critical parameters like speeds, selection regions, and viewing times of moving targets.
"
Inferring Loop Invariants through Gamification,https://dl.acm.org/authorize?N657933,"
In today's modern world, bugs in software systems incur significant costs. One promising approach to improve software quality is automated software verification. In this approach, an automated tool tries to prove the software correct once and for all. Although significant progress has been made in this direction, there are still many cases where automated tools fail. We focus specifically on one aspect of software verification that has been notoriously hard to automate: inferring loop invariants that are strong enough to enable verification. In this paper, we propose a solution to this problem through gamification and crowdsourcing. In particular, we present a puzzle game where players find loop invariants without being aware of it, and without requiring any expertise on software verification. We show through an experiment with Mechanical Turk users that players enjoy the game, and are able to solve verification tasks that automated state-of-the-art tools cannot. We presented INVGAME, a game in which players guess loop invariants. We have shown that INVGAME allows players with no verification knowledge to verify benchmarks that leading automated tools cannot. We believe our minimalist design was one of the key factors contributing to its success: INVGAME directly leverages raw human mathematical skills. Our promising results lay the path for a larger exploration into program verification through gamification and crowdsourcing.

"
CrowdLayout: Crowdsourced Design and Evaluation of Biological Network Visualizations,https://dl.acm.org/authorize?N657934,"
Biologists often perform experiments whose results generate large quantities of data, such as interactions between molecules in a cell, that are best represented as networks (graphs). To visualize these networks and communicate them in publications, biologists must manually position the nodes and edges of each network to reflect their real-world physical structure. This process does not scale well, and graph layout algorithms lack the biological underpinnings to offer a viable alternative. In this paper, we present CrowdLayout, a crowdsourcing system that leverages human intelligence and creativity to design layouts of biological network visualizations. CrowdLayout provides design guidelines, abstractions, and editing tools to help novice workers perform like experts. We evaluated CrowdLayout in two experiments with paid crowd workers and real biological network data, finding that crowds could both create and evaluate meaningful, high-quality layouts. We also discuss implications for crowdsourced design and network visualizations in other domains. Network data is widely used in biological research, but creating meaningful visualizations of these networks is challenging for algorithms and typically requires significant time and expertise for biologists to perform manually. We presented CrowdLayout, a system that leverages crowdsourced human intelligence and creativity to design layouts of biological network visualizations. CrowdLayout uses design guidelines, abstractions, and special editing tools to help crowd workers without biological expertise create layouts that mimic real-world biological structures. The guidelines also serve as evaluation criteria for crowds to assess the quality of their layouts. In two experiments, we showed that crowds could design layouts that satisfy these guidelines as effectively as expert biologists, and crowds could also provide reviews of layouts that were similar to an expert. Consequently, CrowdLayout enables biological network visualization at scale, giving scientists access to meaningful visualizations of many more networks than they would be able to create manually, and potentially supporting new scientific breakthroughs.
"
Imaginary Design Workbooks: Constructive Criticism and Practical Provocation,https://dl.acm.org/authorize?N657945,"
his paper reports on design strategies for critical and experimental work that remains constructive. We report findings from a design workshop that explored the ""home hub"" space through ""imaginary design workbooks"". These feature ambiguous images and annotations written in an invented language to suggest a design space without specifying any particular idea. Many of the concepts and narratives which emerged from the workshop focused on extreme situations: some thoughtful, some dystopian, some even mythic. One of the workshop ideas was then developed with a senior social worker who works with young offenders. A ""digital social worker"" concept was developed and critiqued simultaneously. We draw on Foucault's history of surveillance to ""defamiliarise"" both the home hub technology and the current youth justice system. We argue that the dichotomy between ""constructive"" and ""critical"" design is false because design is never neutral. This paper has argued that the dichotomy between constructive and critical thinking is false. In order to be constructive thinking must be critical. Home technologies that record the minutia of our lives are already with us. The question is not should these data be collected but how should they be used and regulated. In these kinds of “post privacy” spaces design must engage with political, ethical and legal issues. Indeed, it already does, the question is whether designers are prepared to acknowledge this and take responsibility or not.
"
Drunk User Interfaces: Determining Blood Alcohol Level through Everyday Smartphone Tasks,https://dl.acm.org/authorize?N657946,"
Breathalyzers, the standard quantitative method for assessing inebriation, are primarily owned by law enforcement and used only after a potentially inebriated individual is caught driving. However, not everyone has access to such specialized hardware. We present drunk user interfaces: smartphone user interfaces that measure how alcohol affects a person's motor coordination and cognition using performance metrics and sensor data. We examine five drunk user interfaces and combine them to form the ""DUI app"". DUI uses machine learning models trained on human performance metrics and sensor data to estimate a person's blood alcohol level (BAL). We evaluated DUI on 14 individuals in a week-long longitudinal study wherein each participant used DUI at various BALs. We found that with a global model that accounts for user-specific learning, DUI can estimate a person's BAL with an absolute mean error of 0.005% ± 0.007% and a Pearson's correlation coefficient of 0.96 with breathalyzer measurements. Incidents involving inebriation often occur because they happen before an intervention can take place, highlighting the need for a blood alcohol level (BAL) system more ubiquitous than a breathalyzer. We have introduced drunk user interfaces (DUIs), smartphone tasks that use sensing and human performance metrics to estimate a person’s BAL. The combination of different DUIs led us to create the DUI app, which utilizes machine learning on human performance and sensor data features to attain a more complete snapshot of the user’s current state. To evaluate DUI, we conducted a rigorous longitudinal study in which participants used the app at different ground-truth BALs. We trained models that accounted for learning and found that DUI was able to estimate BAL with an absolute mean error of 0.005% ± 0.007%. It is our hope that other researchers will recreate our study or improve upon it with their own tasks so that ubiquitous inebriation assessment might become a reality.
"
“That Really Pushes My Buttons”: Designing Bullying and Harassment Training for the Workplace,https://dl.acm.org/authorize?N657947,"
Workplace bullying and harassment have been identified as two of the most concerning silent and unseen occupational hazards of the 21st century. The design of bespoke training addressing domain-specific job roles and relations presents a particular challenge. Using the concept of data-in-place where data is understood as being bound and produced by a particular place, this paper describes how locally-situated accounts can be used to engage employees in workplace-specific training seminars. Using higher education as a case study, we describe a four-stage design process for future training efforts: (1) in-depth interviews for further understanding of bullying and harassment; (2) design of digital probes for capturing contextual data; (3) probe deployment and subsequent data analysis; (4) data-driven discussion-based seminars. We outline the potential for digital probes in promoting the denormalization of toxic workplace cultures, considerations for novel sensitive data governance models, and the discussion of data-in-place's temporal dimension. In the context of workplace training, e-learning courses have been seen as the virtual panacea for addressing the serious psychosocial hazards of bullying and harassment in higher education. Through a series of interviews and digital probe deployments, this paper explored the possibility for a tailored employee training seminar design. Using the concept of data-in-place, we have demonstrated the nuance and sensitivity to its environment through survivor-led approaches and sensitive data governance with considerations for future use of digital probes in the denormalization of toxic workplace cultures.
"
Taking into Account Sensory Knowledge: The Case of Geo-techologies for Children with Visual Impairments,https://dl.acm.org/authorize?N657948,"
This paper argues for designing geo-technologies supporting non-visual sensory knowledge. Sensory knowledge refers to the implicit and explicit knowledge guiding our uses of our senses to understand the world. To support our argument, we build on an 18 months field-study on geography classes for primary school children with visual impairments. Our findings show (1) a paradox in the use of non-visual sensory knowledge: described as fundamental to the geography curriculum, it is mostly kept out of school; (2) that accessible geo-technologies in the literature mainly focus on substituting vision with another modality, rather than enabling teachers to build on children's experiences; (3) the importance of the hearing sense in learning about space. We then introduce a probe, a wrist-worn device enabling children to record audio cues during field-trips. By giving importance to children's hearing skills, it modified existing practices and actors' opinions on non-visual sensory knowledge. We conclude by reflecting on design implications, and the role of technologies in valuing diverse ways of understanding the world. In this paper, we argue for using the sensory turn as a lens to examine the learning experiences of children with visual impairments. We described the rich practices of teachers to teach children how to use their senses to understand their surroundings and construct geographic knowledge. However, teachers and children were conflicted about the value of this sensory knowledge, which we call the paradox of using nonvisual knowledge in the classroom. We designed a probe enabling children to make and play audio recordings during these field-trips, recordings that could then be displayed on an interactive map or other supports. Through observations, we show changes in practices, and particularly increased agency for children. Through follow-up interviews, we demonstrate a change in discourses, hinting at a re-evaluation of the auditory sensory knowledge of space. Which confirms our initial stance: designing for a diversity of ways of learning and knowing contributes to enable the expression of marginalized views.
"
Understanding the Uncertainty in 1D Unidirectional Moving Target Selection,https://dl.acm.org/authorize?N657949,"
In contrast to the extensive studies on static target pointing, much less formal understanding of moving target acquisition can be found in the HCI literature. We designed a set of experiments to identify regularities in 1D unidirectional moving target selection, and found a Ternary-Gaussian model to be descriptive of the endpoint distribution in such tasks. The shape of the distribution as characterized by μ and σ in the Gaussian model were primarily determined by the speed and size of the moving target. The model fits the empirical data well with 0.95 and 0.94 R2 values for μ and σ , respectively. We also demonstrated two extensions of the model, including 1) predicting error rates in moving target selection; and 2) a novel interaction technique to implicitly aid moving target selection. By applying them in a game interface design, we observed good performances in both predicting error rates (e.g., 2.7% mean absolute error) and assisting moving target selection (e.g., 33% or a greater increase in pointing accuracy). In this paper, we reported our work in understanding and modeling the endpoint distribution in 1D unidirectional moving targeting selection. We proposed a TernaryGaussian model to interpret the distribution of the endpoints for targets moving in two horizontal directions. Results show that our model fit the data very well. We also demonstrated how our model can be used to predict error rates and assist selection of moving targets. We observed good performance when applying these two means in a game interface design. As one of the first attempts to model human behavior uncertainty in moving target selection, our work provides the HCI community with a theoretical foundation and empirical evidence for future research and design in such scenarios. For instance, when designing the difficulty curve of a game which the main challenge is to select moving targets, our model can be used to predict error and guide game designers to modify the game parameters such as healthy points, size and speed of the enemies. Our model is not limited to gaming, in other animation systems (e.g., simulation systems, video monitoring system), our model can also be used to aid target selection on these systems in an implicit manner. In the future, we are interested in extending our research in several directions. First, we will examine whether our model can be transferred into user interfaces with interaction styles other than mouse (e.g., touch-based, pen-based, gesturebased). Second, we will also pursue modeling uncertainty in selecting moving targets with changing speed and in 2D/3D space. Furthermore, we will explore how our model can help to improve interaction efficiency in general user interfaces and compare it with other state-of-the-art pointing techniques (e.g., Comet [11, 13] or Bubble Cursor [10]).
"
Agile 3D Sketching with Air Scaffolding,https://dl.acm.org/authorize?N657940,"
Hand motion and pen drawing can be intuitive and expressive inputs for professional digital 3D authoring. However, their inherent limitations have hampered wider adoption. 3D sketching using hand motion is rapid but rough, and 3D sketching using pen drawing is delicate but tedious. Our new 3D sketching workflow combines these two in a complementary manner. The user makes quick hand motions in the air to generate approximate 3D shapes, and uses them as scaffolds on which to add details via pen-based 3D sketching on a tablet device. Our air scaffolding technique and corresponding algorithm extract only the intended shapes from unconstrained hand motions. Then, the user sketches 3D ideas by defining sketching planes on these scaffolds while appending new scaffolds, as needed. A user study shows that our progressive and iterative workflow enables more agile 3D sketching compared to ones using either hand motion or pen drawing alone. Various computer graphics and 3D interaction techniques strive for more intuitive and expressive 3D modeling in design processes. In this study, we focused on the strengths and weaknesses of two salient input modalities and sought to develop a workflow in which the strengths of each are enforced and the weaknesses are complemented. Our contribution is in recognizing that 3D sketching has no technique analogous to 2D scaffolding in 2D sketching. 3D authoring utilizing hand motion was used to roughly and quickly generate the desired scale and volume, and penbased 3D sketching was used to delicately specify finer features. From the extensive user study, we found that it is possible to design an interaction technique that coherently integrates different input modalities to enable rapid and highfidelity 3D conceptualization in a progressive and iterative workflow that the designers could satisfactorily use in practice.

"
KeyTime: Super-Accurate Prediction of Stroke Gesture Production Times,https://dl.acm.org/authorize?N657941,"
We introduce KeyTime, a new technique and accompanying software for predicting the production times of users' stroke gestures articulated on touchscreens. KeyTime employs the principles and concepts of the Kinematic Theory, such as lognormal modeling of stroke gestures' velocity profiles, to estimate gesture production times significantly more accurately than existing approaches. Our experimental results obtained on several public datasets show that KeyTime predicts user-independent production times that correlate r=.99 with groundtruth from just one example of a gesture articulation, while delivering an average error in the predicted time magnitude that is 3 to 6 times smaller than that delivered by CLC, the best prediction technique up to date. Moreover, KeyTime reports a wide range of useful statistics, such as the trimmed mean, median, standard deviation, and confidence intervals, providing practitioners with unprecedented levels of accuracy and sophistication to characterize their users' a priori time performance with stroke gesture input. KeyTime is a new high-performing technique informed by the Kinematic Theory that delivers very accurate predictions of users’ stroke gesture production times. Through careful evaluations, we showed that KeyTime’s predictions are very close to the actual production times of stroke gestures articulated by real users. Moreover, KeyTime only requires one gesture example that designers can produce themselves, and is readily available to any practitioner both as an online application and a RESTful JSON API. KeyTime also raises the bar for future research on stroke gesture time prediction and analysis by delivering a wide palette of predictors of location and dispersion for production times. It is our hope that KeyTime will provide researchers, designers, and practitioners with unprecedented levels of accuracy and sophistication to characterize their users’ a priori time performance with stroke gesture input, informing better gesture user interface designs.

"
Entrepreneurship and the Socio-Technical Chasm in a Lean Economy,https://dl.acm.org/authorize?N657942,"
Online technologies are increasingly hailed as enablers of entrepreneurship and income generation. Recent evidence suggests, however, that even the best such tools disproportionately favor those with pre-existing entrepreneurial advantages. Despite intentions, the technology on its own seems far from addressing socio-economic inequalities. Using participatory action research, we investigated why this might be, in an intimate, close-up context. Over a 1-year period, we--a collaborative team of university researchers and residents of Detroit's East Side--worked to establish a neighborhood tour whose initial goal was to raise supplementary income and fundraise for community block clubs. We found that in addition to technical requirements, such as communication tools, a range of non-technological efforts is needed to manage projects, build self-efficacy, and otherwise support community participants. Our findings widen Ackerman's ""socio-technical gap"" for some contexts and offer a counterpoint to overgeneralized claims about well-designed technologies being able to address certain classes of social challenges. This study (1) confirmed past technical requirements such as the need for tools to obtain financial and social capital; (2) identified new technical requirements such as the need for tools to support incubation (e.g., set price points) and support human capital (e.g., instill a sense of independence, foster self-efficacy). This study additionally (3) uncovered hidden, non-technical requirements such as the need for support during personal challenges; social support and direct interaction to build trust for entrepreneurial success among novice entrepreneurs in lean economies. We found issues around trust and motivation to be particularly challenging for technology to address, at least for the foreseeable future. And while dramatic new technologies—such as robots indistinguishable from people—might prompt a reassessment, our conclusion for the moment is that supporting people in lean economies toward entrepreneurship and other forms of social-economic growth requires resources applied toward human interventions, rather than the pursuit of tempting technological mirages. These results deepen our understanding of Ackerman’s social-technical gap, and contribute implications for both civic technologists and policy-makers hoping to support entrepreneurship in lean economies.
"
VirtualSpace - Overloading Physical Space with Multiple Virtual Reality Users,https://dl.acm.org/authorize?N657943,"
Although virtual reality hardware is now widely available, the uptake of real walking is hindered by the fact that it requires often impractically large amounts of physical space. To address this, we present VirtualSpace, a novel system that allows overloading multiple users immersed in different VR experiences into the same physical space. VirtualSpace accomplishes this by containing each user in a subset of the physical space at all times, which we call tiles; app-invoked maneuvers then shuffle tiles and users across the entire physical space. This allows apps to move their users to where their narrative requires them to be while hiding from users that they are confined to a tile. We show how this enables VirtualSpace to pack four users into 16m2. In our study we found that VirtualSpace allowed participants to use more space and to feel less confined than in a control condition with static, pre-allocated space. We presented VirtualSpace, a technique for overloading multiple real walking VR users into the same physical space. VirtualSpace achieves this by containing each app in a smaller tile. Frequent “maneuvers” allow apps to incentivize their users to walk across the entire physical space, thereby allowing each app to progress its narrative and to prevent users from noticing that they are confined to a tile. This strategy enables VirtualSpace to achieve packings of the unprecedented density of four users in 16m2 as we demonstrated in our user study.

"
Effects of Socially Stigmatized Crowdfunding Campaigns in Shaping Opinions,https://dl.acm.org/authorize?N657944,"
Donation-based crowdfunding platforms have an increasing number of campaigns on socially stigmatized topics. These platforms' widespread online reachability and the large flow of monetary donations have the potential to shape individuals' opinions by influencing their perceptions. However, little research has been done to investigate whether these campaigns impact individuals' opinions and how. We conducted an experiment to explore how an attitude-inconsistent campaign on fairness and equality for LGBTIQ people influenced participants' opinion on this topic. Although all the participants changed their perceived opinions after reading the support for the campaigns, participants opposing equality were less inclined to change their attitude than participants supporting equality. To examine this difference further, we conducted another experiment where participants were exposed to both attitude-consistent and attitude-inconsistent campaigns with varying levels of social support. Participants opposing equality showed less sensitivity to the level of social support, and wanted to donate significantly more money to anti-equality campaigns compared to those who supported equality. Results demonstrate the complex role of crowdfunding campaigns in shaping individuals' opinions on stigmatized topics. In conclusion, we, as researchers, are not in a position to support the point of view of any group over the other. Rather, we believe that our work will contribute to the large body of literature on social polarization and selective exposure by revealing the complex dynamics of information on stigmatized topics and the pre-existing attitudes of people. Given the potential impact of crowdfunding platforms on social opinions, we need more research to fully understand how various online media are contributing to shaping the opinions of the people on stigmatized topics, and how this will affect the aggregated social opinion of a community. We believe that our findings provide an important starting point for the research community to increase their awareness of the complex influence of crowdfunding campaigns on shaping social opinions on stigmatized social issues.

"
A Design Framework for Awareness Cues in Distributed Multiplayer Games,https://dl.acm.org/authorize?N657055,"
In the physical world, teammates develop situation awareness about each other's location, status, and actions through cues such as gaze direction and ambient noise. To support situation awareness, distributed multiplayer games provide awareness cues - information that games automatically make available to players to support cooperative gameplay. The design of awareness cues can be extremely complex, impacting how players experience games and work with teammates. Despite the importance of awareness cues, designers have little beyond experiential knowledge to guide their design. In this work, we describe a design framework for awareness cues, providing insight into what information they provide, how they communicate this information, and how design choices can impact play experience. Our research, based on a grounded theory analysis of current games, is the first to provide a characterization of awareness cues, providing a palette for game designers to improve design practice and a starting point for deeper research into collaborative play. We see this paper as setting out a research agenda for awareness cues in distributed multiplayer games, improving the relevance of our work to both designers and researchers. First, existing team awareness cues build on common design patterns, both within a particular game genre and across different genres. From this basic language for awareness cues, we can begin to describe the existing set of design patterns for awareness cues that already exist. Game design patterns are useful to facilitate and analyze designs [3], and can also be used to identify negative space in a design space: places where patterns ought to exist but do not. Game design patterns can therefore improve existing distributed awareness cues as well as driving discovery of new patterns and designs. We can use these patterns to analyze, improve, and push forward specific aspects of games. One clear axis is looking at different categories of games—either looking at specific genres such as MOBAs, or looking at games that share common features, such as turn-based play. Another direction is to compare different categories of players, for example understanding what awareness cues are associated with different player roles or how they are deployed differently for experts and novices. Finally, we can look at different types of information, such as how nominal versus continuous information is deployed. Second, we can extend our framework beyond team awareness. When players make decisions, they need to understand their teammates, but, their decisions also incorporate information about themselves, their adversaries, the environment, nonplayer characters (NPCs) and the game system. Finally, we expect to bring this work full circle, by applying these insights in other types of distributed workspaces. Specifically, we expect fast-response organizations, like disaster responders, to benefit from this work. Further, we expect other types of groupware (e.g., distributed document editing) can use our work to identify new awareness cues. The present research contributes to our knowledge of how team members work together, and how teamwork can be supported. We expect our framework to be of value to game designers and researchers, and hope that, through this work, we discover new ways to support team coordination.
"
Empowerment in HCI - A Survey and Framework,https://dl.acm.org/authorize?N657056,"
Empowering people through technology is of increasing concern in the HCI community. However, there are different interpretations of empowerment, which diverge substantially. The same term thus describes an entire spectrum of research endeavours and goals. This conceptual unclarity hinders the development of a meaningful discourse and exchange. To better understand what empowerment means in our community, we reviewed 54 CHI full papers using the terms empower and empowerment. Based on our analysis and informed by prior writings on power and empowerment, we construct a framework that serves as a lens to analyze notions of empowerment in current HCI research. Finally, we discuss the implications of these notions of empowerment on approaches to technology design and offer recommendations for future work. With this analysis, we hope to add structure and terminological clarity to this growing and important facet of HCI research. In this work, we reviewed papers on empowerment in HCI, constructed a framework grounded in work on power and empowerment, and described different lines of research prevalent in our community. Our analysis revealed that notions on power and empowerment differ greatly between eight lines of research. To inspire a more structured discussion about the concept of empowerment, we provided several recommendations which we hope may guide this dialogue. We strongly believe that the turn towards empowerment in HCI has the potential to influence technology design for the better. However, to utilize this potential in a responsible way, in each individual case, it 1) requires a definition of the underlying maxim and goal of empowerment and 2) an evaluation to what extent the envisioned goal has been reached. Unchallenged calls can entail the risk of becoming a mantra and lead to ineffective (and potentially detrimental) design choices. Appropriate metrics are required to prevent such negative effects and ensure the effectiveness of the chosen form of empowerment. They are also needed as a foundation for design guides and best practices and to differentiate between effective and ineffective approaches of empowerment. In order to develop such metrics, a clear understanding of empowerment is needed: Who is the target group, which are the targeted psychological components, do we aim for an increase in power or a decrease in power difference? In Zimmerman’s words, “the development of a universal and global measure of empowerment is not an appropriate goal because it may not mean the same thing for every person, organization, or community everywhere.” [100]. Our framework provides starting points for finding these metrics. But more than that, we hope to have highlighted the need to consciously reflect on the notions of power and empowerment researchers want to adopt for their project.

"
Multiray: Multi-Finger Raycasting for Large Displays,https://dl.acm.org/authorize?N657057,"
We explore and evaluate a multi-finger raycasting design space that we call ""multiray"". Each finger projects a ray on to the display, so the user is interacting from a distance using a form of direct input. Specifically, we propose techniques, where patterns of ray intersections created by hand postures form 2D geometric shapes to trigger actions and perform direct manipulations that go beyond single-point selections. Two formative studies examine characteristics of multi-finger raycasting for different projection methods, shapes, and tasks. Based on the results of those investigations, we demonstrate a number of dynamic UI controls and operations that utilise multiray points and shapes. We presented multiray, or multi-finger raycasting to interact remotely with large displays. As motivated in the introduction, we believe multiray fills a hole in the raycasting design space similar to multitouch filling a hole relative to single touch. While we do not mean to also imply multiray is automatically a significant intuitive leap like multitouch, we hope our work shows that people can use multiray to increase expressivity compared to single ray. We conducted two formative experiments to examine various properties of the technique. Our main findings are: 1) Raycasting from the entire finger (MCP to tip) is more efficient than from just the distal phalanx (DIP to tip). 2) There are signs that single-point targeting with multiple fingers can compensate for noise, but our results were not statistically significant. 3) Horizontal shapes are faster and preferred by people. 4) Short corners and lines are the most efficient shapes. 5) Circle and rectangles are generally less efficient but they also have a high variability among users. 6) Circles are easiest to maintain during motion. 7) People can be initially troubled by the fact that shapes produced on the screen do not always match the physical postures of their hand. Based on those experiments, we created three applications demonstrating several multiray interactions for common operations. Our work paves the way towards future evaluations in context, with more shapes and possibly in coexistence with other non-ray gestures to address the shortcomings that we identified. A further avenue to explore would be to introduce biases towards particular shapes (thus departing from absolute raycasting) so as to facilitate their detection and the correspondence with the physical posture of the hand. Finally, as with any mid-air gesture or single-finger raycasting technique, fatigue is an important aspect that needs to be taken into consideration and rigorously assessed.

"
Feel the Movement: Real Motion Influences Responses to Take-over Requests in Highly Automated Vehicles,https://dl.acm.org/authorize?N657058,"
Take-over requests (TORs) in highly automated vehicles are cues that prompt users to resume control. TORs however, are often evaluated in non-moving driving simulators. This ignores the role of motion, an important source of information for users who have their eyes off the road while engaged in non-driving related tasks. We ran a user study in a moving-base driving simulator to investigate the effect of motion on TOR responses. We found that with motion, user responses to TORs vary depending on the road context where TORs are issued. While previous work showed that participants are fast to respond to urgent cues, we show that this is true only when TORs are presented on straight roads. Urgent cues issued on curved roads elicit slower responses than non-urgent cues on curved roads. Our findings indicate that TORs should be designed to be aware of road context to accommodate natural user responses. In this paper, we investigated whether real motion inuenced users’ responses to TORs. This was motivated by the fact that many evaluations and user studies of TORs tend to be performed in xed-base simulators, which might grossly underestimate situational awareness of the driving scene during non-driving. Indeed, we found that users responded faster and demonstrated better take-over control when motion was available. While they were generally faster in responding to TORs that were designed to be more urgent (cf., [3, 27, 41]), this was not the full story. When real motion was provided, users took more time to respond to urgent TORs that were presented in curved road segments. This highlights the fact that users are sensitive to kinesthetic information from the driving scene, even if they are visually engaged with a non-driving activity. Thus, TORs should be designed to take this into account and to support decision-making post TOR presentation, given the assumption that users of highly automated vehicles are likely to be aware of road conditions such as road curvature. Finally, we provide some recommendations for providing continuous cueing of the driving scene besides motion that might similarly support situational awareness without interfering with non-driving activity.

"
HomeFinder Revisited: Finding Ideal Homes with Reachability-Centric Multi-Criteria Decision Making,https://dl.acm.org/authorize?N657059,"
Finding an ideal home is a difficult and laborious process. One of the most crucial factors in this process is the reachability between the home location and the concerned points of interest, such as places of work and recreational facilities. However, such importance is unrecognized in the extant real estate systems. By characterizing user requirements and analytical tasks in the context of finding ideal homes, we designed ReACH, a novel visual analytics system that assists people in finding, evaluating, and choosing a home based on multiple criteria, including reachability. In addition, we developed an improved data-driven model for approximating reachability with massive taxi trajectories. This model enables users to interactively integrate their knowledge and preferences to make judicious and informed decisions. We show the improvements in our model by comparing the theoretical complexities with the prior study and demonstrate the usability and effectiveness of the proposed system with task-based evaluation. This study characterizes the problem of reachability-centric multi-criteria decision-making for choosing ideal homes. To assist users in expressing their preferences, we introduce several reachability concepts involved in this problem, including activities and constraints. We propose a new graph-based mining model that significantly extends the state-of-the-art method [39] to achieve real-time performance. We used the model as basis to design and develop a novel data-driven system called ReACH. To the best of our knowledge, ReACH is the first interactive visualization system for people to find an ideal home based on massive urban data. The proposed system has been deployed on a local workstation. In the future, we intend to deploy this system as a cloud service on the web. The mining model will be improved to reduce the space complexity, thereby lowering the cost for the cloud deployment. Other types of data, which are of interest to people, will be incorporated to the proposed system.

"
SpaceTokens: Interactive Map Widgets for Location-centric Interactions,https://dl.acm.org/authorize?N657050,"
Map users often need to interact repetitively with multiple important locations. For example, a traveler may frequently check her hotel or a train station on a map, use them to localize an unknown location, or investigate routes involving them. Ironically, these location-centric tasks cannot be performed using locations directly; users must instead pan and zoom the map or use a menu to access locations. We propose SpaceTokens, interactive widgets that act as clones of locations, and which users can create and place on map edges like virtual whiteboard magnets. SpaceTokens make location a first-class citizen of map interaction. They empower users to rapidly perform location-centric tasks directly using locations: users can select combinations of on-screen locations and SpaceTokens to control the map window, or connect them to create routes. Participants in a study overwhelmingly preferred a SpaceTokens prototype over Google Maps on identical smartphones for the majority of tasks. We have asked the question, “Why can’t map users perform location-centric tasks directly using locations?” To answer it, we introduced SpaceTokens, interactive widgets that make location a first-class citizen of map interaction. SpaceTokens empower users to perform location-centric tasks rapidly. We explored using locations as constraints to address the seeing problem, and using locations as building blocks to address the linking problem. Building on the idea of making location a first-class citizen, we also presented SpaceBar, a scrollbar-like navigational instrument that leverages decades of work on scrollbars to help users interact with a route. We implemented SpaceTokens and related tools in a prototype iOS application. In a qualitative user study, we evaluated the prototype application on an iPhone in comparison with Google Maps. Participants overwhelmingly preferred the approaches used in our prototype for the majority of the tasks they tried. Ultimately, we envision spatial entities in general (locations, paths, areas) could be first-class citizens of map interaction.

"
M3 Gesture Menu: Design and Experimental Analyses of Marking Menus for Touchscreen Mobile Interaction,https://dl.acm.org/authorize?N657051,"
Despite their learning advantages in theory, marking menus have faced adoption challenges in practice, even on today's touchscreen-based mobile devices. We address these challenges by designing, implementing, and evaluating multiple versions of M3 Gesture Menu (M3), a reimagination of marking menus targeted at mobile interfaces. M3 is defined on a grid rather than in a radial space, relies on gestural shapes rather than directional marks, and has constant and stationary space use. Our first controlled experiment on expert performance showed M3 was faster and less error-prone by a factor of two than traditional marking menus. A second experiment on learning demonstrated for the first time that users could successfully transition to recall-based execution of a dozen commands after three ten-minute practice sessions with both M3 and Multi-Stroke Marking Menu. Together, M3, with its demonstrated resolution, learning, and space use benefits, contributes to the design and understanding of menu selection in the mobile-first era of end-user computing. In this paper, we proposed M3 Gesture Menu, a variant of the marking menu that is defined on grids rather than radial space, relies on gestural shapes rather than directional marks, and has constant and stationary space use so as to fit the mobile form factor. In addition, gestures in M3 are more robustly recognized using a shape matching algorithm than aggregating stroke direction results as in conventional marking menus. Two experiments were conducted to understand M3’s menu resolution and novice to expert transition behavior. Compared to the literature, M3 exhibited much higher resolution than traditional Marking Menu and the revised Multi-Stroke Marking Menu. We estimated that M3 was at least twice as fast and twice more accurate than traditional Marking Menu and at least twice as fast as Multi-Stroke Marking Menu at a comparable accuracy. Furthermore, M3 was designed for mobile phone’s relatively small screen and tested with the finger rather than the more precise stylus. Because gestures are unistrokes, M3 does not need a dedicated, occlusive space to disambiguate the menu interaction from other overlapping graphical elements as Multi-Stroke Marking Menu does. We also for the first time, to our knowledge, systematically examined and demonstrated empirically successful user behavior transition from novice to expert mode. Users of M3 and Multi-Stroke Marking Menu were able to switch from visually-guided use to recall-based gesture articulation for a dozen commands after a total of 30-minute practice over three days. Subjectively, most but not all users preferred M3 over other alternatives. They found M3 to provide more fluid interaction flow, facilitate memorization of commands with expressive shape gestures, and enable more compact space use. We also found on average it took only seven trials of use before users could switch from novice to expert behavior with M3 and Multi-Stroke Marking Menu. There is a cost to learning these marking menus because in the novice mode, they took a longer time to execute than a linear menu did. We estimated the time cost and saving equilibrium point for M3 versus linear menus at about 30 trials of use, after which each trial of use translates to estimated time savings of 1.7 seconds. Future designer and researchers may use these first-order approximations in their design context to estimate the cost and benefit of adopting menu innovations like M3. In sum, by incorporating innovative features such as shape recognition, M3 Gesture Menu offers a more practical working solution for applying marking menu concepts on to touchscreen mobile devices. Our work also makes foundational contributions to future menu system design and research with empirical findings of M3 and marking menus’ resolution and learning characteristics.

"
Queer Visibility: Supporting LGBT+ Selective Visibility on Social Media,https://dl.acm.org/authorize?N657052,"
LGBT+ people adjust the presentation of their gender and sexual identities in response to social pressures, but their level of visibility differs between social media. We interviewed seventeen LGBT+ students at a socially-conservative university to investigate: (1) how do social media affect LGBT+ user experience of managing self presentation; and (2) how do social media affect participation in LGBT+ communities? We develop implications for design to support queering social media. (1) Give people abilities to present themselves with selective visibility, enabling choices about privacy and sharing, in contrast with the HCI design principle of indiscriminate 'making visible'. That is, enable participants to define their social media identities in their own ways. (2) Conduct studies, with a methodology likewise ensures that participants can define their gender and sexual identities in their own ways, rather than according to a predetermined vocabulary. We interviewed seventeen LGBT+ students in a conservative town in order to see: • How do social media affect LGBT+ user experience of managing self presentation? • How do social media affect participation in LGBT+ communities? We described our participants with their own words to prevent our assumptions from limiting their responses. From our findings, we developed implications for design that emphasize the value of legitimizing the use of multiple profiles and obscuring activities by default. Although our findings are from a small group of LGBT+ students in a conservative town, they are informative about how LGBT+ users manage their self presentation online. We join other research that values LGBT+ experiences as informative for design [16, 19]. In networked publics, it is impossible to prevent the spread of information. Although obscuring user activities with opt-in design cannot prevent all unwanted disclosure, changing the default behavior in a networked public may create new social media places, where LGBT+ users can relax about their visibility. We are optimistic that social media can be (re)designed to give LGBT+ users more agency to be selectively visible so they are more able to gain the benefits of participation on social media [31, 5, 30]. We are especially concerned about the ways in which realname, single-user policies limit that ways in which participants can identity themselves, because those policies seem to impose publicly sanitized, heteronormative performances. Butler’s theory of performativity suggests that the sanitized performances could become new, problematic norms that sanction only the performances Facebook (and other sites with real-name policies) allow. Broadly, our design implications contribute to supporting user’s selective visibility on social media via increased default privacy in contrast with the HCI design principle of indiscriminate ‘making visible.’ Through selective visibility, users are empowered to make decisions regarding sharing vs. privacy of potentially sensitive aspects of their identities. These findings were supported by a study methodology, in which we intentionally enabled participants to define their identities in their own ways. During interviews, we avoided giving users a set vocabulary by which they could identify, and participants described identities we did not anticipate. We suggest that other researchers allow participants to identify freely whenever possible, so that they do not limit participant’s answers to the researcher’s expectations. Designers need to be sensitive, giving marginalized participants agency in vital aspects of self presentation. Queer seeks to “address the margins, but not perhaps as conceived, since it deals with them by repositioning them” [25]. We address the margins by privileging LGBT+ experiences as informative for the design of social media.
"
Bento Browser: Complex Mobile Search Without Tabs,https://dl.acm.org/authorize?N657053,"
People engaged in complex searches such as planning a vacation or understanding their medical symptoms are often overwhelmed by opening and managing many tabs. These challenges are exacerbated as search moves to smartphones and mobile devices where screen real-estate is limited and tasks are frequently suspended, resumed, and interleaved. Rather than continue to utilize tab-based browsing for complex search, we introduce a new way of browsing through a scaffolded interface. The list of search results serves as a mutable workspace, where a user can track progress on a specific information query. The search query serves as a gateway into this workspace, accessed through a task-subtask hierarchy. We instantiate this in the Bento mobile search system and investigate its effectiveness in three studies. We find converging evidence that users were able to make progress on their complex searching tasks with this structure, and find it more organized and easier to revisit. We performed three studies to evaluate the use and usefulness of Bento in web search. Across a controlled lab study, qualitatively-focused deployment, and quantitatively-focused deployment, we found evidence that users appreciated both the task based organization interface, as well as the search results workspace interface. Together, these consistently made users feel more organized and feel like they could resume their activities more easily. Based on these findings, we also have a couple of additional takeaways. Users appeared to use a few strategies, aligned with many goal activation theory approaches [1]. For example, in study 2 P4 noted that she queued up searches for later exploration, largely a prospective planning task. Conversely, we had individuals like P2 and P3 who didn’t really understand the to-read feature, another planning tool we had incorporated into our design. This suggests that different user populations might practice different planning techniques for their exploratory searches, and while the structure appeared to be amenable to most of them, having tools for both planning and retrospective recounting could be essential to the design of these systems. This information was similar to what was found in Study 3 with user preference for different features of the system. Most users liked the overall organization for easy re-finding, however some users liked the specific planning features, such as the to-read feature and the grey progress bars. Supporting both of these resumption use cases will be key proceeding forward. We had several complaints from users about the overhead of Bento for simpler searches. In both studies 2 and 3, individuals noted that they wish they didn’t have to make an entire task card for just simple searches. However, in some of our interviews, individuals noted that their simple searches, such as looking up an actor in a movie, often blossomed into more complex searches, such as looking at what else that actor was in, what roles they typically play, etc. Having a low overhead, while also supporting this transition of simple search into complex search was an issue in Bento that was not entirely resolved.
"
Chibitronics in the Wild: Engaging New Communities in Creating Technology with Paper Electronics,https://dl.acm.org/authorize?N657054,"
We share a study on the public adoption the Chibitronics circuit sticker toolkit, an open source, commercially available hardware toolkit for learning and creating electronics on paper. We examine sales data over a two-and-a-half-year period from November 2013, when the kit was launched commercially, to June 2016. We also look at publicly available project documentation from users during this period. We find that the Chibitronics user community confounds norms for traditional technology-making communities, especially in gender demographics. We explore the artifacts and types of documentation produced by users to learn about the various backgrounds, values, and goals of subcommunities, which includes educators, Makers, and crafters. In particular, we focus on artifacts from the craft community as a surprising and distinctive subset of technology creators. The diversity in public engagement shows how paper electronics tools like Chibitronics can be an effective approach for engaging new and broader audiences to participate in technology creation. In this paper we have shared an early look at how, like the e-textile community, the paper electronics community may offer new pathways to engaging more diverse participants in designing and creating technologies. We examined Chibitronics customers and users as a representative subset of the greater paper electronics creative community. As we continue this research, we are particularly are excited to learn more about creators from the craft community as an emerging node of technology creators and how their adoption may lead to insights for foster overall access, adoption and cultural relevance of crafted electronic technologies as an engineering practice.
"
A Trip to the Moon: Personalized Animated Movies for Self-reflection,https://dl.acm.org/authorize?N657065,"
Self-tracking physiological and psychological data poses the challenge of presentation and interpretation. Insightful narratives for self-tracking data can motivate the user towards constructive self-reflection. One powerful form of narrative that engages audience across various culture and age groups is animated movies. We collected a week of self-reported mood and behavior data from each user and created in Unity a personalized animation based on their data. We evaluated the impact of their video in a randomized control trial with a non-personalized animated video as control. We found that personalized videos tend to be more emotionally engaging, encouraging greater and lengthier writing that indicated self-reflection about moods and behaviors, compared to non-personalized control videos. Converting personal data into a format that can have emotional impact can potentially fuel motivation for self-reflection and positive behavior change; however, a fully-automated system is not capable today of understanding human personal experiences at the kind of level that could make such a story both explicit and accurate. To solve this challenge, we built an automated system to construct an animation that is personalized to the data provided by an individual over a week, utilizing an animated avatar (corgi) that implicitly reflects the person’s behaviors, such as sleep and social interactions, and moods associated with each day of the week. We conducted two studies: The first found that the avatar’s portrayal of a set of moods and behaviors could be perceived accurately in general. The second study tested the emotional engagement of using a personalized story against a challenging control: a similar story that they were told was personalized. The results indicated that personalized animations tended to be more emotionally engaging, encouraging greater and lengthier writing that indicated self-reflection about moods and behaviors. Furthermore, while human imagination plays an important and valuable part in both conditions – test and control – the impact we found suggests that true personalization may be more powerfully influential on moods and self-reflection than simply believing that one is receiving personalized feedback.

"
Coco's Videos: An Empirical Investigation of Video-Player Design Features and Children's Media Use,https://dl.acm.org/authorize?N657066,"
In this study, we present Coco's Videos, a video-viewing platform for preschoolers designed to support them in learning to self-manage their media consumption. We report results from a three-week experimental deployment in 24 homes in which preschoolers used three different versions of the platform: one that is neutral to the limits they set, one that enforces the limits they set, and one that attempts to erode the limits they set by automatically playing additional content after the planned content is finished (""post-play""). We found that post-play significantly reduced children's autonomy and likelihood of self-regulation, extended video-viewing time, and led to increases in parent intervention. We found that the lock-out mechanism did not reduce video-viewing time or the likelihood of parent intervention. Together, our results suggest that avoiding platforms that work to undermine the user's intentions is more likely to help children self-regulate their media use than rigid parental controls. Today’s preschoolers will come of age in a world of constantly connected devices that will provide them with the opportunity to plug in at almost any moment. For these children, defining how they want to engage with this media ecosystem and managing their ongoing media consumption is likely to forever be a part of daily life. In this study, we present one possible alternative to today’s parental controls, in which we move away from the traditionally authoritarian designs of this space. Our results show that through design, we can support children in becoming autonomous users. Or, through features like post-play, we can undermine their ability to self-regulate. Child participants consistently showed evidence of wanting to take ownership of their transitions, and they demonstrated agency in both engaging and disengaging with technology. We conclude that the design community does not need to create tools to police excessive media use—we simply need to stop designing experiences to encourage it.
"
ResearchIME: A Mobile Keyboard Application for Studying Free Typing Behaviour in the Wild,https://dl.acm.org/authorize?N657067,"
We present a data logging concept, tool, and analyses to facilitate studies of everyday mobile touch keyboard use and free typing behaviour: 1) We propose a filtering concept to log typing without recording readable text and assess reactions to filters with a survey (N=349). 2) We release an Android keyboard app and backend that implement this concept. 3) Based on a three-week field study (N=30), we present the first analyses of keyboard use and typing biometrics on such free text typing data in the wild, including speed, postures, apps, auto correction, and word suggestions. We conclude that research on mobile keyboards benefits from observing free typing beyond the lab and discuss ideas for further studies. Mobile touschreen typing has mostly been studied in the lab with transcription tasks. To facilitate free text entry studies in the wild, we developed a logging concept and keyboard app. A survey (N=349) assessed views on related privacy filters. We deployed our app in a three-week field study (N=30) and presented the first analyses of keyboard use and typing biometrics in free text composition in the wild, including speed, postures, apps, auto correction, and word suggestions. Finally, is it worth the effort for researchers to collect typing data from free text composition in everyday life? We believe the answer is yes, it is a valuable additional method. As the HCI community follows diverse research interests with regard to text entry, the study choice should be aligned with those: The lab transcription task is practical if one is mainly interested in speed and error rates, for example to compare a novel text entry method to a baseline. However, as related work showed [52], these tasks should also be conducted in the field. On the other hand, our study showed individual differences: For example, a lab study might enforce a fixed (common) hand posture, but we found that some people prefer other postures than the majority. Similar observations hold for device orientation, use of word suggestions, and typing activity throughout the day. Moreover, users’ favourite posture in our questionnaire differed from the one they actually reported most often while typing. Our results also suggest that the biometric value of typing behaviour shows in different features in free typing in the wild compared to transcription in the lab. In conclusion, we see great value in studying unconstrained typing in users’ daily lives to capture user-specific behaviour. This fits research agendas on adaptive and personalised keyboards and on typing biometrics. Moreover, such data may help to address the challenge of designing for special user groups and varying contexts of use [39, 40].

"
Wearables for Learning: Examining the Smartwatch as a Tool for Situated Science Reflection,https://dl.acm.org/authorize?N657068,"
Relatively little research exists on the use of smartwatches to support learning. This paper presents an approach for commodity smartwatches as a tool for situated reflection in elementary school science. The approach was embodied in a smartwatch app called ScienceStories that allows students to voice record reflections about science concepts anytime, anywhere. We conducted a study with 18 fifth-grade children to investigate first, the effects of ScienceStories on students' science self-efficacy, and second the effects of different motivational structures (gamification, narrative-based, hybrid) designed into the smartwatch app on students' quality and quantity of use. Quantitative results showed ScienceStories increased science self-efficacy especially with a motivational structure. The gamified version had the highest quantity of use, while narrative performance performed worst. Qualitative findings described how students' recordings related to science topics and were contextualized. We discuss how our findings contribute to understanding of how to design smartwatch apps for educational purposes. We proposed an approach that frames the smartwatch as a tool for situated reflection to support informal science learning in everyday life for elementary school students. We incorporated different kinds of motivational structures into the design of the ScienceStories educational smartwatch app, to explore ways to promote app use over time. Our study showed that the smartwatch as a situated reflection tool positively affects science self-efficacy and elicited the richness of students’ in-situ smartwatch science recordings. Much research has shown that self-efficacy is an important predictor of academic performance as well as amount of persistence in school work [68]. However, none of the app versions excelled in all dimensions. The gamified app appeared to be the best compromise in terms of generating sufficient recordings with some connection to the science concept, and resulting in increased science self-efficacy as compared to traditional classroom instruction. More work has to be done to investigate smartwatch effects on actual learning or knowledge increase. As of now, we have barely begun to scrape the surface of the untapped potential of smartwatches for education. With its new set of characteristics focusing on persistent presence on the wearer, the smartwatch may allow teachers in the future to address and scaffold students’ learning in spatial and temporal contexts way beyond the classroom.

"
Investigating How Online Help and Learning Resources Support Children's Use of 3D Design Software,https://dl.acm.org/authorize?N657069,"
3D design software is increasingly available to children through libraries, maker spaces, and for free on the web. This unprecedented availability has the potential to unleash children's creativity in cutting edge domains, but is limited by the steep learning curve of the software. Unfortunately, there is little past work studying the breakdowns faced by children in this domain-most past work has focused on adults in professional settings. In this paper, we present a study of online learning resources and help-seeking strategies available to children starting out with 3D design software. We find that children face a range of challenges when trying to learn 3D design independently-tutorials present instructions at a granularity that leads to overlooked and incorrectly-performed actions, and online help-seeking is largely ineffective due to challenges with query formulation and evaluating found information. Based on our findings, we recommend design directions for next-generation help and learning systems tailored to children. As 3D design tools are increasingly being used by non-professional users, including children, hobbyists, and casual makers, there is a need to refine and update our understanding of how to design software help and learning tools to accommodate these new user populations. This paper provides a first step toward establishing a better understanding of children’s learning and help-seeking behaviors with 3D design software, and we believe that our findings can serve as a foundation for developing next-generation help and learning systems to enable children to learn and improve 3D design skills.
"
Tactile Information Transmission by 2D Stationary Phantom Sensations,https://dl.acm.org/authorize?N657060,"
A phantom sensation refers to an illusory tactile sensation perceived midway between multiple distant stimulations on the skin. Phantom sensations have been used intensively in tactile interfaces owing to their simplicity and effectiveness. Despite that, the perceptual performance of phantom sensations is not completely understood, especially for 2D cases. This work is concerned with 2D stationary phantom sensations and their fundamental value as a means for information display. In User Study 1, we quantified the information transmission capacity using an absolute identification task of 2D phantom sensations. In User Study 2, we probed the distributions of the actual perceived positions of 2D phantom sensations. The investigations included both types of phantom sensations-within and out of the body. Our results provide general guidelines as to leveraging 2D phantom sensations in the design of spatial tactile display. In this paper, we have investigated the absolute identification performance and the continuous localization accuracy of 2D stationary phantom sensations applied to the hands. For clear perception of the illusory sensations, vibration modules were designed to provide localized vibrations using an effective attenuation layer and attached to the four corners of a smart phone held in one hand (PHONE). In the other configuration (RING), vibration modules were worn in the index and ring fingers of both hands, eliciting out-of-the-body phantom sensations in the empty space between the hands. Two user studies carried out to evaluate the perceptual performance of the two kinds of 2D stationary phantom sensations disclosed several important findings, as follows: 1. The identification performance of 2D stationary phantom sensations is not very high and lower than that of 2D moving phantom sensations. 2. The 2D positions that users perceive are systematically correlated with the target positions of 2D phantom sensations. 3. The accuracy of position perception has a bias that increases as the target position moves closer to the boundaries. 4. 2D stationary phantom sensations are not very adequate as a primary means of non-visual information delivery. 5. 2D stationary phantom sensations may still contribute to improving user experiences by providing tactile stimulation to the neighborhood of target position for the emphasis of important events. To the best of our knowledge, this work is among the first that introduces the phenomenon of 2D out-of-the-body stationary phantom sensations without a rigid medium and quantifies the perceptual performance of 2D stationary phantom sensations in terms of identification and continuous positional accuracy.

"
Understanding Artefact and Process Challenges for Designing Low-Res Lighting Displays,https://dl.acm.org/authorize?N657061,"
Low-resolution (low-res) lighting displays are increasingly used by HCI researchers, designers, and in the industry as a versatile and aesthetic medium for deploying ambient interfaces in various contexts. These display types distinguish themselves from conventional high-res screens through: high contrasts, hi-power LED technology which allows visibility even in bright environments, and their ability to take on three-dimensional free forms. However, to date most work on low-res displays has been either of experimental nature or carried out in isolated industry contexts. This paper addresses this gap through an analysis of our own experiences from previous experimental design studies and related work, which led us to five domain challenges for designing low-res displays. We then describe how we approached these challenges in a deployment study, which involved the implementation of a prototype guided by a low-res prototyping toolkit. Based on an analysis of our design process and findings from the deployment study, we present ten design recommendations for low-res lighting displays. As displays become more versatile and embedded into objects and environments around us, we will be consuming more and more information via low-res lighting displays. In this paper, we have shown artefact and process related domain challenges when envisioning, designing, pretesting, and deploying low-res displays. Our contribution to this emerging field can therefore be summarized as: (1) a presentation of design recommendations for low-res lighting displays in the home context, and (2) how the challenges that we derived from various design studies and cases can be addressed in other design contexts by following our toolkit-supported user-centered design process. Practitioners and designers can refer to these recommendations as starting points to scaffold their own process of designing low-res lighting displays.

"
Thermorph: Democratizing 4D Printing of Self-Folding Materials and Interfaces,https://dl.acm.org/authorize?N657062,"
We develop a novel method printing complex self-folding geometries. We demonstrated that with a desktop fused deposition modeling (FDM) 3D printer, off-the-shelf printing filaments and a design editor, we can print flat thermoplastic composites and trigger them to self-fold into 3D with arbitrary bending angles. This is a suitable technique, called Thermorph, to prototype hollow and foldable 3D shapes without losing key features. We describe a new curved folding origami design algorithm, compiling given arbitrary 3D models to 2D unfolded models in G-Code for FDM printers. To demonstrate the Thermorph platform, we designed and printed complex self-folding geometries (up to 70 faces), including 15 self-curved geometric primitives and 4 self-curved applications, such as chairs, the simplified Stanford Bunny and flowers. Compared to the standard 3D printing, our method saves up to 60% - 87% of the printing time for all shapes chosen. In this paper, we presented Thermorph, a rapid prototyping system through self-folding mechanisms of shape memory thermoplastic. While self-folding has been introduced for manufacturing, the main contribution of this paper is to introduce a novel composite material design, a low cost FDM 3D printing approach, and an end-to-end design pipeline to fold a variety of arbitrary 3D geometries from a flat sheet. While our approach has proved to save printing time and post-processing efforts for a lot of complex 3D geometry fabrication comparing to standard 3D printing approach, we see the main promise of the work to achieve novel folding structures with readily available printing materials. For future work, we plan to go beyond desktop 3D printers and explore such self-folding structures on a larger scale.

"
Looks Can Be Deceiving: Using Gaze Visualisation to Predict and Mislead Opponents in Strategic Gameplay,https://dl.acm.org/authorize?N657063,"
In competitive co-located gameplay, players use their opponents' gaze to make predictions about their plans while simultaneously managing their own gaze to avoid giving away their plans. This socially competitive dimension is lacking in most online games, where players are out of sight of each other. We conducted a lab study using a strategic online game; finding that (1) players are better at discerning their opponent's plans when shown a live visualisation of the opponent's gaze, and (2) players who are aware that their gaze is tracked will manipulate their gaze to keep their intentions hidden. We describe the strategies that players employed, to various degrees of success, to deceive their opponent through their gaze behaviour. This gaze-based deception adds an effortful and challenging aspect to the competition. Lastly, we discuss the various implications of our findings and its applicability for future game design. In a lab study of an online strategy game enhanced with oneway shared gaze visualisation, we found that players were able use gaze visualisation to improve their ability to predict an opponent’s intentions. Players were also able to alter their own gaze behaviour deliberately to hide their true intentions and convey false impressions, to a degree. We defined a typology of gaze-based deception strategies, which reflects the general theory of deception [5]. However, this deceptive behaviour required sustained effort, and players could not prevent some of their intentions from ‘leaking’ to the opponent through their gaze. Players found this gaze-based deception task to be challenging and engaging, and we have identified several avenues that may be fruitful for game design to explore.

"
Security During Application Development: an Application Security Expert Perspective,https://dl.acm.org/authorize?N657064,"
Many of the security problems that people face today, such as security breaches and data theft, are caused by security vulnerabilities in application source code. Thus, there is a need to understand and improve the experiences of those who can prevent such vulnerabilities in the first place - software developers as well as application security experts. Several studies have examined developers' perceptions and behaviors regarding security vulnerabilities, demonstrating the challenges they face in performing secure programming and utilizing tools for vulnerability detection. We expand upon this work by focusing on those primarily responsible for application security - security auditors. In an interview study of 32 application security experts, we examine their views on application security processes, their workflows, and their interactions with developers in order to further inform the design of tools and processes to improve application security. To our knowledge, this paper is the first user study to specifically focus on application security experts. Our results provide further evidence that application security work is primarily performed by these experts, separately from software development teams. Separating security from development adds communication overhead and barriers. And finding and fixing security vulnerabilities so late in the software lifecycle can result in costly delays and expense to applications, and an increased likelihood that applications are not adequately secure. In particular, our results highlight the importance of triage to reflect risk assessments, the challenges of security communication that mostly occurs within bug tracking systems, the experts’ role of motivating developers to fix problems, the importance of security champions, and the desires for less configuration and more automation. These results further demonstrate that improving application security will involve a combination of organizational processes to incentivize and support developers in focusing on security issues earlier, developer training to give them more concrete and actionable knowledge and motivation, and tools for both developers and experts that reduce the manual burdens of configuration and analysis. We hope these results can inform a variety of process and tool improvements to reduce the costs of vulnerability detection and remediation, and allow application security experts to focus on deeper and more complex security issues and processes.

"
Insert Needle Here! A Custom Display for Optimized Biopsy Needle Placement,https://dl.acm.org/authorize?N657075,"
Needle-guiding templates are used for a variety of minimally invasive medical interventions. While physically supporting needle placement with a grid of holes, they lack integrated information where needles need to be inserted. Physicians must manually determine the correct holes based on the output of planning software - a workflow that is error-prone and lengthy. We address these issues by embedding a display into the template using electroluminescence (EL) screen printing. The EL display is connected to planning software and illuminates the correct hole. In an empirical evaluation with physicians and researchers from the medical domain, we compare the illuminated against the conventional template as used in magnetic resonance imaging (MRI) guided prostate biopsies. Our results show that the EL display significantly improves task completion time by 51%, task load by 47% and usability by 30%. In the presented research, we developed a customized screenprinted electroluminescense display that can be used in the extreme environment of a strong magnetic field in the MRI of up to 3.0 Tesla. With this, we applied customized screenprinted EL displays to the medical field and improved the needle-guiding template used in prostate biopsies with integrated visual feedback. In a comparative evaluation we showed that task completion time was reduced by 51 %, task load lowered by 47 % and usability improved by 30 % when using our novel EL display approach without changes to the workflow. The participants, physicians and researchers from the medical domain, highly appreciated using the illuminated display. Future work will address more advanced hardware prototypes with thinner, tighter and brighter light emitting lines to further improve the needle guiding. Additionally, we aim at embedding the display technology completely into the acrylic block, close to the top surface, in order to reduce the light diffusion within the template. In future studies, we are going to investigate increasing the hole density, which is currently restricted to 5 mm spacing in the manual approach. Furthermore, we plan to improve the sterilizability and electronic safety of the device for clinical evaluation under an IRB5 -approved study. Since the device does not have any actuated components, obtaining an investigational device exemption (IDE) would be straightforward, compared to robotic systems for needle guidance. With embedded visual needle-guiding, the procedure of placing the needle in the correct hole could potentially be faster and easier for all procedures involving needle-guiding templates like tissue sampling and cancer treatment without changes to the current clinical workflow.

"
Automatic Diagnosis of Students' Misconceptions in K-8 Mathematics,https://dl.acm.org/authorize?N657076,"
K-8 mathematics students must learn many procedures, such as addition and subtraction. Students frequently learn ""buggy' variations of these procedures, which we ideally could identify automatically. This is challenging because there are many possible variations that reflect deep compositions of procedural thought. Existing approaches for K-8 math use manually specified variations which do not scale to new math algorithms or previously unseen misconceptions. Our system examines students' answers and infers how they incorrectly combine basic skills into complex procedures. We evaluate this approach on data from approximately 300 students. Our system replicates 86% of the answers that contain clear systematic mistakes (13%). Investigating further, we found 77% at least partially replicate a known misconception, with 53% matching exactly. We also present data from 29 participants showing that our system can demonstrate inferred incorrect procedures to an educator as successfully as a human expert. We presented a system that automatically identifies incorrect procedural thought processes in K-8 mathematics. Our approach generates programs representing students’ thought processes and explains misconceptions by visualizing those programs side-by-side with the correct algorithm. The evaluations in this paper focus on K-8 mathematics, but in future work we hope to evaluate whether our approach can generalize to other domains. We believe our mechanisms for thought-process reconstruction and visualization could work for math topics that involve deterministic step-by-step computations on a 2D spreadsheet. For example, if we were to add operators that transform variables and coefficients we could potentially extend our approach to solve linear equations. In future work, we will explore how our system can be used in classroom settings by integrating it into an online grading port.

"
Defining and Predicting the Localness of Volunteered Geographic Information using Ground Truth Data,https://dl.acm.org/authorize?N657077,"
Many applications of geotagged content are predicated on the concept of localness (e.g., local restaurant recommendation, mining social media for local perspectives on an issue). However, definitions of who is a ""local"" in a given area are typically informal and ad-hoc and, as a result, approaches for localness assessment that have been used in the past have not been formally validated. In this paper, we begin the process of addressing these gaps in the literature. Specifically, we (1) formalize definitions of ""local"" using themes identified in a 30-paper literature review, (2) develop the first ground truth localness dataset consisting of 132 Twitter users and 58,945 place-tagged tweets, and (3) use this dataset to evaluate existing localness assessment approaches. Our results provide important methodological guidance to the large body of research and practice that depends on the concept of localness and suggest means by which localness assessment can be improved. In this paper, we worked to make more concrete the idea of “localness” as it is used in computing. We formalize three definitions of localness and assess the accuracy of localness assessment techniques that have been employed in the literature according to these definitions. We find that while some techniques are relatively accurate, others are very noisy, especially for certain definitions. Researchers and practitioners can utilize our results to provide methodological guidance when building applications or doing studies for which the concept of localness is important.

"
Bots & (Main)Frames: Exploring the Impact of Tangible Blocks and Collaborative Play in an Educational Programming Game,https://dl.acm.org/authorize?N657078,"
While recent work has begun to evaluate the efficacy of educational programming games, many common design decisions in these games (e.g., single player gameplay using touchpad or mouse) have not been explored for learning outcomes. For instance, alternative design approaches such as collaborative play and embodied interaction with tangibles may also provide important benefits to learners. To better understand how these design decisions impact learning and related factors, we created an educational programming game that allows for systematically varying input method and mode of play. In this paper, we describe design rationale for mouse and tangible versions of our game, and report a 2x2 factorial experiment comparing efficacy of mouse and tangible input methods with individual and collaborative modes of play. Results indicate tangibles have a greater positive impact on learning, situational interest, enjoyment, and programming self-beliefs. We also found collaborative play helps further reduce programming anxiety over individual play. In this paper, we presented a study that systematically explored how modulating input methods and mode of play in an educational programming game can impact learning outcomes and important related factors. Our results provide concrete evidence that incorporating tangibles into the design of an educational programming game can improve programming self-beliefs (i.e., debugging self-efficacy, programming self-concept, programming interest, programming aptitude), situational interest (i.e., exploration intention, instant enjoyment, attention demand, novelty), subjective feelings towards enjoyment, and intentions to replay/recommend the game in the future. They also provide evidence that, while not as effective in other aspects, collaborative play does reduce players’ anxiety towards the act of programming. Ultimately, we have only just begun to touch upon the impact of tangibles and collaborative play in educational programming games. One notable aspect of our study was the university age group of participants. However, many of the current CS education initiatives—such as Hour of Code [91]—are geared towards the K-12 demographic [5]. In future work, we plan to test our tangible and collaborative designs on younger audiences who may benefit more strongly from such approaches.

"
StammerApp: Designing a Mobile Application to Support Self-Reflection and Goal Setting for People Who Stammer,https://dl.acm.org/authorize?N657079,"
Stammering is a speech disorder affecting approximately 1% of the worldwide population. It can have associated impacts on daily life, such as loss of confidence in social situations and increased anxiety levels (particularly when speaking to strangers). Work exploring the development of digital tools to support people who stammer (PwS) is emerging. However, there is a paucity of research engaging PwS in the design process, with participation being facilitated mainly in testing phases. In this paper, we describe the user-centered design, development and evaluation of StammerApp, a mobile application to support PwS. We contribute insights into the challenges and barriers that PwS experience day-to-day and reflect on the complexities of designing with this diverse group. Finally, we present a set of design recommendations for the development of tools to support PwS in their everyday interactions, and provide an example of how these might be envisioned through the StammerApp prototype. Through this study we have offered a deepened understanding of the specific needs and values of people who stammer for the development of digital tools to support them in their day to day lives. Our study has highlighted the homogeneity of stammering as a condition and the need for personalizable tools that support individuals within the settings and situations that they require most. Future research is required to further scope the potential for tools such as StammerApp to make a difference in the lives of PwS. While showing promise in the support of PwS, larger field trials of StammerApp, over longer periods of time and with a wider sample of the stammering community, is required. Whilst outside of the scope of this paper, future evaluations to determine the effectiveness of apps like StammerApp should consider the use of informal attitudinal rating scales (e.g. such as the Overall Assessment of the Speaker’s Experience of Stuttering [43], or the situational anxiety hierarchy [12]), as well as considering methods such as experience sampling, to understand day to day improvements that users might be experiencing. The collection of telemetry around app use would also add depth to future trials.
"
Contextualizing Privacy Decisions for Better Prediction (and Protection),https://dl.acm.org/authorize?N657070,"
Modern mobile operating systems implement an ask-on-first-use policy to regulate applications' access to private user data: the user is prompted to allow or deny access to a sensitive resource the first time an app attempts to use it. Prior research shows that this model may not adequately capture user privacy preferences because subsequent requests may occur under varying contexts. To address this shortcoming, we implemented a novel privacy management system in Android, in which we use contextual signals to build a classifier that predicts user privacy preferences under various scenarios. We performed a 37-person field study to evaluate this new permission model under normal device usage. From our exit interviews and collection of over 5 million data points from participants, we show that this new permission model reduces the error rate by 75% (i.e., fewer privacy violations), while preserving usability. We offer guidelines for how platforms can better support user privacy decision making. This study showed how apps and users respond to a real-world deployment of a novel contextually-aware permission model. The new permission system significantly reduced the error rate from that of the prevailing “ask-on-first-use” model first deployed in Android 6.0. While prior work already demonstrated ways to increase the protection provided by new permission models, we believe our study provides opportunities to further improve performance and address practical limitations in actual implementations.
"
OptiSpace: Automated Placement of Interactive 3D Projection Mapping Content,https://dl.acm.org/authorize?N657071,"
We present OptiSpace, a system for the automated placement of perspectively corrected projection mapping content. We analyze the geometry of physical surfaces and the viewing behavior of users over time using depth cameras. Our system measures user view behavior and simulates a virtual projection mapping scene users would see if content were placed in a particular way. OptiSpace evaluates the simulated scene according to perceptual criteria, including visibility and visual quality of virtual content. Finally, based on these evaluations, it optimizes content placement, using a two-phase procedure involving adaptive sampling and the covariance matrix adaptation algorithm. With our proposed architecture, projection mapping applications are developed without any knowledge of the physical layouts of the target environments. Applications can be deployed in different uncontrolled environments, such as living rooms and office spaces. We have presented OptiSpace, a system for optimizing the placement of interactive projection mapping content, based on empirical user behavior. Developers implement interactive projection mapping applications just once, without knowledge of the actual room geometry or possible user viewing angles. Applications can then be deployed in different uncontrolled environments, not necessarily by the developers themselves. OptiSpace automatically measures the target environment, including users, who are completely uninstrumented. Our optimization is based on our measurements and various programmable attributes and behaviors. We have proposed an approach to estimating the quality of perspectively corrected content. The generic design of our architecture makes it applicable to a broad range of dynamic interactive projection mapping applications for uncontrolled environments.

"
Technology and the Givens of Existence: Toward an Existential Inquiry Framework in HCI Research,https://dl.acm.org/authorize?N657072,"
The profound impact of digital technologies on human life makes it imperative for HCI research to deal with the most fundamental aspects of human existence. Arguably, insights from existential philosophy and psychology are highly relevant for addressing such issues. Building on previous attempts to bring in existential themes and terminology to HCI, this paper argues that Yalom's notion of ""the givens of existence"", as well as related work in experimental existential psychology, can inform the development of an existential inquiry framework in HCI. The envisioned framework is intended to complement current approaches in HCI by specifically focusing on the existential aspects of the design and use of technology. The paper reflects on possible ways, in which existential concepts can support HCI research, and maintains that adopting an existential framework in HCI would be consistent with the overall conceptual development of the field. Psychology played a central role in the emergence of HCI, which is directly obvious even from the titles of the books that, essentially, created the field: The Psychology of Human Computer Interaction [19] and Designing Interaction: Psychology at the Human-Computer Interface [20]. The books offered the vision of HCI as a field at the intersection of computer science and social sciences (most notably, psychology), in which social sciences were expected to inform the design of better systems. As these field-shaping books indicate, in its early years, the field of HCI was generally understood as a problem-solver, not a problem owner. Things are different now. To begin with, the impact of psychology on HCI is not as visible as it used to be. More importantly, HCI is no longer only about helping system developers create useful and usable technologies; it is gradually transforming itself into a field with its own agenda, a field, which is not only a problem solver but also seeks to define itself as a problem owner. This transition does not occur smoothly; despite abundant visible signs of being a booming, highly successful area, HCI appears to be experiencing some sort of an identity crisis [38]. While it is clear that the field is not just about exploring novel design solutions for better digital products, it is less clear what HCI’s own research agenda is. A key direction in which the field explores a possibility to define its own object of inquiry, is, as discussed above, becoming more focused on people, on the impact of technology on how humans make meaning of their lives [3,46]. This object of inquiry is, in a way, opposite to HCI’s initial concern: it implies prioritizing the implications of technology for people, rather than the implications of studying people for the design of technology [22]. One of the main challenges for the transition to a new agenda is a lack of sufficiently developed methods, approaches, and terminology that could be used to more specifically analyze the actual meaning making processes studied in HCI. A number of concepts and methods in the field provide a general conceptual framing for the analysis; for instance, they identify the underlying foundations (e.g., embodiment [21]) or strategies (e.g., design provocations [23,24]) of meaning making. However, there has been relatively little attention to systematic conceptual analysis of meaning making per se. A possible way of dealing with this challenge, discussed in this paper, is turning back to psychology, a discipline that once contributed to the very emergence of HCI. However, the versions of psychology, explored in the paper— existential psychotherapy and experimental existential psychology—are quite different from the information processing psychology of the 1980s. Instead of suggesting models of human cognition, they offer insights into how people deal with the most fundamental challenges of their existence. Two issues need to be clarified in relation to the claims made in this paper. First, it is not the intention of this paper to undermine the importance of HCI research directed at the design of better digital technologies. HCI studies that primarily aim to contribute to design, either by identifying the need for new technologies or creating and testing novel design solutions are, and will always be, a crucial part of HCI. In addition, the adoption of more human-centered approaches in HCI does not exclude potential design contributions. On the contrary, it may actually open new opportunities for the field to make an impact on the development of new technologies. Second, the suggestion to expand the scope of HCI to include existential issues, outlined in this paper, could be perceived as an attempt to go too far in extending the scope of the field. Since HCI cannot be about everything, perhaps existential issues should be left to other disciplines that have a longer experience of engaging with them? The position adopted in this paper is that, while it is true that blurring the borders of HCI can be a problem, a certain overlap between HCI and existing fields is inevitable and probably even desirable. HCI has a valuable and unique experience of conducting research into how digital technologies transform our lives, an experience that other disciplines are lacking. Therefore, it is unlikely that analysis of the impact of digital technologies on human experience of existential issues can be completely delegated to other fields. On a final note, it would be a mistake to label the existential perspective as a basically pessimistic view that only emphasizes the tragic, dark side of our existence. While the perspective does point to enormous existential challenges, it also emphasizes how precious each moment of our life is and suggests that directly facing the “givens of existence” may help us understand how to make our lives more authentic and meaningful. One can argue that it is unrealistic to expect HCI research to contribute to fulfilling this ideal in a concrete and specific way, and it may be true. Or maybe not.
"
Smart Kitchens for People with Cognitive Impairments: A Qualitative Study of Design Requirements,https://dl.acm.org/authorize?N657073,"
Individuals with cognitive impairments currently leverage extensive human resources during their transitions from assisted living to independent living. In Western Europe, many government-supported volunteer organizations provide sheltered living facilities; supervised environments in which people with cognitive impairments collaboratively learn daily living skills. In this paper, we describe communal cooking practices in sheltered living facilities and identify opportunities for supporting these with interactive technology to reduce volunteer workload. We conducted two contextual observations of twelve people with cognitive impairments cooking in sheltered living facilities and supplemented this data through interviews with four employees and volunteers who supervise them. Through thematic analysis, we identified four themes to inform design requirements for communal cooking activities: Work organization, community, supervision, and practicalities. Based on these, we present five design implications for assistive systems in kitchens for people with cognitive deficiencies. In this paper, we investigated the design space of smart kitchens for cognitively impaired persons living in sheltered housing facilities. Within qualitative contextual inquiries, we obtained data by observing the target group during cooking. Furthermore, interviews were conducted to explore design gaps in-depth. By analyzing the interviews, we derived the four relevant themes WORK ORGANIZATION, COMMUNITY, SUPERVISION, and PRACTICALITIES, which emerge as important factors. We conclude with five design implications, which should be considered when designing smart kitchens for cognitively impaired persons: clear communication of tasks, fostering the group experience, prioritizing safety, providing rewards, and enabling contextual adaptivity.
"
Graphical Perception of Continuous Quantitative Maps: the Effects of Spatial Frequency and Colormap Design,https://dl.acm.org/authorize?N657074,"
Continuous 'pseudocolor' maps visualize how a quantitative attribute varies smoothly over space. These maps are widely used by experts and lay citizens alike for communicating scientific and geographical data. A critical challenge for designers of these maps is selecting a color scheme that is both effective and aesthetically pleasing. Although there exist empirically grounded guidelines for color choice in segmented maps (e.g., choropleths), continuous maps are significantly understudies, and their color-coding guidelines are largely based on expert opinion and design heuristics--many of these guidelines have yet to be verified experimentally. We conducted a series of crowdsourced experiments to investigate how the perception of continuous maps is affected by colormap characteristics and spatial frequency (a measure of data complexity). We find that spatial frequency significantly impacts the effectiveness of color encodes, but the precise effect is task-dependent. While rainbow schemes afforded the highest accuracy in quantity estimation irrespective of spatial complexity, divergent colormaps significantly outperformed other schemes in tasks requiring the perception of high-frequency patterns. We interpret these results in relation to current practices, and devise new and more granular guidelines for color mapping in continuous maps. We conducted three experiments to investigate the effects of spatial frequency and colormap characteristics on the perception of continuous, pseudocolor maps. Our results indicate that spatial frequency impacts judgment of the encoded quantities and structures. While viewers’ quantity estimation accuracy exhibited a predictable response, increased data complexity had a more nuanced effect on gradient and pattern comprehension, the impact of which was dependent on the colormap used. Designers should therefore consider both the type of task and the spatial complexity of the underlying data. We re-examined current guidelines and devised new recommendations for color-coding of continuous spatial data.
"
Wall++: Room-Scale Interactive and Context-Aware Sensing,https://dl.acm.org/authorize?N657085,"
Human environments are typified by walls, homes, offices, schools, museums, hospitals and pretty much every indoor context one can imagine has walls. In many cases, they make up a majority of readily accessible indoor surface area, and yet they are static their primary function is to be a wall, separating spaces and hiding infrastructure. We present Wall++, a low-cost sensing approach that allows walls to become a smart infrastructure. Instead of merely separating spaces, walls can now enhance rooms with sensing and interactivity. Our wall treatment and sensing hardware can track users' touch and gestures, as well as estimate body pose if they are close. By capturing airborne electromagnetic noise, we can also detect what appliances are active and where they are located. Through a series of evaluations, we demonstrate Wall++ can enable robust room-scale interactive and context-aware applications. In this work, we introduced Wall++, a low-cost sensing technique that can turn ordinary walls into smart infrastructure, able to sense interactions and activities happening in a room, and potentially throughout an entire building. Our multi-phase exploration of materials, application methods, and electrode patterns informed our proof-of-concept hardware and software implementation. Then, through a series of user studies, we demonstrated that Wall++ can robustly track user touches and poses, as well as detect and track appliances (or tagged users) in a room.

"
Design Vocabulary for Human--IoT Systems Communication,https://dl.acm.org/authorize?N657086,"
Digital devices and intelligent systems are becoming popular and ubiquitous all around us. However, they seldom provide sufficient feed-forwards and feedbacks to reassure users as to their current status and indicate what actions they are about to perform. In this study, we selected and analyzed nine concept videos on future IoT products/systems. Through systematic analysis of the interactions and communications of users with the machines and systems demonstrated in the films, we extracted 38 design vocabulary items and clustered them into 12 groups: Active, Request, Trigger functions, Approve, Reject, Notify, Recommend, Guide, Show problems, Express emotions, Exchange info, and Socialize. This framework can not only inspire designers to create self-explanatory intelligence, but also support developers to provide a language structure at different levels of the periphery of human attention. Through the enhancement of situated awareness, human IoT system interaction can become more seamless and graceful. As we surround ourselves with more and more smart objects, feedback is probably even more essential when we interact with those objects than with other people. We need to know what is happening, what the machine has detected, what its status is, and what actions it is about to perform [32]. To understand the various kinds of possible situation that the user wants to know about or the machine wants to communicate, we selected and analyzed nine concept videos on human–system interactions. From these we extracted a vocabulary of 38 design terms clustered in 12 groups. The findings do not simply cover the feedback design of available systems, but express possibilities for designing future objects with sociability. With this framework, developers would be able to build a language structure to convey the artefact’s intended purpose to the user, while at the same time it could also establish the rational accountability for increasing human users’ situated awareness. In this way, human users could effortlessly anticipate the status of IoT systems from implicit or peripheral communications and switch their attention to have focused interactions when needed. This human-like communication could make the interaction experience more graceful and enjoyable.

"
"Accountability Work: Examining the Values, Technologies and Work Practices that Facilitate Transparency in Charities",https://dl.acm.org/authorize?N657087,"
Charities are subject to stringent transparency and accountability requirements from government and funders to ensure that they are conducting work and spending money appropriately. Charities are increasingly important to civic life and have unique characteristics as organisations. This provides a rich space in which HCI research may learn from and affect both held notions of transparency and accountability, and the relationships between these organisations and their stakeholders. We conducted ethnographic fieldwork and workshops over a seven month period at a charity. We aimed to understand how the transparency obligations of a charity manifest through work and how the workers of a charity reason about transparency and accountability as an everyday practice. Our findings highlight how organisations engage in presenting different accounts of their work; how workers view their legal transparency obligations in contrast with their accountability to their everyday community; and how their labour does not translate well to outcome measures or metrics. We discuss implications for the design of future systems that support organisations to produce accounts of their work as part of everyday practice.support organisations to produce accounts of their work as part of everyday practice. In this paper we set out to explore how an understanding of the everyday work practices of charities could be used to inform the design of systems that seek to support them in becoming transparent and accountable. We explicate that the complex nature of transparency and accountability manifests as a variety of interconnected work practices that are experienced by the charity workers, and how socio-technical systems that are used by organisations also affect these same practices. We then present implications for the design of future systems that embed values of worker control and flexibility in order to support charities navigating their obligations in everyday practice. We discuss this by drawing upon our understanding of the accountable nature of work practices, and how this may be captured and represented through interoperable digital systems that allow charities to configure transparency and accountability in accordance with their needs; leading to the concept of ‘Linked Accounting’. Charity organisations and the HCI community share important civic and social concerns, and the reduction of barriers to a charity’s efforts through digital technologies has far-reaching implications for society. Future work should seek to further engage with charities to collaboratively develop and deploy these systems to discover how they may be appropriated into work practice to achieve organisational goals. Care should be taken to ensure that these novel accounting technologies are developed so that they are not used to control the actions of workers, but used to provide the workers a flexible means to deliver work, and to have this interpreted in a diverse number of ways. In doing so HCI may affect civic change through engagement with this rich design space.
"
Crowdsourcing Treatments for Low Back Pain,https://dl.acm.org/authorize?N657088,"
Low back pain (LBP) is a globally common condition with no silver bullet solutions. Further, the lack of therapeutic consensus causes challenges in choosing suitable solutions to try. In this work, we crowdsourced knowledge bases on LBP treatments. The knowledge bases were used to rank and offer best-matching LBP treatments to end users. We collected two knowledge bases: one from clinical professionals and one from non-professionals. Our quantitative analysis revealed that non-professional end users perceived the best treatments by both groups as equally good. However, the worst treatments by non-professionals were clearly seen as inferior to the lowest ranking treatments by professionals. Certain treatments by professionals were also perceived significantly differently by non-professionals and professionals themselves. Professionals found our system handy for self-reflection and for educating new patients, while non-professionals appreciated the reliable decision support that also respected the non-professional opinion. Back Pain Workshop is a crowd-powered online system that lets users discover LBP treatments that originate from two distinct crowds: professionals and non-professionals. Our results indicated that, according to non-professionals’ assessment, the highest-rated treatments crowdsourced from non-professionals are of comparable quality to the highestrated treatments from professionals. However, in their assessment, the worst treatments from non-professionals were perceived clearly as worse than the worst from nonprofessionals. Professionals, on the other hand, perceived many of their own suggestions to be of lower quality than what non-professionals think of the same treatments. Clinical professionals are, thus, more critical of the presented “gold-standard” solutions. Back Pain Workshop is not only about data: It was seen as a reliable information source that contributed especially by bundling the non-professional opinion in the same tool that offers professional advice. The clinical professionals found value in self-reflection, because the general public disagreed with their assessment of certain treatments. They also found potential in the tool in clinical use with new patients with no extensive experience with back pain. We contribute to crowdsourcing literature with a real-world case study that built diverse, reusable knowledge bases on a relevant and burdensome global issue: low back pain. In the future, we shall focus on improving this particular deployment and experimenting with other medical conditions.
"
Emotional Dialogue Generation using Image-Grounded Language Models,https://dl.acm.org/authorize?N657089,"
Computer-based conversational agents are becoming ubiquitous. However, for these systems to be engaging and valuable to the user, they must be able to express emotion, in addition to providing informative responses. Humans rely on much more than language during conversations; visual information is key to providing context. We present the first example of an image-grounded conversational agent using visual sentiment, facial expression and scene features. We show that key qualities of the generated dialogue can be manipulated by the features used for training the agent. We evaluate our model on a large and very challenging real-world dataset of conversations from social media (Twitter). The image-grounding leads to significantly more informative, emotional and specific responses, and the exact qualities can be tuned depending on the image features used. Furthermore, our model improves the objective quality of dialogue responses when evaluated on standard natural language metrics. We present the first example of an image-grounded conversational agent using visual sentiment, facial expression and scene features. We trained a novel CA on a large dataset of highly naturalistic conversations from Twitter. This model allowed for in-depth analyses of the relationships between such image information and the generated language. Specifically, we analyzed and discussed the influence that image sentiment and image content have on the sentiment and content in the responses to the visually aided questions. Evaluation on an independent set of conversations showed that including image features increased how emotional, informative and relevant the generated dialogue was judged to be. We also found that visual sentiment and facial features in the images were the primary drivers of variations in sentiment in the generated responses. In addition, scene (content) features had more influence on the topic that was generated in the output. Our proposed model also significantly outperformed the baseline on automated linguistic metrics. Grounding conversations using images is an exciting new research domain that could contribute to more natural and engaging CA. Finally, our work can benefit social media researchers as a means to discover novel insights in multimedia posts that combine imagery and dialogue.
"
"Science Everywhere: Designing Public, Tangible Displays to Connect Youth Learning Across Settings",https://dl.acm.org/authorize?N657080,"
A major challenge in education is understanding how to connect learning experiences across settings (e.g., school, afterschool, and home) for youth. In this paper, we introduce and describe the participatory design process we undertook to develop Science Everywhere (SE), which is a sociotechnical system where children share their everyday science learning via social media. Public displays installed throughout the neighborhood invite parents, adults, peers, and community members to interact with children's ideas to better develop connections for learning across settings. Our case study of community interactions with the public displays illuminate how these technologies encouraged behaviors such as the noticing of children's ideas, recognition of people in the neighborhood, and bridging to new learning opportunities for youth. This study contributes deeper insights around recent movements to design and study “smart and connected learning communities” (SCLC) [44]. The overarching vision of SCLC’s is to leverage technology (mobile, sensing, big data tools etc.) to help facilitate learning across settings, build from community needs, and “smartly” coordinate human and social capital to support learning. One might imagine a future world in which learners can find relevant learning opportunities on the fly as they move about their neighborhoods or cities. Individuals may engage in projects that are deeply meaningful to them and have impact on their local community. They may also be recognized for these activities as markers of learning. Finally, learners could – in this far future vision – find help and social support more efficiently and equitably to further spur their learning forward. How do we design to realize this far-term vision? Our current study explores a few design conjectures and illuminates some future needs. We show how technologies that deepen awareness of a neighborhood and its rich assets is a fundamental need, and our findings support past research, such as Carroll and Rosson’s work on community technologies to build connectedness [13]. We build from Erete’s framework of community technologies [23] and show that SE was well designed for social cohesion (helping people recognize each other and assets), small group interaction (within a classroom, church, or afterschool program), and interest-based groups (promoting science awareness in the neighborhood). However, we found that another major challenge is the promotion of social capital or translating this cohesion, interaction, and interest into action that can bring more resources to support children’s learning. Overall our study offers design and qualitative accounts that may be translatable for future SCLC projects. Designers can build from the design decisions we made, and the prior HCI literature on creating public displays and social media [2, 4, 31], to coordinate the sharing of learning in communities. Decisions about technology, interaction design, and the issues of designing for community practices, privacy needs, and the unique contexts of children and families will be important to realize any future initiative. Furthermore, we hope that our documentation of how we embedded SE technologies in partnership with neighborhoods, can provide templates to build upon for future community-facing, design research around SCLCs [28]. Our findings point to the complex challenge of coordinating factors such as awareness, social connection, institutions, learning practices, and social capital to realize the full potential of smart and connected learning communities of the future, that provide equitable and innovative learning ecosystems for learners everywhere.

"
"Collaborative Reflection: A Practice for Enriching Research Partnerships Spanning Culture, Discipline, and Time",https://dl.acm.org/authorize?N657081,"
All too often, research partnerships are project-driven and short-lived. Multi-lifespan design and other longer-term approaches offer alternative models. In this paper, we contribute one alternative model for cross-boundary research partnerships spanning longer timeframes and offer one best practice: collaborative reflection. Specifically, we provide an in-depth case study of a multi-lifespan design partnership (over nine years and ongoing) between a Rwandan NGO focused on peacebuilding and a US university research group focused on information design theory and method. First, we document our process for conducting a collaborative reflection that seeks balance among the contributors while navigating differences in culture, discipline, experience, and skills. Next, we reflect on five themes: (1) common ground: sensibilities and commitments; (2) trust; (3) research landscape: crossing nations and institutions; (4) research as a healing mechanism; and (5) multi-lifespan design partnership. We conclude with a discussion of overarching considerations for design researchers who engage in cross-boundary research partnership. In this paper, we have reflected on a multi-lifespan design partnership spanning culture, discipline, and time. In doing so we have developed a process for cross-boundary collaborative reflection and presented a model for what a multi-lifespan design partnership might look like. While the partnership between the NAR and VRT teams is not perfect—no partnership is—we believe we have explored and begun to develop a set of practices and reflection themes that could be adapted by other design researchers whose work engages in cross-boundary research partnership.
"
Understanding Face and Eye Visibility in Front-Facing Cameras of Smartphones used in the Wild,https://dl.acm.org/authorize?N657082,"
Commodity mobile devices are now equipped with high-resolution front-facing cameras, allowing applications in biometrics (e.g., FaceID in the iPhone X), facial expression analysis, or gaze interaction. However, it is unknown how often users hold devices in a way that allows capturing their face or eyes, and how this impacts detection accuracy. We collected 25,726 in-the-wild photos, taken from the front-facing camera of smartphones as well as associated application usage logs. We found that the full face is visible about 29% of the time, and that in most cases the face is only partially visible. Furthermore, we identified an influence of users' current activity; for example, when watching videos, the eyes but not the entire face are visible 75% of the time in our dataset. We found that a state-of-the-art face detection algorithm performs poorly against photos taken from front-facing cameras. We discuss how these findings impact mobile applications that leverage face and eye detection, and derive practical implications to address state-of-the art's limitations. In this work we collected a dataset of 25,726 photos taken from front-facing cameras of smartphone in the wild and analysed it with respect to face and eye visibility. We are happy to share the dataset for the community to build over our work; please contact the first author for a link to the dataset. Being the first dataset of its kind, we gained several insights. Most importantly, we showed that the user’s face and eyes are not always visible in the front-facing camera view, and not always detectable by state-of-the-art face detection methods. We found that the currently running application, the hand used to carry the phone, and the state of the user influenced the visibility of the user’s eyes and face. There are many cases where the user’s eyes, but not the whole face, were visible. We discussed how the results motivate the need for new computer vision algorithms that rely on the eyes only and not necessarily on the visibility of the full face. We also discussed how applications could guide the user to revealing her face to the front-facing camera, directions for exploiting the dataset and the results of this study, privacy implications, and challenges of this type of research.

"
Perspective on and Re-orientation of Physical Proxies in Object-Focused Remote Collaboration,https://dl.acm.org/authorize?N657083,"
Remote collaborators working together on physical objects have difficulty building a shared understanding of what each person is talking about. Conventional video chat systems are insufficient for many situations because they present a single view of the object in a flattened image. To understand how this limited perspective affects collaboration, we designed the Remote Manipulator (ReMa), which can reproduce orientation manipulations on a proxy object at a remote site. We conducted two studies with ReMa, with two main findings. First, a shared perspective is more effective and preferred compared to the opposing perspective offered by conventional video chat systems. Second, the physical proxy and video chat complement one another in a combined system: people used the physical proxy to understand objects, and used video chat to perform gestures and confirm remote actions. This paper explores the challenges of coordinating objectfocused collaboration when collaborators are remote from one another. Specifically, we considered how collaborators’ perspectives on an object affects the way in which they coordinate activity. We built and studied the Remote Manipulator (ReMa), which automatically orients the proxy object to reflect the orientation at a Tracking Site. We found that a shared perspective on the object is easier for people to manage compared to the default “opposing” perspective offered by conventional video chat. We also found that ReMa can be a useful aid to collaboration, easing the pressure of describing and reproducing verbal reorientation cues on an object. Finally, our analysis shows that ReMa and a video channel complement each other when used together, giving people more effective tools to coordinate their actions in object-focused collaboration. Looking forward, our results suggest ways that researchers should consider new workspaces that improve object-focused collaboration, including supporting simultaneous object manipulation and remote gesture, managing synchronized and unsynchronized object manipulation, and handling bidirectional capture and manipulation.

"
The Index of Pupillary Activity: Measuring Cognitive Load vis-à-vis Task Difficulty with Pupil Oscillation,https://dl.acm.org/authorize?N657084,"
A novel eye-tracked measure of the frequency of pupil diameter oscillation is proposed for capturing what is thought to be an indicator of cognitive load. The proposed metric, termed the Index of Pupillary Activity, is shown to discriminate task difficulty vis-a-vis cognitive load (if the implied causality can be assumed) in an experiment where participants performed easy and difficult mental arithmetic tasks while fixating a central target (a requirement for replication of prior work). The paper's contribution is twofold: full documentation is provided for the calculation of the proposed measurement which can be considered as an alternative to the existing proprietary Index of Cognitive Activity (ICA). Thus, it is possible for researchers to replicate the experiment and build their own software which implements this measurement. Second, several aspects of the ICA are approached in a more data-sensitive way with the goal of improving the measurement's performance. Being able to distinguish a user’s level of cognitive load has significant implications for design and/or evaluation of interactive systems. Measurement of cognitive load could allow a system to respond appropriately, modulating the level of task difficulty (e.g., as in e-learning systems [57]), or by adapting mission-critical systems to the user’s cognitive state [60]. We reviewed Cognitive Load Theory and its connection to task-evoked eye movement measures, namely pupillary responses. We gave a novel method of estimating frequency of pupil oscillation termed the Index of Pupillary Activity. We have discussed the limitations of pupillometric measures and suggested measurement of pupil oscillation as an alternative for estimating task difficulty vis-à-vis cognitive load.
"
Voicesetting: Voice Authoring UIs for Improved Expressivity in Augmentative Communication,https://dl.acm.org/authorize?N657095,"
Alternative and augmentative communication (AAC) systems used by people with speech disabilities rely on text-to-speech (TTS) engines for synthesizing speech. Advances in TTS systems allowing for the rendering of speech with a range of emotions have yet to be incorporated into AAC systems, leaving AAC users with speech that is mostly devoid of emotion and expressivity. In this work, we describe voicesetting as the process of authoring the speech properties of text. We present the design and evaluation of two voicesetting user interfaces: the Expressive Keyboard, designed for rapid addition of expressivity to speech, and the Voicesetting Editor, designed for more careful crafting of the way text should be spoken. We evaluated the perceived output quality, requisite effort, and usability of both interfaces; the concept of voicesetting and our interfaces were highly valued by end-users as an enhancement to communication quality. We close by discussing design insights from our evaluations. In this paper, we introduced the concept of voicesetting interfaces for allowing AAC users to explicitly control the expressive properties of synthetic speech. We contributed two novel designs for voicesetting interfaces that offer different effort/quality tradeoffs. The Expressive Keyboard treats emoji and punctuation as expressive operators over the surrounding speech, adding vocal sound effects, changing the tone of voice, and altering prosodic features such as the pitch of the voice. The Active Listening Mode of the Expressive Keyboard allows users to rapidly express reactions (i.e., vocal sound effects like laughter) while listening to others speak. The Voicesetting Editor provides users with the ability to carefully craft the way their text should be spoken. These designs were evaluated through an online questionnaire to evaluate the perceived quality of speech authored with these interfaces, and through in-lab qualitative and semi-structured feedback sessions with people with ALS.
"
Digital Outdoor Play: Benefits and Risks from an Interaction Design Perspective,https://dl.acm.org/authorize?N657096,"
Outdoor play has been proven to be beneficial for children's development. HCI research on Heads-Up Games suggests that the well-known decline in outdoor play can be addressed by adding technology to such activities. However, outdoor play benefits such as social interaction, creative thinking, and physical activity may be compromised when digital features are added. We present the design & implementation of a novel digitally-enhanced outdoor-play prototype. Our evaluation with 48 children revealed that a non-digital version of the novel outdoor play object afforded social play and game invention. Evaluation of the digitally-enhanced version showed reduced collaborative social interaction and reduced creative thinking when compared with baseline. However, we showed that specific sensing and feedback features better supported outdoor play benefits. For example non-accumulated feedback was shown to increase collaborative play and creative thinking in comparison to accumulated feedback. We provide evidence-based recommendations for designers of outdoor play technologies. This paper presents evidence-based recommendations for designers of outdoor play devices based on design, implementation and evaluation of a digitally enhanced outdoor play object. Our findings revealed the fragility of outdoor play benefits when digital features are added. We showed that different types of sensing and feedback have different impact on outdoor play. For example, Accumulated feedback strengthened competitive play while and Non-accumulated feedback promoted collaborative play and creative thinking. Our study clearly shows that designers should carefully evaluate the implications of integrating sensing and feedback to outdoor play, to make sure their devices do not compromise outdoor play benefits.

"
Leveraging Community-Generated Videos and Command Logs to Classify and Recommend Software Workflows,https://dl.acm.org/authorize?N657097,"
Users of complex software applications often rely on inefficient or suboptimal workflows because they are not aware that better methods exist. In this paper, we develop and validate a hierarchical approach combining topic modeling and frequent pattern mining to classify the workflows offered by an application, based on a corpus of community-generated videos and command logs. We then propose and evaluate a design space of four different workflow recommender algorithms, which can be used to recommend new workflows and their associated videos to software users. An expert validation of the task classification approach found that 82% of the time, experts agreed with the classifications. We also evaluate our workflow recommender algorithms, demonstrating their potential and suggesting avenues for future work. In this paper, we have proposed a hierarchical approach to classifying user workflows by first applying topic modeling to identify high-level tasks, and then applying frequent pattern mining to identify distinct command patterns for each task. An evaluation showed encouraging evidence that topic modeling can effectively categorize logs into meaningful high-level tasks. As well, the hierarchical approach appears to help identify a larger variety of distinct command patterns. Based on this approach, we proposed a design space of workflow-based recommender systems. An evaluation of four such algorithms was encouraging, and suggests that this approach has the potential to effectively support software users.
"
Towards Algorithmic Experience: Initial Efforts for Social Media Contexts,https://dl.acm.org/authorize?N657098,"
Algorithms influence most of our daily activities, decisions, and they guide our behaviors. It has been argued that algorithms even have a direct impact on democratic societies. Human - Computer Interaction research needs to develop analytical tools for describing the interaction with, and experience of algorithms. Based on user participatory workshops focused on scrutinizing Facebook's newsfeed, an algorithm-influenced social media, we propose the concept of Algorithmic Experience (AX) as an analytic framing for making the interaction with and experience of algorithms explicit. Connecting it to design, we articulate five functional categories of AX that are particularly important to cater for in social media: profiling transparency and management, algorithmic awareness and control, and selective algorithmic memory. This article has proposed the concept of algorithmic experience (AX) as a way to conceptualize the ways in which users experience systems and interfaces that are heavily influenced by algorithmic behavior. Building on Bucher [7] we can note that in current social media, AX is largely negative, as the algorithmic influence is foregrounded only when it behaves erroneously or unpredictably. Hence, we suggest that AX can be deliberately designed to foreground algorithmic behavior and increase user awareness of algorithmic influence. It is important to realize that AX and the socioeconomic dynamics of Facebook’s business are only partly aligned [10,27]. While there may be economic and commercial reasons behind the current algorithmic obscureness towards the user, we have here chosen to frame AX from a usercentered perspective. From a commercial perspective, AX may contribute towards a more joyful and faithful relationship with a service, avoiding the possible bad experiences [7,9]. By placing the user in focus, HCI can serve to support an important debate related to the political and legal issues involved around algorithms, privacy, and users’ rights. During the workshops carried out within this project, users consistently expressed that the explicit awareness of and discussion of AX gave them conceptual tools to analyze the technologies they use and to understand how they work. Participants were not previously aware of how their behavior was tracked, and the ways this influenced the Facebook service. After getting familiar with the concept of algorithmic experience, participants reported that they became more aware of their use of digital platforms and also changed their way of using Facebook in order to improve its service in accordance to their interests. The awareness of AX thus works as a “new lens” for users, for understanding and using digital technologies. The concept also can become useful in design. In this article, we suggest five particular ways in which the design of social media can cater for increasing algorithmic awareness and improving AX. Algorithmic profiling transparency provides users ways to understand what the system knows about them and how the offered results are related to that profiling. In this it is important to display both internal profiling, the implicit profiling mechanism based on the user’s behavior within the system, and to make transparent what the system is gathering from external sources such as browser tracking or third party/allied companies’ services. Algorithmic profiling management allows users to refine the profile. Explicit profile management helps users correct profiling behavior, and is an important resource also for designers as it allows for implicit and explicit feedback to personalization [5:213]. Algorithmic user-control could empower users to give them a level of control over their social media services. It includes turning off and on algorithmic interventions as well as tracking. Selective algorithmic memory offers the user a possibility to make the system forget their previous interactions and delimit their influence over the future algorithmic results. Finally, Algorithmic awareness can be directly fostered through informing users about how the algorithms work and how the user’s behavior affects its behavior. In this lies a didactical challenge to present the user with sufficient information, but at the same time not expose the social media platform to user behavior that can compromise its function or commercial viability.
"
Which one is me?: Identifying Oneself on Public Displays,https://dl.acm.org/authorize?N657099,"
While user representations are extensively used on public displays, it remains unclear how well users can recognize their own representation among those of surrounding users. We study the most widely used representations: abstract objects, skeletons, silhouettes and mirrors. In a prestudy (N=12), we identify five strategies that users follow to recognize themselves on public displays. In a second study (N=19), we quantify the users' recognition time and accuracy with respect to each representation type. Our findings suggest that there is a significant effect of (1) the representation type, (2) the strategies performed by users, and (3) the combination of both on recognition time and accuracy. We discuss the suitability of each representation for different settings and provide specific recommendations as to how user representations should be applied in multi-user scenarios. These recommendations guide practitioners and researchers in selecting the representation that optimizes the most for the deployment's requirements, and for the user strategies that are feasible in that environment. In this work, we studied how well passersby can distinguish their own representations from those of others on a large public display. We identified 5 main strategies that users employ to identify themselves in a pre-study. In a follow-up study we quantified the time and accuracy of recognizing one’s own user representation. Furthermore we introduced 6 recommendations to help designers choose the suitable user representation depending on the context of their deployments.

"
Analysis and Modeling of Grid Performance on Touchscreen Mobile Devices,https://dl.acm.org/authorize?N657090,"
Touchscreen mobile devices can afford rich interaction behaviors but they are complex to model. Scrollable two-dimensional grids are a common user interface on mobile devices that allow users to access a large number of items on a small screen by direct touch. By analyzing touch input and eye gaze of users during grid interaction, we reveal how multiple performance components come into play in such a task, including navigation, visual search and pointing. These findings inspired us to design a novel predictive model that combines these components for modeling grid tasks. We realized these model components by employing both traditional analytical methods and data-driven machine learning approaches. In addition to showing high accuracy achieved by our model in predicting human performance on a test dataset, we demonstrate how such a model can lead to a significant reduction in interaction time when used in a predictive user interface. We present an investigation into scrollable two dimensional grid UIs on touchscreen mobile devices. The study contributed a set of useful findings for understanding user behaviors on using a mobile grid UI. Based on these findings, we devised a novel predictive model for performance time where many unique characteristics of grid UIs converge into one formulation. We also discussed how such a predictive model can be used for utility-based optimization of user interfaces. The work contributes to HCI research of user performance modeling, taking steps toward modeling the complex behaviors inherent in mobile user interaction.
"
Communication Behavior in Embodied Virtual Reality,https://dl.acm.org/authorize?N657091,"
Embodied virtual reality faithfully renders users' movements onto an avatar in a virtual 3D environment, supporting nuanced nonverbal behavior alongside verbal communication. To investigate communication behavior within this medium, we had 30 dyads complete two tasks using a shared visual workspace: negotiating an apartment layout and placing model furniture on an apartment floor plan. Dyads completed both tasks under three different conditions: face-to-face, embodied VR with visible full-body avatars, and no embodiment VR, where the participants shared a virtual space, but had no visible avatars. Both subjective measures of users' experiences and detailed annotations of verbal and nonverbal behavior are used to understand how the media impact communication behavior. Embodied VR provides a high level of social presence with conversation patterns that are very similar to face-to-face interaction. In contrast, providing only the shared environment was generally found to be lonely and appears to lead to degraded communication. Embodied virtual reality and face-to-face interaction showed remarkably similar verbal and nonverbal communicative behavior, with the anticipated drop off for VR without bodies. Having a tracked body in the virtual world seems to help people feel that they are really interacting with another person: all but one subjective measure showed no significant difference for social presence between F2F and embodVR, with lower social awareness possibly reflecting the lack of facial information. There was a clear preference for including a body in the experience as people felt “alone"" in no_embodVR and ratings dropped. Removing the body decreased referential pronoun usage and lowered the frequency with which participants took conversational turns. There are, of course, limitations to the work. The first is that this study examined a particular context in which users have a shared visual work space. The activities included a negotiation task and a design task. Behavior may vary for different environments and different activities. A second limitation is that while we measure conversational behavior and subjective experience, we don’t measure the effectiveness of the conversation. Both of these issues point to interesting follow-up work. For example, it would be interesting to examine social conversation to see whether facial motion plays a more dominant role here. Facial animation was excluded from this study both due to technical limitations and in order to focus on the impact of body movement. The study also used relatively lowfidelity models. It would be interesting to see if behavior and experience changes with photo-realistic models that include facial animation.
"
"In the Eye of the Student: An Intangible Cultural Heritage Experience, with a Human-Computer Interaction Twist",https://dl.acm.org/authorize?N657092,"
We critically engage with CHI communities emerging outside the global North (ArabHCI and AfriCHI) to explore how participation is configured and enacted within socio-cultural and political contexts fundamentally different from Western societies. We contribute to recent discussions about postcolonialism and decolonization of HCI by focusing on non-Western future technology designers. Our lens was a course designed to engage Egyptian students with a local yet culturally-distant community to design applications for documenting intangible heritage. Through an action research, the instructors reflect on selected students' activities. Despite deploying a flexible learning curriculum that encourages greater autonomy, the students perceived themselves with less agency than other institutional stakeholders involved in the project. Further, some of them struggled to empathize with the community as the impact of the cultural differences on configuring participation was profound. We discuss the implications of the findings on HCI education and in international cross-cultural design projects. In this paper, we engaged with HCI movements that address overcoming colonization legacy in technology design for non-Western contexts. We took an HCI education lens to engage with future Egyptian designers in genuine open discussions about power dynamics and cultural values to better scaffold their learning experience. Our findings invite the HCI community to question power and diversity in local and global contexts as it takes a broader stance to unbalances in cross-cultural design. Our recommendations, we argue, would enrich the diversity discourse in HCI.
"
shapeShift: 2D Spatial Manipulation and Self-Actuation of Tabletop Shape Displays for Tangible and Haptic Interaction,https://dl.acm.org/authorize?N657093,"
We explore interactions enabled by 2D spatial manipulation and self-actuation of a tabletop shape display. To explore these interactions, we developed shapeShift, a compact, high-resolution (7 mm pitch), mobile tabletop shape display. shapeShift can be mounted on passive rollers allowing for bimanual interaction where the user can freely manipulate the system while it renders spatially relevant content. shapeShift can also be mounted on an omnidirectional-robot to provide both vertical and lateral kinesthetic feedback, display moving objects, or act as an encountered-type haptic device for VR. We present a study on haptic search tasks comparing spatial manipulation of a shape display for egocentric exploration of a map versus exploration using a fixed display and a touch pad. Results show a 30% decrease in navigation path lengths, 24% decrease in task time, 15% decrease in mental demand and 29% decrease in frustration in favor of egocentric navigation. Mobile shape displays combine the benefits of both actuated tangibles (dynamic UIs with controllable lateral kinesthetic haptic feedback) and actuated pin arrays (general purpose shape-change). Mobility allows us to leverage smaller shape displays to provide users with a large workspace while maintaining lower costs and complexity. Through shapeShift, we have explored the passive and active mobility of shape displays. This type of display can be used to render and manipulate static content or to physically explore 2D situated information spaces. Our user evaluation on a spatial map search task using a passively mobile shape display as navigation input shows that it helps increase search efficiency and reduce users’ cognitive load. Self-actuation of tabletop shape displays enables new forms of output such as displaying objects’ 2D spatial motion and providing lateral kinesthetic feedback. Combined with an HMD, the display can also be used as an encountered-type haptic device for VR applications. The added degrees of freedom of mobile tabletop shape displays offer an exciting area of opportunity to better leverage our haptic senses and spatial skills when interacting with digital information. We hope shapeShift’s open-source platform will inspire both designers and researchers in prototyping new interactive physical interfaces.

"
"Where is Community Among Online Learners?: Identity, Efficacy and Personal Ties",https://dl.acm.org/authorize?N657094,"
Research questions about community among online learners are gaining importance as enrollments in online programs explode. However, what community means for this context has not been studied in a comprehensive way. We contribute a quantitative study of learners' feelings and behavior expectations about online community, adapting scales for sense of community (SOC) and developing an instrument to assess community collective efficacy (CCE). Our analysis of students' responses to these scales revealed two factors underlying SOC (shared identity and interpersonal friendship) and three factors underlying CCE (identity regulation, coordination and social support). We used these factors to discuss contrasting definitions of community (shared identity versus ego networks). Exploratory data analyses also revealed relationships to other student variables that begin to articulate roles and mechanisms for online students' felt community, and raise design implications about what we might do with and for the community structure. In sum, we developed a survey instrument to assess an online learning community with respect to sense of community and collective efficacy. We studied the validity and usefulness of the instrument, both with respect to contrasting definitions of community and to relationships to other student behaviors and attitudes of interest. We contribute our empirical findings to others studying similar phenomena and are particularly interested in research that can complement this work on measuring aspects of community constructs with work on designing more effective online learning environments.

"
SpeechBubbles: Enhancing Captioning Experiences for Deaf and Hard-of-Hearing People in Group Conversations,https://dl.acm.org/authorize?N657005,"
Deaf and hard-of-hearing (DHH) individuals encounter difficulties when engaged in group conversations with hearing individuals, due to factors such as simultaneous utterances from multiple speakers and speakers whom may be potentially out of view. We interviewed and co-designed with eight DHH participants to address the following challenges: 1) associating utterances with speakers, 2) ordering utterances from different speakers, 3) displaying optimal content length, and 4) visualizing utterances from out-of-view speakers. We evaluated multiple designs for each of the four challenges through a user study with twelve DHH participants. Our study results showed that participants significantly preferred speechbubble visualizations over traditional captions. These design preferences guided our development of SpeechBubbles, a real-time speech recognition interface prototype on an augmented reality head-mounted display. From our evaluations, we further demonstrated that DHH participants preferred our prototype over traditional captions for group conversations. We propose SpeechBubbles, a real-time captioning interface with a bubble-like display to enhance DHH individuals’ group conversation experiences. We interviewed eight DHH individuals and discovered their group conversation issues, and also asked them to co-design ideal visualizations for potential captioning solutions. To better understand user preferences for the prototype design, we conducted a 12-person user study— using comprehension and comfort as factors—to explore several ideal designs for the speech bubble display and hint cues. We evaluated our prototype from the design choices on six participants. Their feedback provided potential ideas to further expand our design perspective for enhancing DHH individuals’ group conversation experiences.

"
Juxtapeer: Comparative Peer Review Yields Higher Quality Feedback and Promotes Deeper Reflection,https://dl.acm.org/authorize?N657006,"
Peer review asks novices to take on an evaluator's role, yet novices often lack the perspective to accurately assess the quality of others' work. To help learners give feedback on their peers' work through an expert lens, we present the Juxtapeer peer review system for structured comparisons. We build on theories of learning through contrasting cases, and contribute the first systematic evaluation of comparative peer review. In a controlled experiment, 476 consenting learners across four courses submitted 1,297 submissions, 4,102 reviews, and 846 self assessments. Learners assigned to compare submissions wrote reviews and self-reflections that were longer and received higher ratings from experts than those who evaluated submissions one at a time. A second study found that a ranking of submissions derived from learners' comparisons correlates well with staff ranking. These results demonstrate that comparing algorithmically-curated pairs of submissions helps learners write better feedback. This paper demonstrates how theories of learning through contrasting cases can be applied to peer review, and provides empirical evidence that comparison helps reviewers give better feedback on peers’ work, and more deeply reflect on their own. To scaffold comparative review at scale, we introduce Juxtapeer, an online platform which has been evaluated in seven courses from four institutions. Learning through examples has traditionally required careful expert curation. These results point to a future where comparing algorithmically curated examples can yield similar benefits in more diverse contexts. For example, job seekers could compare resumes from successful applicants, or newsreaders could study coverage of the same story from multiple sources. In peer review and in the real world, large corpora of examples can help develop nuanced understanding.

"
Evorus: A Crowd-powered Conversational Assistant Built to Automate Itself Over Time,https://dl.acm.org/authorize?N657007,"
Crowd-powered conversational assistants have been shown to be more robust than automated systems, but do so at the cost of higher response latency and monetary costs. A promising direction is to combine the two approaches for high quality, low latency, and low cost solutions. In this paper, we introduce Evorus, a crowd-powered conversational assistant built to automate itself over time by (i) allowing new chatbots to be easily integrated to automate more scenarios, (ii) reusing prior crowd answers, and (iii) learning to automatically approve response candidates. Our 5-month-long deployment with 80 participants and 281 conversations shows that Evorus can automate itself without compromising conversation quality. Crowd-AI architectures have long been proposed as a way to reduce cost and latency for crowd-powered systems; Evorus demonstrates how automation can be introduced successfully in a deployed system. Its architecture allows future researchers to make further innovation on the underlying automated components in the context of a deployed open domain dialog system. We introduced Evorus, a crowd-powered system conversational assistant built to automate itself over time. Informed by two phases of public field deployment and testing with real users, we iteratively designed and refined its flexible framework for open-domain dialog. We imagine a future where thousands of online service providers can develop their own chatbots, not only to serve their own users in a task-specific context, but also to dynamically integrate their services into Evorus with the help of the crowd, allowing users to interact freely with thousands of online services via a universal portal. Supporting this scale offers opportunities for future research. For example, one direction is to improve the learning framework to support third-party chatbots that also improves overtime, or to better balance between the exploitation and exploration phases (like in a multi-armed bandit problem). Evorus could also be used to collect valuable fail cases to enable third-party developers to improve their bots (i.e., when a bot was triggered, but its proposed response was rejected). Evorus has three main advantages as compared to previous approaches. First, it is a working system that can serve as a scaffold for automation over time. A core advantage of starting with a working system is that users can talk to Evorus naturally from day one, ensuring conversation quality while collecting training data for automation. Second, given the oversight of the crowd, Evorus has a high tolerance for errors from its automated components. Even an imperfect automation component (e.g., chatbots) can contribute to a conversation without hurting quality, which yields more space for algorithms to “explore” different actions (e.g., selecting a chatbot with medium confidence.) Finally, Evorus allows a mixed group of humans and bots to collaboratively hold open conversations. Most automated systems created from crowd work simply use the crowd for data; Evorus tightly integrates crowds and machine learning, and provides specific points where automated components can be introduced. This architecture allows each component to be improved, providing a common research harness on which researchers specializing in different areas may innovate and compete. For instance, “response generation” has long been developed in the NLP community; Evorus provides a natural evaluate it within a larger conversational system. The flexibility of the Evorus framework potentially allows for low cost integration between many online service providers and fluid collaboration between chatbots and human workers to form a single user-facing identity. Given the complexity of conversational assistance, Evorus is likely to be crowd-powered in part for some time, but we expect it to continue to increasingly rely on automation.

"
"""Play PRBLMS"": Identifying and Correcting Less Accessible Content in Voice Interfaces",https://dl.acm.org/authorize?N657008,"
Voice interfaces often struggle with specific types of named content. Domain-specific terminology and naming may push the bounds of standard language, especially in domains like music where artistic creativity extends beyond the music itself. Artists may name themselves with symbols (e.g. M S C RA) that most standard automatic speech recognition (ASR) systems cannot transcribe. Voice interfaces also experience difficulty surfacing content whose titles include non-standard spellings, symbols or other ASCII characters in place of English letters, or are written using a non-standard dialect. We present a generalizable method to detect content that current voice interfaces underserve by leveraging differences in engagement across input modalities. Using this detection method, we develop a typology of content types and linguistic practices that can make content hard to surface. Finally, we present a process using crowdsourced annotations to make underserved content more accessible. Voice is a rapidly growing way of interacting with consumer-facing applications. We have presented one approach to identify disadvantaged content which can be generalized to other domains. Voice interfaces are made up of components based on textual, speech, and behavioral data. Groups that are underrepresented in training data, including those with different accents or members of sociolinguistic groups that do not use the majority dialect, will be disadvantaged. Similarly, content less likely to occur in large-scale speech training corpora, may be less likely to be recognized. This makes voice applications particularly prone to biases. Our case study shows that certain genres of content are more affected. We classified 12 linguistic and stylistic practices that present problems in current voice contexts. It is crucial to discover types of content that experience issues in scalable and easy to apply ways. In our evaluation, we showed our method increased accessibility of previously disadvantaged content. Our method focuses specifically on enabling access to diverse content within the music space but this approach is extensible to many other domains. Developers are increasingly using public ASR APIs similar to what our prototype used. For example, take a developer creating an application containing many local, slang or dialectal terms, or app/company-specific terminology, or professionspecific scientific, medical, legal, industrial terms. While some domain-specialized ASR services are available (e.g. Nuance has medical and legal ASR products), for especially smaller developers with special purpose domains, these may not suffice. Similar issues will arise when automatically making apps voice-accessible; which commands will and will not work may not be clear. Terms may be comparably rare in the data that the general-purpose ASR API was trained on. This rarity in training data could then result in the ASR API transcribing more common similar-sounding phrases or words rather than the specialized terminology needed. Our method could identify these incorrect transcriptions and ensure that they still resolve to the action that the user desired.
"
Exploring the Value of Parent Tracked Baby Data in Interactions with Healthcare Professionals: A Data-Enabled Design Exploration,https://dl.acm.org/authorize?N657009,"
This paper presents a designerly exploration of the potential values of parent-tracked baby data in interactions between parents and healthcare professionals (HCPs). Where previous work has used parent-tracked data as part of the solution to a problem, we contribute by starting our design exploration from data, using it as creative material in our design process. As we intend to work towards a system that could be viable across different levels of care, we invited three different types of HCPs and five families with newborns, for a five-week situated design exploration. Facilitated by an open and dynamic data collection toolkit, parents and HCPs could together decide what data to collect. In a continuous dialogue, they reflected on the relevance of that data in their interaction. Based on this, we continuously and remotely developed two concepts. From related previous work, we saw two clear ways in which others have approached data-intensive projects. The first uses data only as part of the solution; the second departs from data and questions how it might be relevant in a given context. The latter, however, always defined what data was used upfront. In this paper, we present a third approach, based on situated and explorative data-enabled design approach. Many of our initial insights show a large overlap with those presented in previous work. However, as we moved forward and used different designs to embody these insights, they became more detailed, nuanced and, simply, different; they build on more complex insights from combinations of data in different settings that continuously evolve. In the case study, we have shown how we brought together parents, HCPs and design researchers who were enabled to actively contribute by means of the data-enabled canvas. Through this, we demonstrated how different stakeholders, as experts in their own areas, together with design researchers have used data as a creative material to allow new design interventions to emerge iteratively, while situated in real life.
"
Tangible Awareness: How Tangibles on Tabletops Influence Awareness of Each Other's Actions,https://dl.acm.org/authorize?N657000,"
Tangibles on multitouch tabletops increase speed, accuracy, and eyes-free operability for individual users, and verbal and behavioral social interaction among multiple users around smaller tables with a shared focus of attention. Modern multitouch tables, however, provide sizes and resolutions that let groups work alongside each other in separate workspaces. But how aware do these users remain of each other's actions, and what impact can tangibles have on their awareness? In our study, groups of 2--4 users around the table played an individual game grabbing their attention as primary task, while they also had to occasionally become aware of other players'actions and react as secondary task. We found that players were significantly more aware of other players'actions using tangibles than those using pure multitouch interaction, indicated by faster reaction times. This effect was especially strong with more players. We close with qualitative user feedback and design recommendations. We found that players were significantly more aware of other players'actions using tangibles than those using pure multitouch interaction, indicated by faster reaction times. This effect was especially strong with more players. We close with qualitative user feedback and design recommendations. We showed that users around a large multitouch tabletop react significantly faster to other users’ actions when those use tangibles instead of multitouch interactions. This indicates a higher awareness of others’ tangible actions. We found that this effect increased with more users, and provide some initial design guidelines for such systems from these and other qualitative findings. A within-groups study with feedback from users after experiencing both tangible and virtual conditions could help better understand how tangibles improve awareness. We chose to use tangibles only for the secondary task to isolate their effect on user awareness, and because we expect that also using them for a primary task will introduce new distractions and thus decrease their beneficial effects on awareness. A follow-up study could help verify and quantify this theory. While commercial tangibles for multitouch surfaces have started to appear, users are not generally familiar with them yet, similarly to when smartphones introduced multitouch gestures. Learning effects may thus still play a significant role when studying users interacting with tangibles. Since some participants stated that they tried to listen to the sound the tangible made when being moved, we suggest investigating further what types of subtle, inherent feedback tangibles may provide to both the user and collocated actors around a large multitouch table. Finally, eye-tracking could reveal even more precise information about users’ current locus of attention and reaction times. For example, it might show if a player who already recognized an incoming attack rather decided to catch another mole before performing his defense.

"
Grand Challenges in Shape-Changing Interface Research,https://dl.acm.org/authorize?N657001,"
Shape-changing interfaces have emerged as a new method for interacting with computers, using dynamic changes in a device's physical shape for input and output. With the advances of research into shape-changing interfaces, we see a need to synthesize the main, open research questions. The purpose of this synthesis is to formulate common challenges across the diverse fields engaged in shape-change research, to facilitate progression from single prototypes and individual design explorations to grander scientific goals, and to draw attention to challenges that come with maturity, including those concerning ethics, theory-building, and societal impact. In this article we therefore present 12 grand challenges for research on shape-changing interfaces, derived from a three-day workshop with 25 shape-changing interface experts with backgrounds in design, computer science, human-computer interaction, engineering, robotics, and material science. The field of shape-changing interfaces has greatly expanded during the last few years, showcasing novel technologies and embracing new disciplines. At the same time, the field has matured considerably, and is beginning to converge on key questions surrounding technology, design, theory-building, and societal implications. In this paper we have identified and discussed what we consider the 12 grand challenges currently facing researchers. Although there are many examples of shape-changing interfaces that provide delightful experiences and offer better ergonomics, the challenges identified need be addressed to help deliver the full benefits of shape-change. It is important that we also recognize and deal with limitations of shape-changing interfaces and temper this optimism. Shneiderman [84] discussed the ten plagues of the information age, many of which apply directly to shape-changing research. In research on proxemics, researchers have discussed sinister “dark patterns” [28], that is, applications of proxemics where users are manipulated and deceived. We appreciate the transparency and open dialogue encouraged by such work, which highlights how things can go wrong. We envision similar negative and even dangerous appropriations of shape-changing interfaces will also occur; understanding and minimising that impact crosses many of the presented challenges. We are excited about the potential of shape-changing interfaces and how they will transform interaction with computing systems. We see the identification of these grand challenges as a positive step in focussing research efforts towards this common goal. These challenges will, no doubt, evolve as technology matures, society changes, and our understanding of users increases. We hope you will join us in developing these interfaces to shape the future of human-computer interaction.
"
Investigating the Role of an Overview Device in Multi-Device Collaboration,https://dl.acm.org/authorize?N657002,"
The availability of mobile device ecologies enables new types of ad-hoc co-located decision-making and sensemaking practices in which people find, collect, discuss, and share information. However, little is known about what kind of device configurations are suitable for these types of tasks. This paper contributes new insights into how people use configurations of devices for one representative example task: collaborative co-located trip-planning. We present an empirical study that explores and compares three strategies to use multiple devices: no-overview, overview on own device, and a separate overview device. The results show that the overview facilitated decision- and sensemaking during a collaborative trip-planning task by aiding groups to iterate their itinerary, organize locations and timings efficiently, and discover new insights. Groups shared and discussed more opinions, resulting in more democratic decision-making. Groups provided with a separate overview device engaged more frequently and spent more time in closely-coupled collaboration. In summary, with our study we found trends that the overview allowed users to detect patterns of locations (F1), led to consideration of more factors through a shared understanding (F2), and eventually allowed groups to iterate their itinerary (F4). Displaying the overview on a separate device encouraged more closely-coupled collaborations (F5) which resulted in sharing of more information and more active discussions. This in return led to a more democratic decisionmaking process (F3). Whilst we observed territorial behaviour with personal devices (F6), the overview device acted as a spatial and contextual focus point for collaborators to initiate discussions (F5) and focus their attention, and aided groups in mediating their collaboration. Many previous works have proposed cross-device and multidevice interactions and systems for collaborative works, but only few have been studied in everyday practices. Whilst our study was conducted in the lab and with the focus on a specific activity – collaborative trip planning – we believe that the insights show merits for real-world collaborative multidevice settings. We showed how changing device configuration benefits groups’ exchange of information and their collaboration during their sensemaking and decision-making activity. This is promising for future work on deploying multidevice systems in everyday situations.
"
Greater than the Sum of its PARTs: Expressing and Reusing Design Intent in 3D Models,https://dl.acm.org/authorize?N657003,"
With the increasing popularity of consumer-grade 3D printing, many people are creating, and even more using, objects shared on sites such as Thingiverse. However, our formative study of 962 Thingiverse models shows a lack of re-use of models, perhaps due to the advanced skills needed for 3D modeling. An end user program perspective on 3D modeling is needed. Our framework (PARTs) empowers amateur modelers to graphically specify design intent through geometry. PARTs includes a GUI, scripting API and exemplar library of assertions which test design expectations and integrators which act on intent to create geometry. PARTs lets modelers integrate advanced, model specific functionality into designs, so that they can be re-used and extended, without programming. In two workshops, we show that PARTs helps to create 3D printable models, and modify existing models more easily than with a standard tool. 3D printing enables users to fabricate new objects. However, creating geometry that will function in the real-world is difficult, and so many users print models that others have created. Unfortunately, as shown in our workshop, nonexperts can find it difficult to customize such models, even when they are carefully parameterized. Standard tools do not explicitly express design intent, leaving a gap between form and function. PARTs is at its essence one of the first 3D modeling tools to support end-user programming concepts, thus helping to bridge this gap. PARTs brings design intent to the forefront of the design process. With functional geometry, designers can create reusable designs that capture design intent and support future modelers. In the future, we hope to build on lessons learned and explore how PARTs can support different modelers by creating customized domain specific libraries and implement more complex systems. By building more complex ways of expressing and acting on design intent, we hope to increase the engineering quality of results in parts. At the moment, PARTs is a powerful first step in enabling non-experts to make use of their common practices in a reusable fashion.

"
Calling for a Revolution: An Analysis of IoT Manifestos,https://dl.acm.org/authorize?N657004,"
Designers and developers are increasingly writing manifestos to express frustration and uncertainty as they struggle to negotiate between the possibilities that IoT technologies offer, and the ethical concerns they engender. Manifestos are defining of a 'moment of crisis' and their recent proliferation indicates a desire for change. We analyze the messages manifesto authors have for their readers. Emerging from a sense of uncertainty, these manifestos create publics for debate, demand attention and call for change. While manifestos provide potential roadmaps for a better future, they also express a deep concern and even fear of the state of the world and the role of technology in it. We highlight how practitioners are responding to unstable and rapidly changing times and detail what solutions they envision, and what conflicts these might bring about. Our analysis suggests new ways HCI might theorize and design for responsibility while attending to the perils of responsibilisation.
"
Share and Share Alike?: Social Information and Interaction Style in Coordination of Shared Use,https://dl.acm.org/authorize?N657015,"
Interfaces are commonly designed from the perspective of individual users, even though most of the systems we use in everyday life are in fact shared. We argue that more attention is needed for system sharing, especially because interfaces are known to influence coordination of shared use. In this work, we aim to deepen the understanding of this relation. To do so, we design three interfaces for a shared lighting system that vary in the type of social information they allow people to share with others and in their overall interaction style. We systematically compare longitudinal and real-life use of the interfaces, evaluating (1) people's appraisal of three types of social information and (2) the influence of an interaction style on coordination of shared use. The results disclose relations between the interface and the amount of verbal communication, consideration, and accountability. With this work, we urge the need for interaction designers to consider shared use. In this work, we have investigated how the design of an interface (in terms of social information and interaction style) can influence coordination of shared use, and we have provided in-depth insights into the social dynamics that evolved around that coordination. We found that design decisions in the interface can impact coordination by stimulating verbal communication and negotiation, by helping people to take each other into consideration and adjust their interactions accordingly, and by increasing feelings of accountability. We also found that although intention, authorship, and preference are potentially useful information types, people need the information at the right moment, should be able to estimate the relevance of the information, and interpret what a lack of information means to make the information useful in coordination of shared use. To conclude, if interfaces are designed from an individual perspective, there is a risk that they do not support people in shared settings and shared use. Therefore, we emphasize the importance of considering shared use when designing interfaces for everyday systems.

"
What's the Difference?: Evaluating Variations of Multi-Series Bar Charts for Visual Comparison Tasks,https://dl.acm.org/authorize?N657016,"
An increasingly common approach to data analysis involves using information dashboards to visually compare changing data. However, layout constraints coupled with varying levels of visualization literacy among dashboard users make facilitating visual comparison in dashboards a challenging task. In this paper, we evaluate variants of bar charts, one of the most prevalent class of charts used in dashboards. We report an online experiment (N = 74) conducted to evaluate four alternative designs: 1) grouped bar chart, 2) grouped bar chart with difference overlays, 3) bar chart with difference overlays, and 4) difference bar chart. Results show that charts with difference overlays facilitate a wider range of comparison tasks while performing comparably to charts without them on individual tasks. Finally, we discuss the implications of our findings, with a focus on supporting visual comparison in dashboards. We presented a study that evaluated four variants of multiseries bar charts in terms of their capacity for facilitating comparison tasks. Our choice of chart designs was motivated and also constrained by the context of information dashboards, where both bar charts and comparison tasks are particularly prevalent. We chose the four chart designs according to recent classifications of comparison appearing in the visualization literature [17, 24]. The results of our online experiment with 74 participants indicated that charts with difference overlays, or hybrid designs that combine aspects of juxtaposition and explicit encoding with superposition, are just as good or better than solely juxtaposition or explicit encoding based charts on individual tasks. Additionally, these hybrid designs have the advantage that they afford more tasks by combining elements of juxtaposition and explicit encoding-based designs. We discussed key observations regarding difference overlays and the potential implications of these in the context of information dashboards. Finally, we highlighted limitations of the current study and open areas for future research such as how difference overlays can be used to enhance the storytelling capabilities of information dashboards.

"
Eliciting Users' Demand for Interface Features,https://dl.acm.org/authorize?N657017,"
How valuable are certain interface features to their users? How can users' demand for features be quantified? To address these questions, users' demand curve for the sorting feature was elicited in a controlled experiment, using personal finance as the user context. Users made ten rounds of investment allocation across up to 77 possible funds, thus encountering choice overload, typical of many online environments. Users were rewarded for positive investment returns. To overcome choice overload, users could sort the alternatives based on product attributes (fees, category, fund name, past performance). To elicit their demand for sorting, the experimental design enabled users to forgo 0%-9% of their reward in return for activating the sorting feature. The elicited downward sloping demand curve suggests a curvilinear relationship between sorting use and cost. More broadly, the study offers a way to quantify user demand of UI features, and a basis for comparison between features. We conclude with a call for more HCI research using economic theory and tools. As a discipline that studies allocation of scarce resources [32], economics has developed a set of tools and methods to study people’s choices in various circumstances. With the growing constraints placed on resources such as users’ time, attention and screen real estate, HCI researchers and practitioners can benefit from using more economic theory to better understand users and inform designers.
"
The Space of Possibilities: Political Economies of Technology Innovation in Sub-Saharan Africa,https://dl.acm.org/authorize?N657018,"
HCI researchers work within spaces of possibility for potential designs of technology. New methods (e.g., user centrism); expected types of interaction (user with device); and potential applications (urban navigation) can extend the boundaries of these possibilities. However, structural and systemic factors can also foreclose them. A recent wide and shallow survey of 116 individuals involved in technology development across 26 countries in sub-Saharan Africa reveals how factors of political economy significantly impact upon technological possibilities. Monopolies, international power dynamics, race, and access to capital open or constrain technological possibilities at least as much as device-centric or user-focused constraints do. Though their thrust may have been anticipated by reference to political economic trends, the structural constraints we found were underestimated by technologists even a decade ago. We discuss the implications for technology development in Africa and beyond.
"
"""Genderfluid"" or ""Attack Helicopter"": Responsible HCI Research Practice with Non-binary Gender Variation in Online Communities",https://dl.acm.org/authorize?N657019,"
As non-binary genders become increasingly prevalent, researchers face decisions in how to collect, analyze and interpret research participants' genders. We present two case studies on surveys with thousands of respondents, of which hundreds reported gender as something other than simply women or men. First, Tumblr, a blogging platform, resulted in a rich set of gender identities with very few aggressive or resistive responses; the second case study, online Fantasy Football, yielded opposite proportions. By focusing on variation rather than dismissing non-binary responses as noise, we suggest that researchers can better capture gender in a way that 1) addresses gender variation without othering or erasing non-binary respondents; and 2) minimizes ""trolls'"" opportunity to use surveys as a mischief platform. The analyses of these two distinct case studies find significant gender differences in community dimensions of participation in both networked spaces as well as offering a model for inclusive mixed-methods HCI research. This study presents descriptive qualitative findings and quantitative analysis from two independent surveys that ask about gender and behavior in online communities. As HCI researchers continue to make use of qualitative and quantitative methods, we offer guidelines and a practical case study in the careful and ethical analyses of gender beyond the binary in survey methods. We argue that careful and sensitive study design, analysis and interpretation is an important commitment for the HCI research community. By probing our categories, we can better excavate our biases and blind spots, facilitating a more inclusive and exhaustive field of inquiry moving forward. To do so, we recommend taking up the approach of infrastructural inversion set out by [11]. In the case of gender, the binary categories of man and woman are the dominant scaffolding that shape our understanding of the social world [45]. By inverting the question to ask users to identify their categories without binary constraints, we uncovered nuance and variation among our respondents that moves beyond binary categories and instead reflects users’ range of identity and expression. We encourage quantitative and qualitative researchers within the HCI community to account for and be inclusive of marginalized (non-binary) gender populations within data sets. We hope our research is a starting point for future studies in which non-binary gender data is given more attention and inclusion. Categories, by definition, delimit what we can know about the social world. As such, on principle and for empirical reasons, we make the infrastructure of our methods and analysis categories visible.
"
"“Debrief O'Clock”: Planning, Recording, and Making Sense of a Day in the Field in Design Research",https://dl.acm.org/authorize?N657010,"
Design research is generative, intuitive, experiential, and tactical. Documenting the design research process helps to communicate these decisions, judgements, and values that are embodied in design products. Yet, practices for documenting design research are underreported in the CHI community, particularly for immersive design research field studies. We contribute the ""Debrief O'Clock"" fieldnote practice for documenting design research field studies, comprising collaborative discussion sessions and the production of written research accounts. We show how the Debrief O'Clock practice emerged in the context of a Digital Community Noticeboard project with a very remote Australian Aboriginal community, and explain three key purposes of Debrief O'Clock as: 1) an early stage data recording and analysis process; 2) a tactical manoeuvre in responsive project planning; and 3) a mechanism for personal debriefing and reflexivity. We conclude with a series of open practical, ethical, and methodological questions to advance the discussion of design research documentation practices. Design research documentation is a valuable resource for telling the story of a project through time and making visible the intuitions, judgements, and values that are embodied in design products. The contribution of this paper is to make clear that through our design fieldwork, we have found that design decisions, theories, and outcomes do not emerge from a one-off structured analysis of data gathered through systematic means. Rather, they emerge from a reflective summarising of the day’s events, and resultant tactical shifts and insights that arise in an iterative process over many days, weeks, or months. We offer the Debrief O’Clock fieldnote practice as an approach to support design fieldwork and theory development. We feel that fieldnote practices such as Debrief O’Clock can enhance design research dissemination by communicating the integrative and experiential processes that have shaped design work. We believe in the value of continuing the discussion of the practical, ethical, and methodological concerns relating to design research documentation practices, in order to learn from each other and be attentive to the blind spots in our methods.

"
Family Health Promotion in Low-SES Neighborhoods: A Two-Month Study of Wearable Activity Tracking,https://dl.acm.org/authorize?N657011,"
Low-socioeconomic status (SES) families face increased barriers to physical activity (PA)-a behavior critical for reducing and preventing chronic disease. Research has explored how wearable PA trackers can encourage increased activity, and how the adoption of such trackers is driven by people's emotions and social needs. However, more work is needed to understand how PA trackers are perceived and adopted by low-SES families, where PA may be deprioritized due to economic stresses, limited resources, and perceived crime. Accordingly, we conducted a two-month, in-depth qualitative study, exploring low-SES caregivers' perspectives on PA tracking and promotion. Our findings show how PA tracking was impacted by caregivers' attitudes toward safety, which were influenced by how they perceived social connections within their neighborhoods; and cognitive-emotional processes. We conclude that PA tracking tools for low-SES families should help caregivers and children to experience and celebrate progress. Wearable physical activity (PA) tracking use cannot be separated from the social connections and the emotional experiences that families have. Our qualitative study with families in low-socioeconomic status neighborhoods shows that the efficacy of PA trackers can be limited by caregivers’ perceptions of crime. It was the depth of social contacts that caregivers had in their neighborhoods that helped empower them to be resilient in the face of disempowering narratives of crime. Furthermore, caregivers’ explanations for their success or failure (as depicted by the PA trackers) were associated with emotional responses that could motivate or dissuade PA. Therefore, rather than presenting whether a goal was achieved or missed, we suggest that family PA trackers should guide caregivers and children to experience and celebrate progress.

"
Crowdsourcing vs Laboratory-Style Social Acceptability Studies?: Examining the Social Acceptability of Spatial User Interactions for Head-Worn Displays,https://dl.acm.org/authorize?N657012,"
The use of crowdsourcing platforms for data collection in HCI research is attractive in their ability to provide rapid access to large and diverse participant samples. As a result, several researchers have conducted studies investigating the similarities and differences between data collected through crowdsourcing and more traditional, laboratory-style data collection. We add to this body of research by examining the feasibility of conducting social acceptability studies via crowdsourcing. Social acceptability can be a key determinant for the early adoption of emerging technologies, and as such, we focus our investigation on social acceptability for Head-Worn Display (HWD) input modalities. Our results indicate that data collected via a crowdsourced experiment and a laboratory-style setting did not differ at a statistically significant level. These results provide initial support for crowdsourcing platforms as viable options for conducting social acceptability research. We reported on the results of a study comparing social acceptability ratings of HWDs interaction techniques among users in laboratory-style study and a remote crowdsourcing platform. Our results showed that, overall, data collected from both groups are similar and suggest that there is potential feasibility for running social acceptability studies of new technologies using crowdsourcing platforms if the data is treated carefully.
"
Empirical Support for a Causal Relationship Between Gamification and Learning Outcomes,https://dl.acm.org/authorize?N657013,"
Preparing for exams is an important yet stressful time for many students. Self-testing is known to be an effective preparation strategy, yet some students lack motivation to engage or persist in self-testing activities. Adding game elements to a platform supporting self-testing may increase engagement and, by extension, exam performance. We conduct a randomized controlled experiment (n=701) comparing the effect of two game elements -- a points system and a badge system -- used individually and in combination. We find that the badge system elicits significantly higher levels of voluntary self-testing activity and this effect is particularly pronounced amongst a relatively small cohort. Importantly, this increased activity translates to a significant improvement in exam scores. Our data supports a causal relationship between gamification and learning outcomes, mediated by self-testing behavior. This provides empirical support for Landers' theory of gamified learning when the gamified activity is conducted prior to measuring learning outcomes. We investigated the use of two common gamification elements, points and badges, to influence the study behavior of students prior to a high-stakes final exam. Our results showed that the presence of the game elements in the user interface of our tool caused a 37% increase in the median number of answers that students submitted (H1). Our badge system encouraged goal setting behavior, and was more effective than our points system which lacked clear targets and included components that students could not directly control. In our implementation, combining the points and badge systems did not lead to higher levels of student activity (H2). There was a strong positive relationship between the number of questions students answered during practice and their subsequent exam performance (H3), and this acted as a mediator in the relationship between gamification and exam outcomes (H4). Of particular note, the presence of the badge system led to a 23% improvement in exam scores for the most active students. This work provides unique empirical support for Landers’ theory of gamified learning and illustrates that game elements targeting productive student behaviors can be used in online tools to positively impact learning outcomes.
"
Going the Distance: Trust Work for Citizen Participation,https://dl.acm.org/authorize?N657014,"
Trust is a vital component of citizen participation-whether citizens decide to engage in opportunities for participation in local government can hinge entirely on the existence of trust between citizens and public officials. Understanding the role of trust in this space is vital for HCI and the growing area of Digital Civics which works to improve or create new modes of citizen participation. Currently, however, trust is understudied from the perspectives of public officials. This gap creates a critical blind spot as technical interventions may be mismatched to the ways trust is put into action by public officials working to support citizen participation. We begin to address this gap by presenting a broad qualitative study of how public officials in a large US city operationalize trust in citizen participation. We found trust is enacted through ongoing practices that man-age distance in relationships between public officials and city residents. Public officials view trust as essential to enable citizen participation in local government. This was made clear by how many aspects of the work performed by public officials (how a service is provided or a planning process arranged) can be constrained, limited, or enhanced depending on the level of trust (or distrust). For this reason, trust work often comes to be its own distinct mode of work entirely. Trust occurs through both interpersonal and institutional relationships that rely on different characteristics of trust such as transparency, honesty, and predictability. Across the eight salient practices of trust work we found, there was a unifying goal to traverse various manifestations of distance: distance in social familiarity, distance in domain knowledge, distance in decision making-power, distance in temporal length, distance in physical proximity and distance in hypothetical uncertainty. These distances combine to produce risk and uncertainty in citizen participation efforts. Ideally, these distances could be eliminated but more often they can only be reduced temporarily thus leaving risk in place. This is where trust comes into play—as trust is a mechanism allowing for action in the face of risk. Technology and design will have a role to play in “Going the Distance” but to do so we need to understand how trust develops within the relationships, risks and expectations that characterize citizen participation.
"
Methods for Intentional Encoding of High Capacity Human-Designable Visual Markers,https://dl.acm.org/authorize?N657025,"
Previous techniques for human-designable visual markers have focused on small encoding spaces, and assume artists do not need to encode specific bit representations. We present a general framework for human-designable visual markers for artists to encode specific bit representations in large spaces. A three-part study, conducted over three weeks, methodically evaluates the usability of different encoding methods when artists encode specific bit representations. The methods span different shape characteristics suitable for artist encoding (convexity, hollowness, number, size, and distance from centroid) and visualization tools are proposed to aid in this process. We further demonstrate that any of the methods presented may be practically used to encode a URL with the aid of a universally available database like TinyURL (rather than a task-specific database), making human-designable visual markers practical for applications such as advertisements. In this work, we have approached the problem of drawing visual markers while intentionally matching a target encoding, and developed a drawing application with built-in tools to ease that process. We presented a general framework for creating new encoding schemes based on the region adjacency tree proposed by Costanza and Huang [5], and conducted a three phase user study to evaluate several of these schemes. This study has shown that, although designers tend to prefer encoding schemes that impose few restrictions and have a low information density, it is possible for them to persevere and produce viable markers with any of the schemes tested.

"
Investigating the Impact of Annotation Interfaces on Player Performance in Distributed Multiplayer Games,https://dl.acm.org/authorize?N657026,"
In distributed multiplayer games, it can be difficult to communicate strategic information for planning game moves and player interactions. Often, players spend extra time communicating, reducing their engagement in the game. Visual annotations in game maps and in the gameworld can address this problem and result in more efficient player communication. We studied the impact of real-time feedback on planning annotations, specifically two different annotation types, in a custom-built, third-person, multiplayer game and analyzed their effects on player performance, experience, workload, and annotation use. We found that annotations helped engage players in collaborative planning, which reduced frustration, and shortened goal completion times. Based on these findings, we discuss how annotating in virtual game spaces enables collaborative planning and improves team performance. Our study continues prior research that investigated the impact of CCMs on player performance in collaborative games [34, 49, 52, 59]. We focused on the effect of using annotations on player performance, workload, and experience in distributed multiplayer games and how they engage players in collaborative planning. We found that annotation tools improved actual and perceived performance, reduced frustation, and enhanced communication. Moreover, the use of annotations increased when these annotations are visible in the gameworld and map when compared to only being visible on the map. Furthermore, we identified five different use cases for the annotations: real-time way-guiding, marking locations and objects, handwriting messages, expressing emotions, and spamming. Based on these findings, we see an opportunity to design games that focus on engaging players in collaborative planning tasks. Using annotations in games opens up further research on how different gestures and non-verbal communication can be used to facilitate remote planning and collaborations. This study helps us define potential advantages of annotation interfaces and their implications for the design of collaborative games. These annotation tools could have further benefits beyond games; for example in virtual training scenarios such as disaster response training.
"
"Let's Talk About Race: Identity, Chatbots, and AI",https://dl.acm.org/authorize?N657027,"
Why is it so hard for chatbots to talk about race? This work explores how the biased contents of databases, the syntactic focus of natural language processing, and the opaque nature of deep learning algorithms cause chatbots difficulty in handling race-talk. In each of these areas, the tensions between race and chatbots create new opportunities for people and machines. By making the abstract and disparate qualities of this problem space tangible, we can develop chatbots that are more capable of handling race-talk in its many forms. Our goal is to provide the HCI community with ways to begin addressing the question, how can chatbots handle race-talk in new and improved ways? In writing this paper, we set two essential questions to guide this work: 1) How can chatbots handle race in new and improved ways? and 2) Why is race-talk so difficult for chatbots? These questions have taken us down many paths to understand how race-talk is interwoven with technical configurations supporting chatbots. An important contribution of this research is helping HCI practitioners understand how specific technosocial configurations are fundamentally entangled with their work. By drawing together technosocial interactions involved in race-talk and hate speech relative to databases, NLP, and ML, we strive to support the development of generative technosocial solutions——like a multiplicity of chatbots that upend the all-knowing agent. Chatbots are already exacerbating social harm specific to race. In working to mitigate these harms, there is potential for novel racefocused developments for chatbots specifically and for AI generally, like building off of work in raciolinguistics. This work also makes contributions for HCI practitioners broadly concerned with identity, race, or equity in design. We demonstrate how social and technical conditions develop together in ways that must be reckoned with when forming human-machine interactions. For NLP and ML practitioners (and others who work with bots), seeing the connection between known problems and ethically critical topics like race is important. Hard problems in AI require practitioners to develop context specific solutions (i.e., focused on humor, language, or race). Staying with the trouble is not about neat resolutions. It is about embracing the complexities of our lives to enable better, though still troubled, paths forward. Clarifying a context, like race, and its manifestations can help guide these efforts. Through making tangible the abstract and disparate qualities of race and chatbots, this paper works as a synthetic guide, pointing us towards possible futures where chatbots are more capable of handling race-talk in its many forms. The one question left is, what is the racial context of your chatbot?
"
Extracting Regular FOV Shots from 360 Event Footage,https://dl.acm.org/authorize?N657028,"
Video summaries are a popular way to share important events, but creating good summaries is hard. It requires expertise in both capturing and editing footage. While hiring a professional videographer is possible, this is too costly for most casual events. An alternative is to place 360 video cameras around an event space to capture footage passively and then extract regular field-of-view (RFOV) shots for the summary. This paper focuses on the problem of extracting such RFOV shots. Since we cannot actively control the cameras or the scene, it is hard to create ""ideal' shots that adhere strictly to traditional cinematography rules. To better understand the tradeoffs, we study human preferences for static and moving camera RFOV shots generated from 360 footage. From the findings, we derive design guidelines. As a secondary contribution, we use these guidelines to develop automatic algorithms that we demonstrate in a prototype user interface for extracting RFOV shots from 360 videos. The improving quality and diversity of commercial cameras makes it possible to explore new capture configurations for a range of different applications. We focus on the task of creating video summaries of social events and propose the use of static 360 video cameras to acquire footage. A key advantage of this setup is that the cameras can capture large portions of the event in a passive way that does not require a dedicated videographer. Our work takes initial steps towards identifying common challenges and tradeoffs in extracting good RFOV shots from 360 event footage. By analyzing viewer preferences, we find that central framings and smooth camera moves are generally preferred, but in many cases, the arrangement of people and objects in the scene require that we take into account other objectives, like avoiding distractors, awkward cropping boundaries, and distortion due overly wide FOVs. In addition, our automated algorithms provide preliminary evidence that these design guidelines can be operationalized to extract good shots from 360 footage with small amounts of user input. Looking ahead, there are still many open questions around the best ways to make use of 360 cameras to produce RFOV video. However, we believe our work represents a valuable stepping stone for future research on this topic.

"
“Is More Better?”: Impact of Multiple Photos on Perception of Persona Profiles,https://dl.acm.org/authorize?N657029,"
In this research, we investigate if and how more photos than a single headshot can heighten the level of information provided by persona profiles. We conduct eye-tracking experiments and qualitative interviews with variations in the photos: a single headshot, a headshot and images of the persona in different contexts, and a headshot with pictures of different people representing key persona attributes. The results show that more contextual photos significantly improve the information end users derive from a persona profile; however, showing images of different people creates confusion and lowers the informativeness. Moreover, we discover that choice of pictures results in various interpretations of the persona that are biased by the end users' experiences and preconceptions. The results imply that persona creators should consider the design power of photos when creating persona profiles. In conclusion, we postulate that there is a tradeoff between informativeness and perceptional bias when increasing the number of information elements in persona profiles. Determining the optimum calls for awareness of how the information is perceived by the end users. Consequently, more research is needed to determine the ideal persona layout in terms of information content and type in a variety of contexts. Methods such as multivariate testing with live users can help in approaching the development of the optimal persona profiles. Moreover, results point to that, when developing personas, the end users in the organization need to be taken into account prior to deciding on the information content of the persona profile.
"
Speak Up: A Multi-Year Deployment of Games to Motivate Speech Therapy in India,https://dl.acm.org/authorize?N657020,"
The ability to communicate is crucial to leading an independent life. Unfortunately, individuals from developing communities who are deaf and hard of hearing tend to encounter difficulty communicating, due to a lack of educational resources. We present findings from a two-year deployment of Speak Up, a suite of voice-powered games to motivate speech therapy, at a school for the deaf in India. Using ethnographic methods, we investigated the interplay between Speak Up and local educational practices. We found that teachers' speech therapy goals had evolved to differ from those encoded in the games, that the games influenced classroom dynamics, and that teachers had improved their computer literacy and developed creative uses for the games. We used these insights to further enhance Speak Up by creating an explicit teacher role in the games, making changes that encouraged teachers to build their computer literacy, and adding an embodied agent. In this paper, we described ethnographic insights generated from two-weeks of fieldwork in an Indian school for children who are DHOH and our simultaneous technology iteration on the Speak Up games. This paper documents the unexpected ways in which Speak Up was used and influenced classroom dynamics, and how we incorporated those insights into the technology development process. It demonstrates the need for and documents a case study of explicitly making capacity building the focus of technology development for underserved communities. Lastly, it introduces initial findings regarding the potential for embodied physical agents to enhance educational technologies for children in resource-constrained settings. There are a few notable limitations of this research. First, the power differential and language barrier between the researcher and the community hindered our ability to get accurate information about the game usage. Secondly, the novelty aspect may have influenced how students used Speak Up – although they had used the games for two years, the presence of a researcher from America likely made them more excited than usual to use the games. Lastly, the timeline of this research – the fact that it consisted of two fieldwork visits two years apart – hindered our ability to get accurate information about and support the teachers in their game usage during the interim. This work opens exciting avenues for future research. On the ethnographic front, it motivates follow-up directions of study including: what the long-term effects of the hierarchy that develops in Speech class are; how long-term usage of the embodied physical agent impacts classroom dynamics; and how students think about and perceive value in Speak Up. On the technological front, it prompts further work on speech recognition to understand student pronunciation; one way to ensure that such research is community-centric is by using the existing games to record anonymized voice data that then informs the design of a speech recognition system. Lastly, on the capacity building front, this work motivates further research into how to measure the impact of capacity building in ICTD research, both within technological tools and in more general interactions between researchers and the community.
"
Understanding the Effect of In-Video Prompting on Learners and Instructors,https://dl.acm.org/authorize?N657021,"
Online instructional videos are ubiquitous, but it is difficult for instructors to gauge learners' experience and their level of comprehension or confusion regarding the lecture video. Moreover, learners watching the videos may become disengaged or fail to reflect and construct their own understanding. This paper explores instructor and learner perceptions of in-video prompting where learners answer reflective questions while watching videos. We conducted two studies with crowd workers to understand the effect of prompting in general, and the effect of different prompting strategies on both learners and instructors. Results show that some learners found prompts to be useful checkpoints for reflection, while others found them distracting. Instructors reported the collected responses to be generally more specific than what they have usually collected. Also, different prompting strategies had different effects on the learning experience and the usefulness of responses as feedback. This paper investigated the effects of in-video prompting on both learners and instructors. In-video prompting enables instructors to collect specific comments from learners. To understand how in-video prompting affects the learning experience, we conducted two studies with crowd workers. Results showed that different prompting strategies have different effects on the learning experience. Learners perceive that learning-centered questions are less interrupting, more enjoyable, and more helpful for learning. Interviews with instructors revealed that in-video prompting gives specific comments to them and that responses from experience-centered questions are more actionable. Instructional designers emphasized the importance of coupling question design with instructional design components for more useful feedback.

"
Force Jacket: Pneumatically-Actuated Jacket for Embodied Haptic Experiences,https://dl.acm.org/authorize?N657022,"
Immersive experiences seek to engage the full sensory system in ways that words, pictures, or touch alone cannot. With respect to the haptic system, however, physical feedback has been provided primarily with handheld tactile experiences or vibration-based designs, largely ignoring both pressure receptors and the full upper-body area as conduits for expressing meaning that is consistent with sight and sound. We extend the potential for immersion along these dimensions with the Force Jacket, a novel array of pneumatically-actuated airbags and force sensors that provide precisely directed force and high frequency vibrations to the upper body. We describe the pneumatic hardware and force control algorithms, user studies to verify perception of airbag location and pressure magnitude, and subsequent studies to define full-torso, pressure and vibration-based feel effects such as punch, hug, and snake moving across the body. We also discuss the use of those effects in prototype virtual reality applications. This paper introduces Force Jacket, a novel wearable haptic interface that can provide strong and variable forces to the upper body along with vibrotactile sensations, using pneumatically actuated airbags. This system offers the unique capability of delivering haptic actuation over large areas with a relatively low number of actuators, as compared to conventional techniques. Furthermore, the use of inflatable airbags offers the ability to apply strong static pressure to the user as well as high frequency vibrations, which is not possible with other techniques. To validate the efficacy of this approach a series of user studies have been conducted to evaluate basic human perception of this type of haptic actuation in terms of location and magnitude on the user’s body. These findings were incorporated into a haptic effects editor that allows designers and engineers to create custom higher order haptic effects, and a second users study was conducted to create a library of haptic sensations. Finally, this library of effects was used to enhance three virtual reality experiences. Ultimately the Force Jacket provides a new haptic actuation method that can deliver far more immersive experiences by engaging the whole body.

"
Toward Health Information Technology that Supports Overweight/Obese Women in Addressing Emotion- and Stress-Related Eating,https://dl.acm.org/authorize?N657023,"
Emotion- and stressed-related eating (ESRE) is associated with weight management difficulties and is more likely to affect women than men. Additionally, health information technology (HIT) for weight management tends to be less effective for women than it is for men, and less effective for people who engage in ESRE. Therefore, this study explores how HIT can support overweight/obese women curb ESRE behavior. Study participants, all adult overweight/obese women (BMI ' 25), logged dietary intake for 10 days with the Lose It! smartphone app as an elicitation exercise. Cross sectional, semi-structured interviews (N = 22) were then conducted to explore technology support needs concerning ESRE behavior. Findings revealed participants had the following needs: holistic health goal development, building motivation to achieve goals, and assistance with handling stress. Resulting HIT guidelines include supporting holistic health goals, developing and sustaining motivation, exchange of emotional support, understanding of behavior, and change in ESRE mindset. Study findings showed the unique needs of overweight/obese women for technology support in the areas of: 1) holistic health goals, 2) initiating and sustaining motivation; 3) venting and processing emotions, 4) real time eating awareness, 5) understanding behavior and changing mindset, and 6) simultaneous tracking of emotions and dietary intake to facilitate such understanding. Prevailing HIT interventions provide limited support in relation to such needs. Accordingly, findings suggest new directions for improving HIT design for ESRE, a behavior more common in women. Notably, although women make up the majority of subjects in weight loss intervention studies [21] and are the weight management market’s main target population [36], there has been a dearth of interventions that address their specific needs. Accordingly, by focusing on the needs of overweight/obese women in particular, results hold potential for improving design for the largest group of digital weight loss intervention users.
"
Parody in Place: Exposing Socio-spatial Exclusions in Data-Driven Maps with Design Parody,https://dl.acm.org/authorize?N657024,"
This paper describes the development of Parody in Place, a design parody that depicts Seattle neighborhoods with typographic arrangements derived from data generated by technology platforms such as Yelp and Zillow. The project invites inquiry into what technology corporations make matter and where, in ways that challenge the neutrality of neighborhood-based data. We designed the subject of our parody, a mock company called Dork Posters, to explore how the modes of parody by which the system operates expose socio-spatial exclusions both contested and propagated by digital platforms. Our interventions reveal shifts in response toward mapping techniques, from ambivalence to curiosity. We used Dork Posters to question reductionist techniques of data aggregation and ad hoc theories of data provenance. Our engagements also prompted reflection on the politics of measurement: how data sources shape result- ing insights and valuations. We end by discussing possibilities for expanding the design research program within human-computer interaction through parody. With Parody in Place we have sought to extend HCI’s concerns for mapping and research through design fiction to consider the sociopolitical consequences of data-driven maps. Our goal was not to surface implicit biases (e.g., through a data dump) nor to produce the best of all possible maps or mapping algorithms. Rather we sought to explore alternative ideas of mapping implicated within elite design and technology networks. We wanted to bring a sense of irony, curiosity and exploration to the process of inquiry in order to elicit conversations and insights that we would have otherwise missed. Our hope was to expose different approaches to making sense of neighborhood-based data by calling attention to contemporary narratives that define the influence of socio-spatial data on the city of Seattle, WA.
"
From Scanning Brains to Reading Minds: Talking to Engineers about Brain-Computer Interface,https://dl.acm.org/authorize?N657035,"
We presented software engineers in the San Francisco Bay Area with a working brain-computer interface (BCI) to surface the narratives and anxieties around these devices among technical practitioners. Despite this group's heterogeneous beliefs about the exact nature of the mind, we find a shared belief that the contents of the mind will someday be ""read' or ""decoded' by machines. Our findings help illuminate BCI's imagined futures among engineers. We highlight opportunities for researchers to involve themselves preemptively in this nascent space of intimate biosensing devices, suggesting our findings' relevance to long-term futures of privacy and cybersecurity. As engineers in the San Francisco Bay Area, the participants in our study sit at an historical site of techno/political power. Our technology probe indicates these engineers believe the mind is physical, and therefore amenable to sensing. What are the consequences for the rest of us? We hope our study will encourage engineers to closely examine the potential of these devices for social harm, and encourage researchers to remain closely attuned to this emerging class of consumer biosensor.
"
Crowdsourcing Exercise Plans Aligned with Expert Guidelines and Everyday Constraints,https://dl.acm.org/authorize?N657036,"
Exercise plans help people implement behavior change. Crowd workers can help create exercise plans for clients, but their work may result in lower quality plans than produced by experts. We built CrowdFit, a tool that provides feedback about compliance with exercise guidelines and leverages strengths of crowdsourcing to create plans made by non-experts. We evaluated CrowdFit in a comparative study with 46 clients using exercise plans for two weeks. Clients received plans from crowd planners using CrowdFit, crowd planners without CrowdFit, or from expert planners. Compared to crowd planners not using CrowdFit, crowd planners using CrowdFit created plans that are more actionable and more aligned with exercise guidelines. Compared to experts, crowd planners created more actionable plans, and plans that are not significantly different with respect to tailoring, strength and aerobic principles. They struggled, however, to satisfy exercise requirements of amount of exercise. We discuss opportunities for designing technology supporting physical activity planning by non-experts. Our results demonstrate that crowd workers can create exercise plans that did not significantly differ in quality from expert plans on criteria of tailoring, balance, strength and aerobic guidelines. Feedback about amount and balance helped the crowd follow the relevant guidelines while creating the plan. We find that rich user profiles and exercise databases can facilitate tailoring plans to the needs of clients and requirements of national guidelines. The crowd workers reconciled competing constraints, such as following national recommendations while also satisfying various personal needs that clients have. Techniques used in CrowdFit can successfully enable non-experts to take on tasks that otherwise performed by professional coaches.
"
Bottom-Up Imaginaries: The Cultural-Technical Practice of Inventing Regional Advantage through IT R&D,https://dl.acm.org/authorize?N657037,"
Recent HCI research on social creativity and bottom-up innovation has highlighted how concerted efforts by the government policy and business communities to develop innovation ecosystems are increasingly intertwined with IT research and development. We note that many such efforts focus on cultivating regional advantage [20] in the form of innovation hubs that are situated in and leverage distinct sociocultural histories and geographies. Cultivating regional advantage entails achieving broad consensus about what that region's advantage might be, that is, the construction of a regional advantage imaginary beyond the policies, IT supports, and practices to make it happen. Here, we document how an ongoing public debate among makers and manufacturers in Taiwan as a region-distinguished by direct engagement with design, fabrication, prototyping, and manufacturing processes-are proposing pathways toward a regional advantage that both reflects Taiwan's recent sociocultural and economic histories and also its near future aspirations. This research contributes HCI research on supporting creativity at scale. If an understanding of organizations and institutions was required to understand creative communities of practice [10], then it seems that understanding regional innovation entails an understanding of creativity as a cultural process. Specifically, cultural issues are tied up in IT innovation agendas in at least two ways. First, creativity itself is situated in cultural traditions such as the arts; and second, developing regional advantage in the form of an innovation ecology entails leveraging distinctive features of one’s culture. Focusing primarily on the latter, we have highlighted how bottom-up imaginaries of innovation are both mediated by sociotechnical systems (in this case, Facebook) and are likely to result in the development/appropriation of sociotechnical systems at scale to support these imaginaries (in this case, the hope for the next Facebook—from Taiwan). We have demonstrated the complex threads that tie IT innovation to cultural dispositions (e.g., artistic creativity, attitudes towards risk and failure, etc.), and to social policy (e.g., the pragmatic challenge posed by KPI of innovation). We saw that the pursuit of Taiwan’s regional advantage at times seemed to entail a rejection of Taiwanese culture itself, a deep ideological contradiction that revealed how IT agendas cloak themselves in mythmaking, and that these myths can be demonstrably (counter-) productive. One aspect of supporting creativity and innovation at the regional scale is, therefore, attending to the subtle yet efficacious (for better or worse) roles of ideologies and myths.
"
Balancing Privacy and Information Disclosure in Interactive Record Linkage with Visual Masking,https://dl.acm.org/authorize?N657038,"
Effective use of data involving personal or sensitive information often requires different people to have access to personal information, which significantly reduces the personal privacy of those whose data is stored and increases risk of identity theft, data leaks, or social engineering attacks. Our research studies the tradeoffs between privacy and utility of personal information for human decision making. Using a record-linkage scenario, this paper presents a controlled study of how varying degrees of information availability influences the ability to effectively use personal information. We compared the quality of human decision-making using a visual interface that controls the amount of personal information available using visual markup to highlight data discrepancies. With this interface, study participants who viewed only 30% of data content had decision quality similar to those who had full 100% access. The results demonstrate that it is possible to greatly limit the amount of personal information available to human decision makers without negatively affecting utility or human effectiveness. However, the findings also show there is a limit to how much data can be hidden before negatively influencing the quality of judgment in decisions involving person-level data. Despite the reduced accuracy with extreme data hiding, the study demonstrates that with proper interface designs, many correct decisions can be made with even legally de-identified data that is fully masked (74.5% accuracy with fully-masked data compared to 84.1% with full access). Thus, when legal requirements only allow for de-identified data access, use of well-designed interface can significantly improve data utility. For legitimate data work such as data integration and verification using PII data, different people need to have access to personal information, which sacrifices the personal privacy of those whose data is stored. Often, the primary methods for handling privacy concerns are either to restrict data access at the expense of data utility, or to open the data to more people to improve throughput and utility at the expense of reduced privacy. We study the use of data hiding and visual masking as a means of limiting the amount of PII available for human review while providing supplemental markup to help communicate essential properties needed for effective decision making. In a controlled experiment, we found evidence of tradeoffs between data restriction and decision quality. The results demonstrate that extreme limits to data disclosure can significantly reduce the quality of decision making. However, when legal requirements only allow for de-identified data access, use of an appropriate interface can significantly improve data utility, as participants achieved 74.5% accuracy with fullymasked data compared to 84.1% with unrestricted data access in the full condition. Moreover, the results demonstrate that it is possible to significantly reduce PII disclosure without noticeably affecting decision accuracy. Through the use of visual indicators of metadata and data discrepancies, participants who made data decisions while viewing only 30% of PII content had average decision quality similar to those who had full 100% access to the data. The findings of this work are important for understanding how to design privacy-preserving data systems for data workers.
"
Streets for People: Engaging Children in Placemaking Through a Socio-technical Process,https://dl.acm.org/authorize?N657039,"
In this paper, we present a socio-technical process designed to engage children in an ongoing urban design project-Streets for People-in Newcastle, UK. We translated urban design proposals developed by residents and the local authority to enable children to contribute ideas to the project. Our process comprised three stages: situated explorations and evidence gathering through digitally supported neighbourhood walks; issue mapping and peer-to-peer discussions using an online engagement platform; and face-to-face dialogue between children, residents, and the local authority through a 'Town Hall' event. We report insights gained through our engagement and show how our activities facilitated issue advocacy and the development of children's capacities, but also surfaced tensions around the agency of children in political processes. We reflect on the challenges of working in this space, and discuss wider implications for technology design and ethical questions that 'scaling up' such work might pose. This paper has reported several findings from a sociotechnical process developed to engage children in a ‘live’ urban design project. Our study demonstrates the potential for HCI to support the creation of further tools and processes to open up spaces for children’s participation in civic affairs. We suggest toolkits and ‘intergenerational’ platforms as two ways in which HCI can support innovation in this area and create a more level playing field for children with respect to participation in placemaking processes. We suggest HCI researchers can facilitate meaningful connections with civics by working with schools and within ‘live’ political processes such as Streets for People. More research is needed to further explore how we can design processes that invite collaborations between children, local authorities and citizens in placemaking and that account for the tensions of opening intergenerational participatory spaces.
"
Vanishing Importance: Studying Immersive Effects of Game Audio Perception on Player Experiences in Virtual Reality,https://dl.acm.org/authorize?N657030,"
Sound and virtual reality (VR) are two important output modalities for creating an immersive player experience (PX). While prior research suggests that sounds might contribute to a more immersive experience in games played on screens and mobile displays, there is not yet evidence of these effects of sound on PX in VR. To address this, we conducted a within-subjects experiment using a commercial horror-adventure game to study the effects of a VR and monitor-display version of the same game on PX. Subsequently, we explored, in a between-subjects study, the effects of audio dimensionality on PX in VR. Results indicate that audio has a more implicit influence on PX in VR because of the impact of the overall sensory experience and that audio dimensionality in VR may not be a significant factor contributing to PX. Based on our findings and observations, we provide five design guidelines for VR games. Our studies in this paper addressed the lack of research on the effects of audio on immersive experiences in VR. We conducted two studies: (1) A within-subjects experiment with 12 participants using a commercial horror-adventure game to study the effects of VR and monitor-display versions of the same game on PX. (2) A between-subjects study of 40 participants on the effects of audio dimensionality on PX in VR. When comparing the HMD-VR condition to the monitordisplay condition, we found that audio played a less prominent role in VR and that participants were more preoccupied with their sensory experiences in VR. In the second study, subjectivity was found in the perceived importance of audio types. Overall, our research has presented three important HCI contributions: (1) We found that visual and aesthetic factors mattered more for participants in the HMD-VR condition, while, in the monitor-display condition, they were more concerned about the pursuit of in-game progression. (2) We found that audio dimensionality does not influence PX in VR games. (3) We discussed five guidelines which emerged from our studies for developing and researching immersive VR experiences in games. This research presents an important step towards understanding effects of VR game audio on player experience.

"
Flexible Learning with Semantic Visual Exploration and Sequence-Based Recommendation of MOOC Videos,https://dl.acm.org/authorize?N657031,"
Massive Open Online Course (MOOC) platforms have scaled online education to unprecedented enrollments, but remain limited by their rigid, predetermined curricula. To overcome this limitation, this paper contributes a visual recommender system called MOOCex. The system recommends lecture videos across different courses by considering both video contents and sequential inter-topic relationships mined from course syllabi; and more importantly, it allows for interactive visual exploration of the semantic space of recommendations within a learner's current context. When compared to traditional methods (e.g., content-based recommendation and ranked list representations), MOOCex suggests videos from more diverse perspectives and helps learners make better video playback decisions. Further, feedback from MOOC learners and instructors indicates that the system enhances both learning and teaching effectiveness. We have presented MOOCex, an interactive tool that offers a flexible learning experience with dynamic recommendations and visual exploration of MOOC videos. Its recommendation engine considers sequential relationships between videos based on course syllabi, to ease the learning of concepts. Also, its visual interface supports a richer semantic representation of videos in the information space, allowing learners to quickly make sense of the recommendations and decide their next step. Three studies are conducted to evaluate different aspects of MOOCex by comparing it with traditional methods (i.e., content-based recommendation and ranked list), and the results indicate that MOOCex is effective and useful in various MOOC learning scenarios. In the future, we plan to extend our recommendation engine to including more video features such as audio and image frames, and further enhance the visual interface. We also would like to experiment with recommending and exploring video subsegments, as well as non-MOOC scenarios. Finally, we aim to conduct a deployment study to collect real-world user data, thus further evaluating both the recommender algorithm and the visual interface of MOOCex.

"
“Trust Us”: Mobile Phone Use Patterns Can Predict Individual Trust Propensity,https://dl.acm.org/authorize?N657032,"
An individual's trust propensity - i.e., a dispositional willingness to rely on others"" - mediates multiple socio-technical systems and has implications for their personal, and societal, well-being. Hence, understanding and modeling an individual's trust propensity is important for human-centered computing research. Conventional methods for understanding trust propensities have been surveys and lab experiments. We propose a new approach to model trust propensity based on long-term phone use metadata that aims to complement typical survey approaches with a lower-cost, faster, and scalable alternative. Based on analysis of data from a 10-week field study (mobile phone logs) and ""ground truth"" survey involving 50 participants, we: (1) identify multiple associations between phone-based social behavior and trust propensity; (2) define a machine learning model that automatically infers a person's trust propensity. The results pave way for understanding trust at a societal scale and have implications for personalized applications in the emerging social internet of things. In this work, we have proposed a new approach to infer individual trust propensities using phone features as an alternative to conventional methods like surveys and lab experiments. Using phone-based behavioral features allowed us to build predictive models by means of machine learning classification algorithms whose accuracy, AUC, and F1 scores were promising and encouraging. To the best of our knowledge, there has been no previous study that analyzes the link between individual trust propensity and phone-based behavioral features. Hence, these results pave way for more research on leveraging ubiquitous sensing data for understanding the interconnections between socio-mobile behavioral data and trust propensities. With further technical and ethical ground work, the proposed approach can be used for inferring trust propensities of individuals at a scale of billions of people. Hence, with the growth in mobile phone penetration, the proposed approach could have multiple implications for individuals (e.g., customized apps) and societies as they engage in higher levels of technology-mediated interactions.
"
"“Suddenly, we got to become therapists for each other”: Designing Peer Support Chats for Mental Health",https://dl.acm.org/authorize?N657033,"
Talk therapy is a common, effective, and desirable form of mental health treatment. Yet, it is inaccessible to many people. Enabling peers to chat online using effective principles of talk therapy could help scale this form of mental health care. To understand how such chats could be designed, we conducted a two-week field experiment with 40 people experiencing mental illnesses comparing two types of online chats-chats guided by prompts, and unguided chats. Results show that anxiety was significantly reduced from pre-test to post-test. User feedback revealed that guided chats provided solutions to problems and new perspectives, and were perceived as ""deep,"" while unguided chats offered personal connection on shared experiences and were experienced as ""smooth."" We contribute the design of an online guided chat tool and insights into the design of peer support chat systems that guide users to initiate, maintain, and reciprocate emotional support. In this paper, we designed an online guided chat tool with prompts based on psychotherapy skills and compared it to unguided chat. We found that guided and unguided chats reduced average symptoms of anxiety, but did so in qualitatively different ways. Guided chats were perceived as deeply valuable for gaining solutions and insights, but provoked unwanted focus on troubles in some cases. Unguided chats were experienced as smooth and easy-going, but tended toward distraction from troubles rather than emotional support. We hope that this work will motivate future digital peer interventions for mental health care.
"
Accountability in the Blue-Collar Data-Driven Workplace,https://dl.acm.org/authorize?N657034,"
This paper examines how mobile technology impacts employee accountability in the blue-collar data-driven workplace. We conducted an observation-based qualitative study of how electricians in an electrical company interact with data related to their work accountability, which comprises the information employees feel is reasonable to share and document about their work. The electricians we studied capture data both manually, recording the hours spent on a particular task, and automatically, as their mobile devices regularly track data such as location. First, our results demonstrate how work accountability manifests for employees' manual labor work that has become data-driven. We show how employees work through moments of transparency, privacy, and accountability using data focused on location, identification and time. Second, we demonstrate how this data production is interdependent with employees' beliefs about what is a reasonable level of detail and transparency to provide about their work. Lastly, we articulate specific design implications related to work accountability. This paper examined data-driven work, focusing on how employees’ ability to generate high quality data in an electrical company becomes a mechanism for negotiating work accountability. Here, work accountability refers to the level of detail and transparency that electricians provide about their work, using a CRM system. In an observationbased study of the day-to-day data work of electricians in a mid-sized electrical company, we examined: How do employees negotiate work documentation on a day-to-day basis, and how is the level of tracking adjusted accordingly? First, we demonstrated how the production of quality data is interdependent with employees' understanding of how to negotiate the level of detail and transparency about their work. As such, when electricians lack the skill to navigate how and when they provide data about their work, it is practically impossible to develop a good understanding of use and non-use, including a consistent practice around the use of CRM and acceptable workarounds. Despite the obvious risk of inconsistencies in automatic data generation, we find that middle managers are by design presented with data analysis on time approval sheets, and in some cases make use of it. From this perspective, skill matters both in terms of electricians’, CEOs’, and middle managers’ understanding of how data from tracking are produced, and how this data may be incomplete due to an ‘flexible’ incomplete practice. Skill, in other words, is a requirement for a boundary regulation process of the use of data from tracking, and cannot be assumed for any party involved. Avoiding the risk of increased tracking requires that electricians are able to conduct relevant workarounds to operate the CRM system, and thus also to be compliant by meeting the request for documentation in the CRM. Secondly, we reflect on how data about work becomes a mechanism for negotiating work accountability. Accountability, we find, is based on some negotiation capacity, even if employees cannot exactly manipulate the CRM to achieve their desired level of transparency. The backwards tracing of responsibility that is embedded in the design of the CRM system does not take into account that most work in the electrical company is collaborative in practice. This is especially true for work carried out in the larger construction sites. When electricians understand how and when their manual data entries can be combined with automatically generated data, they can effectively negotiate work accountability, meaning in this case the level of detail and transparency they provide about their work. However, the company as a whole also benefits when electricians know how to maneuver in the CRM system, since this is the basis for how the company demonstrates accountability to customers and legal authorities. Thus, we demonstrate in the paper how work accountability manifests for manual labor employees’ work as they themselves and the company rely on CRM documentation in the case of, for example, a customer complaint. Lastly, we offer specific design implications related to work accountability. Data and Design Implications The design implications of our findings reflect how the dayto-day use of CRM is socially embedded; thus, the functioning of CRM, including the documentation of work, depends on employees’ motivation to produce high-quality data, which is generally true for work tracking. It is important that employees have equal access and opportunity to develop the skills to operate as data-workers, our findings suggest: 1) Because the CRM is currently based on the tracking and documentation of individual employees – whereas work in a construction site is mainly collaborative – the scale and steps for a company to use and scrutinize the data should be clear to employees for them to stay motivated as producers of high-quality data. 2) The CRM supports data that can be aggregated and moved upstream; however, it is crucial in this process that any change made to data (for example, the invoice to clients every half hour – whereas employees check-in and check-out on the exact time) is transparent to employees and can be accounted for by the company. 3) Support and training is equally important to provide both management and employees with opportunities for the reflection and skill development needed to transition to datadriven work.
"
Food Democracy in the Making: Designing with Local Food Networks,https://dl.acm.org/authorize?N657045,"
This paper introduces the concept of 'food democracy' as a theoretical framing for HCI to engage in human-food interaction. Extending existing foci of health and environmental sustainability, food democracy requires thinking through aspects of social and economic justice, and democratic governance as directions for the study and design of technologies for alternative food movements. To exemplify food democracy, we report on field observations and interviews about the opportunities and challenges for supporting the development of local food networks with communities in deprived neighbourhoods using an online direct food marketing platform. Using a food democracy framing, we identify tensions around environmental, social, and economic goals; challenges of local food businesses operating within the existing economic paradigm; and differing perspectives on ownership and governance in the network. We discuss the need for HCI to design for systems change and propose a design space for HCI in supporting food democracy movements. This paper introduced food democracy as a useful theoretical framing for both understanding of and designing with local food networks. The proposed theoretical lens broadens HCI’s engagement with food beyond health and environmental sustainability and helps to engage with questions of social and economic justice and democratic governance of our food system. Through empirical analysis of our engagements with a variety of actors and organisations, this paper begun to explore some of the concrete challenges organisations face in developing a collaborative network for local food that aim at changing the underlying system. Digital platforms, like the Open Food Network, can support local food networks by managing order processes and aggregating demand. While such technology can increase efficiency, our findings show how locally produced food can remain inaccessible for deprived communities. We highlighted tensions between ambitions of our partners in realising ideas and values aligning with food democracy and the affordability of food. This is largely due to the fact that ethical practise still has to operate within the existing system of neoliberal market economics. Collaboration across organisations and community development to achieve a system change, faces significant hurdles. Discussions and proposals remained largely within the neoliberalist and modernist logic. We argue therefore for HCI to engage in design of technology and models of civic food networks that moves beyond the transactional interactions between ‘consumers’ and ‘producers’ towards food citizenship. We discuss that this change needs to happen through incremental changes within the current system that in the long run question the condition of the market as a ‘natural’ fact and realise alternative models. In particular, in designing with local food networks, we discuss a research and design agenda for HCI that follows the six principles of social justice within a food democracy framing. Finally, we conclude with a call for HCI to support sharing of and learning from success stories internationally.
"
Better Understanding of Foot Gestures: An Elicitation Study,https://dl.acm.org/authorize?N657046,"
We present a study aimed to better understand users' perceptions of foot gestures employed on a horizontal surface. We applied a user elicitation methodology, in which participants were asked to suggest foot gestures to actions (referents) in three conditions: standing up in front of a large display, sitting down in front of a desktop display, and standing on a projected surface. Based on majority count and agreement scores, we identified three gesture sets, one for each condition. Each gesture set shows a mapping between a common action and its chosen gesture. As a further contribution, we suggest a new measure called specification score, which indicates the degree to which a gesture is specific, preferable and intuitive to an action in a specific condition of use. Finally, we present measurable insights that can be implemented as guidelines for future development and research of foot interaction. The current work presented a three-part user elicitation study of foot gestures. The main contribution of this work is the elicitation of three user-defined foot gestures sets. In addition, we introduced and demonstrated the use of a new measure called Specification Score, which indicates how specific a gesture is to an action. This can help understand which gestures are preferable, intuitive, or most suitable to use in a specific condition. Finally, we listed several measurable insights and observations regarding the preferable and intuitive use of foot gestures, and discussed various foot gesture properties. The provided gesture sets, as well as the presented insights and observations, can be used as guidelines for developers and researchers when designing future foot-based interactive systems.

"
InfoNice: Easy Creation of Information Graphics,https://dl.acm.org/authorize?N657047,"
Information graphics are widely used to convey messages and present insights in data effectively. However, creating expressive data-driven infographics remains a great challenge for general users without design expertise. We present InfoNice, a visualization design tool that enables users to easily create data-driven infographics. InfoNice allows users to convert unembellished charts into infographics with multiple visual elements through mark customization. We implement InfoNice into Microsoft Power BI to demonstrate the integration of InfoNice into data analysis workflow seamlessly, bridging the gap between data exploration and presentation. We evaluate the usability and usefulness of InfoNice through example infographics, an in-lab user study, and real-world user feedback. Our results show that InfoNice enables users to create a variety of infographics easily for common scenarios. In this paper, we introduced InfoNice, an authoring tool for designing infographic-style data visualizations. InfoNice allows users to follow an integrated workflow of data exploration, data visualization, and visualization embellishments, hence lowering the barrier to create engaging infographic design and bridging the gap between data exploration and presentation. Through flexible data binding options, users can easily bind data with visual elements and combine them together to create infographic designs. We demonstrated the expressiveness through example graphics and conducted a user study and a real user survey to understand the use of InfoNice. In the future, we will continue releasing new versions and provide more functions to facilitate users’ design of infographics.

"
Metamaterial Textures,https://dl.acm.org/authorize?N657048,"
We present metamaterial textures---3D printed surface geometries that can perform a controlled transition between two or more textures. Metamaterial textures are integrated into 3D printed objects and allow designing how the object interacts with the environment and the user's tactile sense. Inspired by foldable paper sheets (""origami"") and surface wrinkling, our 3D printed metamaterial textures consist of a grid of cells that fold when compressed by an external global force. Unlike origami, however, metamaterial textures offer full control over the transformation, such as in between states and sequence of actuation. This allows for integrating multiple textures and makes them useful, e.g., for exploring parameters in the rapid prototyping of textures. Metamaterial textures are also robust enough to allow the resulting objects to be grasped, pushed, or stood on. This allows us to make objects, such as a shoe sole that transforms from flat to treaded, a textured door handle that provides tactile feedback to visually impaired users, and a configurable bicycle grip. We present an editor assists users in creating metamaterial textures interactively by arranging cells, applying forces, and previewing their deformation. We proposed an approach that leverages metamaterials to create transformable textures on 3D printed objects. We demonstrated the benefits of our approach in three objects and provided an interactive editor to allow researchers and users to create novel textures. We see metamaterial textures as a first step to integrate transforming textures into 3D printed objects. In the long run, we think such an approach might be relevant to disseminate more expressive haptics in everyday objects. We hope this opens new dialogs between UX and product designers and results in novel everyday objects with multiple pre-integrated textures that can be activated by the end user.

"
A Case for Design Localization: Diversity of Website Aesthetics in 44 Countries,https://dl.acm.org/authorize?N657049,"
Adapting the visual designs of websites to a local target audience can be beneficial, because such design localization increases users' appeal, trust, and work efficiency. Yet designers often find it difficult to decide when to adapt and how to adapt the designs, mainly because there are currently no guidelines that describe common website designs in various countries. We contribute the first large-scale analysis of 80,901 website designs across 44 countries, made available via an interactive web-based design catalog. Using computational image metrics to compare the ~2,000 most visited websites per country, we found significant differences between several design aspects, such as a website's colorfulness, visual complexity, the number of text areas and the average saturation of colors. Our results contribute a snapshot of web designs that users in 44 countries frequently see, showing that the design of websites with a global reach are more homogenized compared to local websites between countries. We contributed a large-scale analysis of current website designs in 44 countries. Our results demonstrate that website designs significantly differ between countries, and that these differences are predominantly driven by variations in the designs of local websites. In contrast, global websites that are popular in a number of different countries often do not localize their designs. Our work allows designers to determine whether to adapt their website to another country and how to do that. In order to support these decisions, we provided findings that suggest when it is worthwhile adapting (e.g., to appear more “local” in a given country), a dataset that includes tangible image metrics that can serve as design guidelines, and a tool that allows visually comparing websites from our 44 countries. These guidelines, orchestrated with other important website localization aspects, like translation, content, image composition, color interpretation, and symbols, will point designers into the direction of successful solutions. Our contributions will help designers engage with target audiences across the world, and adapt in an ever changing online ecosystem of local and global aesthetics.
"
As We May Study: Towards the Web as a Personalized Language Textbook,https://dl.acm.org/authorize?N657040,"
We present a system designed to enable learners of a foreign language to read materials that are personally interesting to them from the web and practice vocabulary with interactive exercises based on their past readings. We report on the results of deploying the system for one month with three classes of Dutch highschool students learning French. The students and their teacher were positive about the system and in particular about the personalization aspects that the system enables. We presented a system aimed to be a minimal viable product for a personalized language textbook that uses the web as its content source. We deployed the system with sixty high school students for one month. Based on telemetry we see that students take advantage of the possibility of personalization by reading articles that are interesting and by practicing words in exercises generated from their past readings. Based on their interactions we can see that their vocabulary is enriched with new words and the knowledge of other words is strengthened. In their feedback, the students appreciate the possibility of reading personally relevant texts and the ease of interaction with the texts that is provided by the Reader component of our system. However, they also want better ways to find personally relevant content. The teacher of the three classes thinks that such a system is critical for the modern classroom, but wants more detailed data about student activity within the system. As future work we see two salient directions. First, more work with teachers is needed to better understand how to combine the individual focus of personalized textbook with the collective experience of the learners in a classroom. Second, improved content recommenders and difficulty estimators are needed in order to provide an even better personalization of content and thus increase learner interest and motivation.
"
KnobSlider: Design of a Shape-Changing UI for Parameter Control,https://dl.acm.org/authorize?N657041,"
Physical controls are widely used by professionals such as sound engineers or aircraft pilots. In particular knobs and sliders are the most prevalent in such interfaces. They have advantages over touchscreen GUIs, especially when users require quick and eyes-free control. However, their interfaces (e.g., mixing consoles) are often bulky and crowded. To improve this, we present the results of a formative study with professionals who use physical controllers. Based on their feedback, we propose design requirements for future interfaces for parameters control. We then introduce the design of our KnobSlider that combines the advantages of a knob and a slider in one unique shape-changing device. A qualitative study with professionals shows how KnobSlider supports the design requirements, and inspired new interactions and applications. In this paper, we identified six requirements for a physical interface for flexible continuous parameters control. Based on these requirements, we designed a working prototype, KnobSlider. It combines the benefits of a physical knob and a physical slider through on-demand shape change. We evaluated the KnobSlider with target users. Results suggested future improvements of the KnobSlider and implications for shape-changing interfaces. In future work, we will iterate the design so that we can conduct a longterm field study with professionals. We will also explore novel interaction techniques based on this device.

"
BreathVR: Leveraging Breathing as a Directly Controlled Interface for Virtual Reality Games,https://dl.acm.org/authorize?N657042,"
With virtual reality head-mounted displays rapidly becoming accessible to mass audiences, there is growing interest in new forms of natural input techniques to enhance immersion and engagement for players. Research has explored physiological input for enhancing immersion in single player games through indirectly controlled signals like heart rate or galvanic skin response. In this paper, we propose breathing as a directly controlled physiological signal that can facilitate unique and engaging play experiences through natural interaction in single and multiplayer virtual reality games. Our study (N = 16) shows that participants report a higher sense of presence and find the gameplay more fun and challenging when using our breathing actions. From study observations and analysis we present five design strategies that can aid virtual reality game designers interested in using directly controlled forms of physiological input. In this work we presented breathing as a directly controlled input technique for VR games. We designed and implemented four breathing actions and two game experiences that explore different natural mappings of the actions to effects in the virtual world. Analysis of results from a study with 16 participants provided us with more insights into the pros and cons of using breathing actions. We further identified six design strategies for VR game designers interested in adding breathing actions to their games. As VR becomes broadly accepted and new forms of input emerge, we need to consider player needs to balance interaction using traditional input methods like keyboards or controllers with newer immersive physiological inputs.
"
Choosing to Help Monsters: A Mixed-Method Examination of Meaningful Choices in Narrative-Rich Games and Interactive Narratives,https://dl.acm.org/authorize?N657043,"
The potential of narrative-rich games to impact emotions, attitudes, and behavior brings with it exciting opportunities and implications within both entertainment and serious game contexts. However, effects are not always consistent, potentially due to game choices not always being perceived as meaningful by the players. To examine these perceptual variations, we used a mixed-method approach. A qualitative study first investigated meaningful game choices from the players' perspectives. Building on the themes developed in this first study, a quantitative study experimentally examined the effect of meaningful game choices on player experiences of appreciation, enjoyment, and narrative engagement. Results highlight the importance of moral, social, and consequential characteristics in creating meaningful game choices, which positively affected appreciation. Meaningfulness of game choices may therefore be crucial for narrative-rich games and interactive narratives to impact players. Using a mixed-methods approach, meaningful choices in narrative-rich games and their impact on the effect of making a choice on appreciation in narrative-rich games were examined through two studies. The results of the qualitative study 1 revealed meaningful choices in participants’ accounts to be defined by the presence of social and moral characteristics and a belief that their choices impacted the resulting immediate consequences. The results of the quantitative experimental study 2 found the presence of these mechanics to significantly impact appreciation as the experience of meaningfulness, thereby validating the findings of study 1. Furthermore, the finding that making a choice had an effect on appreciation only for the high-meaningful choice condition supports the argument that adding a choice as a form of narrative interactivity is not a silver bullet, but may only be effective when the choices the player can make are perceived as meaningful.
"
Support for Social and Cultural Capital Development in Real-time Ridesharing Services,https://dl.acm.org/authorize?N657044,"
Today's transportation systems and technologies have the potential to transform the ways individuals acquire resources from their social networks and environments. However, it is unclear what types of resources can be acquired and how technology could support these efforts. We address this gap by investigating these questions in the domain of real-time ridesharing systems. We present insights from two qualitative studies: (1) a set of semi-structured interviews with 13 Uber drivers and (2) a set of semi-structured interviews with 13 Uber riders. Our results show that both drivers and riders acquired and benefited from informational, emotional and instrumental resources, as well as cultural exchanges via interactions with each other and with online platforms. We argue that these interactions could support the development of social and cultural capital. We discuss our findings in the context of labor and contribute design implications for in-car social and cultural experiences and for the ways technologies such as GPS and location-based services can support the additional emotional, social, and cultural labor that drivers provide to their riders. We took a user-centered approach to identify opportunities for technology to incorporate support for and enhance driver-rider interaction. We contribute results from two qualitative studies of drivers and riders to understand opportunities for technology to foster the development of social and cultural capital in the context of real-time ridesharing systems. Our findings show that both drivers and riders gained informational, emotional, and instrumental resources, and companionship by being part of the real-time ridesharing ecosystem. Our contributions suggest future research opportunities to explore. For example, there are open opportunities to facilitate drivers’ support of other drivers. Moreover, both riders and drivers valued and mutually benefited from the cultural exchanges, suggesting that there are opportunities to grow cultural capital within the ridesharing ecosystem. While we conclude based on the interactions that there are opportunities for the growth of social and cultural capital in the case of ridesharing, the acquisition of social and cultural capital is itself contingent on several individual and community characteristics. Ethnicity, socio-economic backgrounds, and educational attainment [41] are some factors that determine whether individuals can take advantage of and benefit from these opportunities. Examining how these factors played into the opportunities created and whether and how they were turned into something beneficial by drivers and riders is a potential next step for our research. Our results also suggest opportunities for future research to explore ways to acknowledge drivers’ additional labor. Are there ways in which systems could acknowledge drivers’ additional labor to extend in offline contexts? Could on-demand platforms be integrated with OpenBadges2 to show the skills drivers demonstrated? While facilitating support for drivers is crucial to enhancing real-time ridesharing systems, it is important to consider the future of the ecosystem. For example, autonomous vehicles equipped with driver assistance will change the nature of driver roles and their function within ridesharing services. The potential loss of this stakeholder in the ecosystem must also be taken into consideration. Drivers are central to the opportunities that exist for riders’ acquisition of social and cultural capital. How might this loss affect the modified driver roles?
"
Are You Dreaming?: A Phenomenological Study on Understanding Lucid Dreams as a Tool for Introspection in Virtual Reality,https://dl.acm.org/authorize?N657155,"
Virtual reality (VR) is resurging in popularity with the advancement of low-cost hardware and more realistic graphics. How might this technology help others? That is, to increase mental well-being? The ultimate VR might look like lucid dreaming, the phenomenon of knowing one is dreaming while in the dream. Lucid dreaming can be used as an introspective tool and, ultimately, increase mental well-being. What these introspective experiences are like for lucid dreamers might be key in determining specific design guidelines for future creation of a technological tool used for helping people examine their own thoughts and emotions. This study describes nine active and proficient lucid dreamers' representations of their introspective experiences gained through phenomenological interviews. Four major themes emerged: sensations and feelings, actions and practices, influences on experience, and meaning making. This knowledge can help design a VR system that is grounded in genuine experience and preserving the human condition. Virtual reality could be the platform that allows its users to explore introspection and have a positive impact on wellbeing because of its potential to provide an immersive space that feels real despite its user knowing it is not. A completely immersive VR experience draws may parallels to a real life “virtual” phenomenon people already experience, namely lucid dreaming. By using qualitative research methods to study introspection in lucid dreaming, we described the essence of this phenomenon as well as the context in which it occurs. The descriptions generated were used to develop design considerations for an immersive virtual reality system that will be used as a tool for introspection. From preliminary findings, there are four themes: Sensations and Feelings, Actions and Practices, Influences on Experience, and Meaning Making. Overall, it seems like lucid dreaming can be used as an effective tool for introspection and can be helpful as a new lens in which to look through in designing for introspective virtual reality experiences.
"
A Large-Scale Study of iPhone App Launch Behaviour,https://dl.acm.org/authorize?N657156,"
There have been many large-scale investigations of users' mobile app launch behaviour, but all have been conducted on Android, even though recent reports suggest iPhones account for a third of all smartphones in use. We report on the first large-scale analysis of app usage patterns on iPhones. We conduct a reproduction study with a cohort of over 10,000 jailbroken iPhone users, reproducing several studies previously conducted on Android devices. We find some differences, but also significant similarities: e.g. communications apps are the most used on both platforms; similar patterns are apparent of few apps being very popular but there existing a 'long tail' of many apps used by the population; users show similar patterns of 'micro-usage'; almost identical proportions of people use a unique combination of apps. Such similarities add confidence but also specificity about claims of consistency across smartphones. As well as presenting our findings, we discuss issues involved in reproducing studies across platforms. Study reproduction is a valuable practice in HCI. Not every piece of research can consider all groups of users or study a phenomenon over many years, so each study is likely limited to considering a relatively small number of users at one time. For the field to keep learning, it is necessary to determine whether results still hold, among different subpopulations, or as environments evolve. We have attempted to reproduce previous studies on the use of Android, hoping to gain a general sense of whether findings would also generalise to our users, and the extent to which such comparisons were even possible. We collect data from jailbroken iOS users, who form an interesting cohort in itself, but we do not claim our findings would generalise to all iOS users. We present our findings based around 3 hypotheses. We found evidence to support H1, our hypothesis that users would behave very similarly across platforms among strongly observed trends such as apps for communication being the most used on both platforms, proportionately highest in the afternoon and evening; most apps being used by a small proportion of users, and only few apps used highly frequently; over 99% of users differentiable by their app signatures over a 45 day period, and micro-usage behaviour being within variation spreads already seen [13]. We found a more mixed message in examining H2, where we assessed whether the jailbroken iOS users would be more intense device users and be regular eager experimenters with new apps. Our users did use apps for longer, across most categories, and they combined more apps into their usage sessions, but on average they used a smaller set of apps over 45 days than users in the previous Android study, Our third hypothesis was quite strongly refuted by our data – there is not a lesser degree of overlap in the used apps of our users, and Welke et al.’s results hold across platforms. For many trends, therefore, it seems that even though we have recruited users on a different platform, from a specific subpopulation, there is a high degree of similarity in usage, and we can be more confident in claiming that these patterns are consistent across smartphones in general. However, our reproduced studies have uncovered enough distinct behaviour to suggest that there is heterogeneity of use across platforms. Having identified which areas show distinct usage patterns, we also offer a challenge for future work: to further explore these, possibly through qualitative methods, to ascertain why results appear as they do. Moreover, we propose that, in designing for future generations of smartphones, it is not sufficient to constantly draw implications from the same platform-specific subset of users. We also discuss challenges encountered in our (and other) comparison studies. The two platforms under consideration share enough in terms of fundamental design that we could simply compare broad concepts such as counts of apps or usage durations. More difficulties arose in terms of specific app availability, categorisation, sample matching, data logging intricacies or varying ethical practices across institutions. However, many of these issues are not unique to cross-platform studies, and could arise in any attempts to re-examine previously published results. We propose a number of responses to such challenges, some technical (e.g. developing standardised frameworks for experimental recruitment, and for data representation and processing) and some practical (e.g. more continuous and communal approaches to such experimentation). We hope to contribute, along with others, to such future developments.
"
Physical Keyboards in Virtual Reality: Analysis of Typing Performance and Effects of Avatar Hands,https://dl.acm.org/authorize?N657157,"
Entering text is one of the most common tasks when interacting with computing systems. Virtual Reality (VR) presents a challenge as neither the user's hands nor the physical input devices are directly visible. Hence, conventional desktop peripherals are very slow, imprecise, and cumbersome. We developed a apparatus that tracks the user's hands, and a physical keyboard, and visualize them in VR. In a text input study with 32 participants, we investigated the achievable text entry speed and the effect of hand representations and transparency on typing performance, workload, and presence. With our apparatus, experienced typists benefited from seeing their hands, and reach almost outside-VR performance. Inexperienced typists profited from semi-transparent hands, which enabled them to type just 5.6 WPM slower than with a regular desktop setup. We conclude that optimizing the visualization of hands in VR is important, especially for inexperienced typists, to enable a high typing performance. Consumer virtual reality is still in the fledgling stages and mainly targets entertainment use cases. We have shown the potential of VR for a wide variety of use cases by enabling natural generic text input on a physical keyboard while being immersed in a virtual environment. Our apparatus comprises a calibration free, low latency, and accurate finger tracking with a state of the art head mounted display. Thus we can create virtual environments allowing for effortless typing in VR. We conducted a text input study with a total of 32 typists of different skill levels and tested their typing performance based on various hand representations and transparency in a virtual environment. In the study, we found no significant difference in typing speed for the experienced typists using avatar hands. Inexperienced typists require hand visualizations to orient themselves in VR while transparency has no effects. They were just 5.6 WPM slower in VR using transparent realistic hands. Further, results show that all typists benefit from seeing a representation of their hands during non-contiguous typing. Results show that experienced typists are less affected by different hand rendering. For all typists, realistic hands rendering are in favor to generates the highest presence with the lowest workload. A new combination of mobile HMDs, advanced finger trackers and a keyboard could allow us to have a truly mobile office. For the future, we envision working in well-known but highly flexible virtual environments completely independent of where we are physically located.

"
“Bursting the Assistance Bubble”: Designing Inclusive Technology with Children with Mixed Visual Abilities,https://dl.acm.org/authorize?N657158,"
Children living with visual impairments (VIs) are increasingly educated in mainstream rather than special schools. But knowledge about the challenges they face in inclusive schooling environments and how to design technology to overcome them remains scarce. We report findings from a field study involving interviews and observations of educators and children with/without VIs in mainstream schools, in which we identified the ""teaching assistant bubble"" as a potential barrier to group learning, social play and independent mobility. We present co-design activities blending elements of future workshops, multisensory crafting, fictional inquiry and bodystorming, demonstrating that children with and without VIs can jointly lead design processes and explore design spaces reflective of mixed visual abilities and shared experiences. We extend previous research by characterising challenges and opportunities for improving inclusive education of children with VIs in mainstream schools, in terms of balancing assistance and independence, and reflect on the process and outcomes of co-designing with mixed-ability groups in this context. We interviewed and observed educators and children living with visual impairments in mainstream schools in order to identify challenges facing inclusion and areas that lend themselves to effective technological intervention. Our field study presents rich descriptions of how existing educational and social support structures could lead to “teaching assistance bubbles” with undesirable effects on group learning, social play and independent mobility. As schools move towards more technology in the classroom, technological support for children with special educational needs should move beyond accessibility concerns to consider effective support for inclusive interactions. We suggested areas where technology could more usefully intervene to promote inclusion by characterising the subtle balance between accessibility and independence in terms of bridging space, language, and material disconnects. Our adaptations of co-designing methods for children with mixed visual abilities demonstrate the effectiveness of opening up novel design spaces and producing conceptual designs for technology that can accommodate and augment the varied abilities present in inclusive schooling environments.

"
Review of Intrinsic Motivation in Simulation-based Game Testing,https://dl.acm.org/authorize?N657159,"
This paper presents a review of intrinsic motivation in player modeling, with a focus on simulation-based game testing. Modern AI agents can learn to win many games; from a game testing perspective, a remaining research problem is how to model the aspects of human player behavior not explained by purely rational and goal-driven decision making. A major piece of this puzzle is constituted by intrinsic motivations, i.e., psychological needs that drive behavior without extrinsic reinforcement such as game score. We first review the common intrinsic motivations discussed in player psychology research and artificial intelligence, and then proceed to systematically review how the various motivations have been implemented in simulated player agents. Our work reveals that although motivations such as competence and curiosity have been studied in AI, work on utilizing them in simulation-based game testing is sparse, and other motivations such as social relatedness, immersion, and domination appear particularly underexplored. We have overviewed the common intrinsic motivations and psychological needs discussed in the literature on motivation and games. Subsequently, we have reviewed how these motivations have been implemented in intelligent agents. A primary strategy is to have an intrinsic motivation module that translates the agent’s observations and extrinsic rewards into intrinsic rewards, which the agent then attempts to maximize. In effect, such an intrinsic motivation module usually implements emotions through one or more computational appraisals such as an evaluation of the predictability of the reward. The appraisals are then combined and encoded in the intrinsic reinforcement signal; in essence, this means that positive emotions are treated as intrinsic rewards, and negative emotions are treated as punishment that the agents avoid. Of the common intrinsic motivations – competence, autonomy, social relatedness, curiosity, immersion, domination – curiosity appears to be the most often utilized one both in general AI and game-playing agents, implemented through appraisals of unpredictability of the rewards or observations. Together with the need for competence and challenges, curiosity helps both real organisms and simulated agents explore and learn even if the extrinsic rewards are rare or otherwise do not provide strong guidance. Another common computational appraisal is information-theoretic empowerment, denoting the magnitude of change the agent can have in its sensory inputs through its actions. Empowerment has been suggested to be related to both competence and autonomy [24], and it may also be linked to the domination motivations identified in games, as being able to dominate others enables new degrees of freedom for controlling a social environment. Work on social relatedness and in particular immersion and domination is sparse, with only a few examples. In general, despite the prevalence of the term ""intrinsic motivation"", intrinsically motivated AI seems to in practice focus more on appraisal theories of emotion than intrinsic motivation theories such as Self-Determination Theory. This is understandable, as appraisal theory provides more clear concepts that can be implemented as AI code, although it might be less well known in the game research community than player type research or Self-Determination Theory. In the domain of player modeling, research has utilized both explicit motivation models based on computational appraisals, and implicit models learned from player data. Explicit models appear less prevalent; in the end, we found surprisingly few papers to include in this review. What also appears to be missing is a combination of both approaches: explicit models validated and fine-tuned based on real player data. We believe that such models have the potential to provide both the high accuracy of data-driven models and high versatility and generalization capabilities of data-free models. Explicit computational appraisals also have the benefit of acting both as behavior drivers and as measures of the agent’s affective state. Thus, they could be used for both testing for player behavior and player experience, which could perhaps be combined with or substituted for the expressive but labor-intensive physiological player experience research methods such as Biometric Storyboards [56]. To enable developing and validating better computational models of motivation and emotion, closer collaboration of AI and player experience researchers is called for, with the goal of compiling rich datasets and benchmarks. Such datasets should include time-stamped data streams with enough temporal resolution, e.g., gameplay videos, game event logs, and affect signals such as Galvanic Skin Response, Heart-rate Variability, and player facial expressions. Further, the datasets should be collected from games that allow the integration of custom AI agents, e.g., through the OpenAI Gym interface [12]. To the best of our knowledge, no current publicly available dataset satisfies all the criteria.
"
Semi-Automated Coding for Qualitative Research: A User-Centered Inquiry and Initial Prototypes,https://dl.acm.org/authorize?N657150,"
Qualitative researchers perform an important and painstaking data annotation process known as coding. However, much of the process can be tedious and repetitive, becoming prohibitive for large datasets. Could coding be partially automated, and should it be? To answer this question, we interviewed researchers and observed them code interview transcripts. We found that across disciplines, researchers follow several coding practices well-suited to automation. Further, researchers desire automation after having developed a codebook and coded a subset of data, particularly in extending their coding to unseen data. Researchers also require any assistive tool to be transparent about its recommendations. Based on our findings, we built prototypes to partially automate coding using simple natural language processing techniques. Our top-performing system generates coding that matches human coders on inter-rater reliability measures. We discuss implications for interface and algorithm design, meta-issues around automating qualitative research, and suggestions for future work. Could parts of qualitative coding be automated, and should they be? To answer these questions, we conducted a user study, interviewing researchers and observing them code. We found that across disciplines, researchers follow several practices well-suited to automation. Researchers desire automation, but only after having developed a codebook and coded a subset of data, particularly in extending their coding to unseen data. They also require any assistive tool to be transparent about its recommendations. Based on our findings, we built prototypes to partially automate coding using simple NLP techniques. Our top-performing system generates coding that performs as well as human coders on inter-rater reliability measures. We present implications for interface and algorithm design.
"
An Eye For Design: Gaze Visualizations for Remote Collaborative Work,https://dl.acm.org/authorize?N657151,"
In remote collaboration, gaze visualizations are designed to display where collaborators are looking in a shared visual space. This type of gaze-based intervention can improve coordination, however researchers have yet to fully explore different gaze visualization techniques and develop a deeper understanding of the ways in which features of visualizations may interact with task attributes to influence collaborative performance. There are many ways to visualize characteristics of eye movements, such as a path connecting fixation points or a heat map illustrating fixation duration and coverage. In this study, we designed and evaluated three unique gaze visualizations in a remote search task. Our results suggest that the design of gaze visualizations affects performance, coordination, searching behavior, and perceived utility. Additionally, the degree of task coupling further influences the effect of gaze visualizations on performance and coordination. We then reflect on the value of gaze visualizations for remote work and discuss implications for the design of gaze-based interventions. In this work, we evaluated three unique gaze visualizations across the same visual search task with varying degrees of task coupling. The results of this study demonstrated that the design of gaze visualizations play a critical role in how they are used to support coordination. Furthermore, the properties of the task determined how the gaze visualizations impact attention. We have identified availability of the visualization and amount of information displayed as important features of visualizations that influence how pairs allocate attention and coordinate with each other. The properties of the task can determine which features are most appropriate to support effective collaboration. As remote collaboration continues to grow in popularity and find new applications it is important to consider the role of non-verbal cues in facilitating communication. Gaze visualizations can be an effective tool for enhancing communication in a variety of contexts. A broader understanding of the features of gaze visualizations can help designers adapt to new contexts quickly and effectively use gaze visualizations to support specific task goals.
"
Assisting Students with Intellectual and Developmental Disabilities in Inclusive Education with Smartwatches,https://dl.acm.org/authorize?N657152,"
Smartwatches have a large potential to support everyday activities. However, their potential as assistive technologies in inclusive academic environments is unclear. To investigate how smartwatches can support students with intellectual and developmental disabilities (IDDs) to perform activities that require emotional and behavioral skills and involve communication, collaboration and planning, we implemented WELI. WELI (Wearable Life) is a wearable application designed to assist young adults with IDDs attending a postsecondary education program. This paper reports on the user-centric design process adopted in the development of WELI, and describes how smartwatches can assist students with IDDs in special education. The results reported are drawn from 8 user studies with 58 participants in total. WELI features include behavioral intervention, mood regulation, reminders, checklists, surveys and rewards. Results indicate that several considerations must be taken into account when designing for students with IDD, and that overall the students are enthusiastic about adopting an innovative smartwatch application in class, as they reacted positively about the technology and features provided. To assist young adults with intellectual and developmental disabilities (IDDs) and help them to become more independent and engaged in a post-secondary inclusive program, we designed and developed WELI. WELI is available for Android smartphones and Android smartwatches, serving as a complementary tool for assistants who send wearable notifications to the students using a mobile application. The features provided include reminders for focus, silence, participation, rewards, countdown, checklist, survey and self-assessment. The application requirements were elicited and refined through eight user studies conducted with students with IDDs, their assistants and experts in special education. By providing help to the students and facilitating the work of the assistants, WELI enables students to depend less on the assistants in class, improving their self-confidence. The results of our usability study reveal that designing for students with IDDs require special considerations to accommodate their needs, but also that students are excited about using novel wearable technologies in class to assist them. In addition to that WELI requires no training for the students to learn how to interact with the watch. All the features provided by WELI received positive feedback, particularly the rewards feature. As future work, we will conduct long-term field studies involving students and assistants to better assess how WELI impacts their daily routines in inclusive classes, to gather additional feedback about the application. We will also add features to help the students with IDDs to give a presentation in class and to help the assistants to take notes. Further customization options will be added for personalization. Moreover, we will explore the use of the watch sensors to provide smart and automated support, for instance to suggest breaks for the students when they are feeling sleepy in class. Finally, we plan to extend WELI to cover activities outside of the class.

"
Making as Expression: Informing Design with People with Complex Communication Needs through Art Therapy,https://dl.acm.org/authorize?N657153,"
There is a growing emphasis on designing with people with diverse health experiences rather than designing for them. Yet, collaborative design becomes difficult when working with individuals with health conditions (e.g., stroke, cancer, abuse, depression) that affect their ability or willingness to engage alongside researchers and verbally express themselves. The present paper analyzes how the clinical practice of art therapy engages these individuals in co-creative, visual expression of ideas, thoughts, and experiences. Drawing on interviews with 22 art therapists and over two years of field work in a clinical setting, we detail how art therapists view making as expression for people with complex communication needs. Under this view, we argue that art therapy practice can inspire collaborative design engagements by understanding materials as language, creating space for expression, and sustaining expressions in a broader context. We discuss practical and ethical implications for design work involving individuals with complex communication needs. There is a growing interest in involving individuals with complex communication needs in design work, particularly around the creation and use of new technologies. Current strategies for engaging these individuals often focus on bridging gaps in communication left by an impairment or stigmatizing experience. At times, we narrowly scope and limit the conversational space so that we, as researchers, can understand. We argue that art therapy provides a model for how the research community can view making as expression and shift the terms of communication to understand what is most important to participants and points of connection for future design work. Further, art therapy provides a lens on the privileged position of verbal discourse in collaborative design work and opens up new avenues for fostering supportive and experience affirming co-creative design engagements.
"
The Unexpected Entry and Exodus of Women in Computing and HCI in India,https://dl.acm.org/authorize?N657154,"
In India, women represent 45% of total computer science enrollment in universities, almost three times the rate in the United States, where it is 17%. At the same time, women make up an estimated 25-30% of the HCI community in India, half the rate in the U.S. We investigate the complexities of these surprising phenomena through qualitative research of Indian computer science and human-computer interaction researchers and professionals at various life stages. We find among other things that Indian familial norms play a significant role in pressuring young women into computing as a field; that familial pressures and workplace discrimination then cause a precipitous exit of women from computing at the onset of marriage; and that HCI occupies an interstitial space between art and technology that affects women's careers. Our findings underscore the societal influence on women's representation in the tech sector and invite further participation by the HCI community in related questions. We presented the gender-related complexities that Indian women face in computing and HCI throughout their life time, drawing from qualitative research with Indian researchers, practitioners, and students. Against a backdrop of entrenched gender discrimination and relatively low literacy rates, India provides a interesting case study of women in computing. Cultural and economic expectations around working in the software industry forged supportive environments for women in the undergraduate and Master’s levels (45% women in undergraduate CS programs); but swiftly devolved into discrimination and eventual exit from Ph.D., academia, and IT jobs (where women constitute 80% of entry-level jobs). Specifically, societal stigma against highly qualified women, limited female role models for the family, unfriendly work policies, and delayed promotions affect professional progress. HCI suffers from the converse effect—in viewing HCI education as arts-centric, the field was viewed as less desirable to enter the IT force (only an estimated 25-30% women in HCI programs). Over and above the perception, women were discriminated in the core aspects of HCI research, such as fieldwork, brainstorming, and design critiques. Future work could extend inquiry to lower-tier colleges, other gender-inequal countries, and include parents and partners of women. We hope that by bringing attention to the societal influence on women’s education and careers, we can move towards more equitable computing and HCI for all.
"
ProtoAR: Rapid Physical-Digital Prototyping of Mobile Augmented Reality Applications,https://dl.acm.org/authorize?N657165,"
The latest generations of smartphones with built-in AR capabilities enable a new class of mobile apps that merge digital and real-world content depending on a user's task, context, and preference. But even experienced mobile app designers face significant challenges: creating 2D/3D AR content remains difficult and time-consuming, and current mobile prototyping tools do not support AR views. There are separate tools for this; however, they require significant technical skill. This paper presents ProtoAR which supplements rapid physical prototyping using paper and Play-Doh with new mobile cross-device multi-layer authoring and interactive capture tools to generate mobile screens and AR overlays from paper sketches, and quasi-3D content from 360-degree captures of clay models. We describe how ProtoAR evolved over four design jams with students to enable interactive prototypes of mobile AR apps in less than 90 minutes, and discuss the advantages and insights ProtoAR can give designers. This paper presents a new augmented reality (AR) tool, ProtoAR, specifically designed with interaction designers in mind. ProtoAR comes with a visual editor that composes AR interfaces from multiple layers with 2D/3D content and an app that provides a live preview of the AR interface in realtime. Over a series of four AR design jams with interaction design students, we learned about the strengths and weaknesses of both physical prototyping with paper and Play-Doh and digital prototyping based on these materials. Future work should be dedicated to adding support for explicit user interaction using touch and gesture. As in previous work [24], we argued that a lot of the primary interaction in AR is implicit in that content needs to react to changes in the camera view. ProtoAR’s live AR previews of all layers already make it possible to simulate interaction in a Wizard of Oz manner [8, 9], by manually moving 2D/3D objects around in the editor in response to the camera feed. This can be done in real-time with marker tracking enabled or disabled, and provides a useful basis for future research.

"
SurfaceConstellations: A Modular Hardware Platform for Ad-Hoc Reconfigurable Cross-Device Workspaces,https://dl.acm.org/authorize?N657166,"
We contribute SurfaceConstellations, a modular hardware platform for linking multiple mobile devices to easily create novel cross-device workspace environments. Our platform combines the advantages of multi-monitor workspaces and multi-surface environments with the flexibility and extensibility of more recent cross-device setups. The SurfaceConstellations platform includes a comprehensive library of 3D-printed link modules to connect and arrange tablets into new workspaces, several strategies for designing setups, and a visual configuration tool for automatically generating link modules. We contribute a detailed design space of cross-device workspaces, a technique for capacitive links between tablets for automatic recognition of connected devices, designs of flexible joint connections, detailed explanations of the physical design of 3D printed brackets and support structures, and the design of a web-based tool for creating new SurfaceConstellation setups.

"
"Revisiting ""The Rise and Decline"" in a Population of Peer Production Projects",https://dl.acm.org/authorize?N657167,"
Do patterns of growth and stabilization found in large peer production systems such as Wikipedia occur in other communities? This study assesses the generalizability of Halfaker et al.'s influential 2013 paper on ""The Rise and Decline of an Open Collaboration System."" We replicate its tests of several theories related to newcomer retention and norm entrenchment using a dataset of hundreds of active peer production wikis from Wikia. We reproduce the subset of the findings from Halfaker and colleagues that we are able to test, comparing both the estimated signs and magnitudes of our models. Our results support the external validity of Halfaker et al.'s claims that quality control systems may limit the growth of peer production communities by deterring new contributors and that norms tend to become entrenched over time. Our study supports RAD’s claim that quality control practices help explain increases in entrenchment and decreases in growth among peer production communities. Our work contributes to social computing and peer production research by providing evidence in support of the external validity of RAD, an influential empirical study. We also contribute to a small but growing literature on replication in HCI by demonstrating a replication study focused on generalizability. Our evidence in support of generalizability rests not only on the signs of our regression coefficients, but also on the similarity of our point estimates and visualizations. This work supports designers and community managers who are acting on the implications in RAD’s earlier work.
"
Designing Pronunciation Learning Tools: The Case for Interactivity against Over-Engineering,https://dl.acm.org/authorize?N657168,"
Paired role-play is a common collaborative activity in language learning classrooms, adding meaning and cultural context to the learning process. This is complemented by teachers' immediate and explicit feedback. Interactive tools that provide explicit feedback during collaborative learning are scarce, however. More commonly, supporting dialogue practice takes the form of computer-aided single-student read-and-record activities. This limitation is partly due to the complexity of processing language learners' speech in unconstrained tasks. In this paper, we assess the value of pronunciation error detection algorithms within a realistic, software-aided, paired role-playing task with beginning learners of French. We found that students' pronunciations improve regardless of the type of error detector employed -- even for those using simple heuristics. We suggest that speech technologies for language learning have been too focused on engineering goals. Instead, new interactive designs supporting collaboration may be used to overcome engineering limitations and properly support students' engagement. While speech technologies have improved dramatically in recent years, their use in language learning applications has largely been limited to “read-and-record” activities. Teaching pronunciation continues (justifiably) to be more effectively conducted in the context of classroom instruction, through activities such as paired role-playing. Speech-enabled ComputerAssisted Pronunciation Training (CAPT) applications do not yet provide adequate support to such environments despite a need for them, particularly in larger-size classrooms. We investigated the suitability of CAPT speech technologies, specifically Pronunciation Error Detectors (PEDs), for providing feedback in role-playing tasks for beginner learners of French. Our ecologically valid experiment provides evidence that sentence-level processing with no fine-grained audio processing can support the language learning process. We propose that the design of CAPT systems to perform a specific task within an interaction between two learners is far more important to learning outcomes than technological goals, making the field perfectly ripe for HCI intervention.
"
"From Her Story, to Our Story: Digital Storytelling as Public Engagement around Abortion Rights Advocacy in Ireland",https://dl.acm.org/authorize?N657169,"
Despite the divisive nature of abortion within the Republic of Ireland and Northern Ireland, where access to safe, legal abortion is severely restricted, effecting legislative reform demands widespread public support. In light of a building pro-choice counter-voice, this work contributes to a growing body of HCI research that takes an activist approach to design. We report findings from four design workshops with 31 pro-choice stakeholders across Ireland in which we positioned an exploratory protosite, HerStoryTold, to engender critical conversations around the use of sensitive abortion narratives as a tool for engagement. Our analysis shows how digital storytelling can help reject false narratives and raise awareness of the realities of abortion laws. It suggests design directions to curate narratives that provoke empathy, foster polyvocality, and ultimately expand the engaged community. Furthermore, this research calls for designers to actively support community mobilization through providing 'stepping stones' to activism. This research has addressed a current dearth in HCI for Reproductive Rights and responded to calls for activist design that stimulates critical conversations around contested issues. We have pointed to design opportunities for digital storytelling to be effectively curated as an advocacy tool. Specifically, we have presented opportunities to: raise awareness and challenge stigma through the exposing of hidden realities; foster empathy and polyvocality as a means of expanding the collective community; and provide stepping stones to activism that allow for engagement at different levels. In doing so, we respond to calls for further-understanding of how digital storytelling might be used beyond catharsis, and contribute insight into a gap in knowledge about the role online engagement plays in effecting tangible change.
"
"Capturing, Representing, and Interacting with Laughter",https://dl.acm.org/authorize?N657160,"
We investigate a speculative future in which we celebrate happiness by capturing laughter and representing it in tangible forms. We explored technologies for capturing naturally occurring laughter as well as various physical representations of it. For several weeks, our participants collected audio samples of everyday conversations with their loved ones. We processed those samples through a machine learning algorithm and shared the resulting tangible representations (e.g., physical containers and edible displays) with our participants. In collecting, listening to, interacting with, and sharing their laughter with loved ones, participants described both joy in preserving and interacting with laughter and tension in collecting it. This study revealed that the tangibility of laughter representations matters, especially its symbolism and material quality. We discuss design implications of giving permanent forms to laughter and consider the sound of laughter as a part of our personal past that we might seek to preserve and reflect upon. We demonstrated that laughter can be given enduring forms and play a role in people’s reflection on past memory both in personal and social contexts. This work contributes a qualitative evaluation of our prototype system for capturing, representing, and interacting with laughter. Our study showed that the tangibility of a laughter representation matters, as it serves to reconstrue what was once disembodied into a concrete reminder of a happy moment that people can preserve or share with others. We also found that people valued the ensoulment of objects with their loved ones’ laughter, not the quantity of laughter itself. We offered design insights for considering the sound of laughter as a part of our past we would want to preserve and reflect upon.
"
Geocaching with a Beam: Shared Outdoor Activities through a Telepresence Robot with 360 Degree Viewing,https://dl.acm.org/authorize?N657161,"
People often enjoy sharing outdoor activities together such as walking and hiking. However, when family and friends are separated by distance it can be difficult if not impossible to share such activities. We explore this design space by investigating the benefits and challenges of using a telepresence robot to support outdoor leisure activities. In our study, participants participated in the outdoor activity of geocaching where one person geocached with the help of a remote partner via a telepresence robot. We compared a wide field of view (WFOV) camera to a 360° camera. Results show the benefits of having a physical embodiment and a sense of immersion with the 360° view. Yet challenges related to a lack of environmental awareness, safety issues, and privacy concerns resulting from bystander interactions. These findings illustrate the need to design telepresence robots with the environment and public in mind to provide an enhanced sensory experience while balancing safety and privacy issues resulting from being amongst the general public. Our work builds on the related literature by exploring the use of telepresence robots for supporting outdoor leisure activities over distance. Here we focused on geocaching as an exemplar activity given that it contains a variety of basic activities within it including, walking or hiking, conversing, and looking for specific objects (similar to sightseeing). We found that by having a physical embodiment in the form of a telepresence robot, remote participants felt a strong sense of presence in the remote space. The mobility of the robot aided these feelings. The experience was limited, however, as remote users did not always have a strong understanding of the environment and they missed out on sensations typical of outdoor spaces, e.g., smells, wind, warmth from the sun. We also found challenges as a result of being in a public space with a variety of people with the potential for different perspectives and safety and privacy issues. These findings show that if telepresence robots are to be used to support outdoor leisure activities, similar to those we studied, that designs can be improved through the incorporation of additional environmental sensations as well as features to balance safety and privacy issues resulting from being amongst the general public. Of course, outdoor leisure activities can be more complex than the basic activities we studied. They might, for example, involve sports or activities with greater movement (e.g., throwing a Frisbee). It is likely that our findings about safety, privacy, and environmental concerns with telepresence robots extend to other settings, yet different types of telepresence robots would certainly be required with additional capabilities (e.g., faster movement, arms) for other more complex types of leisure activities.
"
PalmTouch: Using the Palm as an Additional Input Modality on Commodity Smartphones,https://dl.acm.org/authorize?N657162,"
Touchscreens are the most successful input method for smartphones. Despite their flexibility, touch input is limited to the location of taps and gestures. We present PalmTouch, an additional input modality that differentiates between touches of fingers and the palm. Touching the display with the palm can be a natural gesture since moving the thumb towards the device's top edge implicitly places the palm on the touchscreen. We present different use cases for PalmTouch, including the use as a shortcut and for improving reachability. To evaluate these use cases, we have developed a model that differentiates between finger and palm touch with an accuracy of 99.53% in realistic scenarios. Results of the evaluation show that participants perceive the input modality as intuitive and natural to perform. Moreover, they appreciate PalmTouch as an easy and fast solution to address the reachability issue during one-handed smartphone interaction compared to thumb stretching or grip changes. We presented PalmTouch, an additional input modality on smartphones using the palm to perform input. We proposed four use cases and evaluated PalmTouch in a user study. Participants perceived PalmTouch as a natural and intuitive gesture for a wide range of use cases, including the use as a shortcut and to improve reachability in one-handed scenarios. We investigated an abstract scenario in which we addressed reachability issues during one-handed smartphone interaction and found that PalmTouch was perceived as an easier, faster and more accurate solution than a grip change which could drop the device. While a quantitative analysis revealed that participants were indeed faster with PalmTouch, they appreciated its short learning curve. Especially on recent devices with an edge-to-edge display (e.g., iPhone X), PalmTouch provides an alternative to the removed home button. We implemented PalmTouch using capacitive images collected in a controlled study, and a convolutional neural network to differentiate between touches being made by fingers and palms. In contrast to previous work, our approach uses low-resolution fingerprints instead of heuristics that only work with multiple touch points (i.e., pen and palm) or that would introduce latency through temporal features. This enables us to build a model with an accuracy of 99.53% in a realistic scenario evaluation. Since we only modified the software of an off-the-shelf Nexus 5 smartphone, PalmTouch could be readily deployed onto recent smartphones, e.g., through software updates. We focused on the feasibility and usability of PalmTouch as an input modality on smartphones. With a precision of 99.35%, we showed a neglectable likelihood of unintended triggers either through classification errors or unintended palm touches. Future work could evaluate PalmTouch in the wild and investigate unintended palm touches. The palm detection model could be extended to differentiate between unintended and intended palm touches to support the device’s native palm rejection and to improve the accuracy of intended palm touches.

"
How “Wide Walls” Can Increase Engagement: Evidence From a Natural Experiment in Scratch,https://dl.acm.org/authorize?N657163,"
A core aim for designing constructionist learning systems and toolkits is enabling ""wide walls""-a metaphor used to describe supporting a diverse range of creative outcomes. Ensuring that a broad design space is afforded to learners by a toolkit is a common approach to achieving wide walls. We use econometric methods to provide an empirical test of the wide walls theory through a natural experiment in the Scratch online community. We estimate the causal effect of a policy change that gave a large number of Scratch users access to a more powerful version of Scratch data structures, effectively widening the walls for learners. We show that access to and use of these more powerful new data structures caused learners to use data structures more frequently. Our findings provide support for the theory that wide walls can increase engagement and learning. We believe that our work represents the first large-scale quantitative test of the “wide walls” design principle. These results are excellent news for designers who apply this popular and influential principle, as well as for proponents of constructionism. Although not without limitations or caveats, we hope that our estimates of a positive causal effect of wider walls will encourage more designers of educational technology and creativity support tools to adopt the principle in their practice.
"
Activity Tracking in vivo,https://dl.acm.org/authorize?N657164,"
While recent research has emphasized the importance of understanding the lived experience of personal tracking, very little is known about the everyday coordination between tracker use and the surrounding environment. We combine behavioral data from trackers with video recordings from wearable cameras, in an attempt to understand how usage unfolds in daily life and how it is shaped by the context of use. We recorded twelve participants' daily use of activity trackers, collecting and analyzing 244 incidents where activity trackers were used. Among our findings, tracker use was strongly driven by reflection and learning-in action, contrasting the traditional view that learning is one of deep exploration, following the collection of data on behaviors. We leverage on these insights and propose three directions for the design of activity trackers: facilitating learning through glances, providing normative feedback and facilitating micro-plans. In closing, our study took a step forward towards understanding how tracker use is enmeshed within everyday life, and how these devices could be better designed to support long-term use. Activity tracking, as a practice, is diversified, dependent upon and threaded into what goes on around us. Following these developments is difficult. Yet, as these devices become ever more central to the ongoing discourse on behavior change and patientdriven healthcare, a richer understanding of the lived dynamics of activity tracking trackers is crucial.

"
Understanding Users' Capability to Transfer Information between Mixed and Virtual Reality: Position Estimation across Modalities and Perspectives,https://dl.acm.org/authorize?N657175,"
Mixed Reality systems combine physical and digital worlds, with great potential for the future of HCI. It is possible to design systems that support flexible degrees of virtuality by combining complementary technologies. In order for such systems to succeed, users must be able to create unified mental models out of heterogeneous representations. In this paper, we present two studies focusing on the users' accuracy on heterogeneous systems using Spatial Augmented Reality (SAR) and immersive Virtual Reality (VR) displays, and combining viewpoints (egocentric and exocentric). The results show robust estimation capabilities across conditions and viewpoints. In this work we presented the results of two user studies that focus on the user’s accuracy in mixed reality systems contrasted with pure VR versions. The first study considered variations – and similarities – between conditions from an egocentric viewpoint, while the second study evaluated the complementarity of egocentric and non-egocentric viewpoints for target estimation. The obtained results indicate that, as with other spatial tasks, the accurate perception of the space is supported by the presence of landmarks. Participants showed a remarkable capability to transfer information between SAR and VR modalities, even between ego/exocentric POVs. Additionally, perceiving the scene from closer seems to increase the participants’ expectations on their accuracy. It is worth mentioning that depth compression was significantly higher than in purely physical scenarios only for egocentric tasks that happened solely in VR. Hybrid MR conditions do not seem to suffer from this any more than in purely physical tasks in the case of egocentric estimation. Additionally, the lack of depth compression when changing scale (Study 2) challenges previous work [33], perhaps caused by the range (peripersonal space) and the availability of landmarks (both physical and virtual). These results indicate that the participants were able to construct a unified mental model from heterogeneous representations and views. The presented research follows a rich history of perception and cognition studies, proposing the evaluation of mixed and hybrid systems from both a perceptual and cognitive standpoints. In the future, it would be of interest to study up to which extent the complementarity of modalities extends to real world scenarios, by exploring more complex and ecological tasks. Finally, it would be important to test the limits of systems with heterogeneous representations, looking for cases where users cannot transfer knowledge between display modalities.

"
Seemo: A Computational Approach to See Emotions,https://dl.acm.org/authorize?N657176,"
Successful human interactions are based on becoming aware of other's emotion and making adaptations accordingly. However, understanding emotion is a complex task that has generated countless debates among researchers over the past decades. The abstract nature of human emotion highlights the need for a new data-driven approach that can better describe and compare across fine-grained emotional states. In this study, we propose Seemo, a novel neural embedding framework, which allows us to map human emotions into vector space representations. Seemo is trained using Twitter data and is evaluated on two fundamental use cases in traditional emotion research: determining the underlying dimensions of emotions and identifying the set of basic emotions. The evaluation reveals that on both tasks Seemo can generate results consistent with the mainstream theories. Results also show that the vector space representation of Seemo can effectively decode the important relationships between emotions that were usually not explicitly presented. Motivated from a practical standpoint, in this study we proposed Seemo, a deep learning based model, to map human emotions into vector space representations. To prove the validity and practicality of Seemo, we trained it using Twitter data and applied the derived emotion vector space on testing two classic emotion philosophies: the valence-arousal space model of emotion, and the theory of basic emotions. The consistent findings on both use cases with mainstream views in the literature proved Seemo’s effectiveness as an alternative assessment for psychologists to conduct largerscale emotion studies.
"
"Knowing You, Seeing Me: Investigating User Preferences in Drone-Human Acknowledgement",https://dl.acm.org/authorize?N657177,"
In the past, human proxemics research has poorly predicted human robot interaction distances. This paper presents three studies on drone gestures to acknowledge human presence and clarify suitable acknowledging distances. We evaluated four drone gestures based on non-verbal human greetings. The gestures included orienting towards the counterpart and salutation gestures. We tested these individually and in combination to create a feeling of acknowledgement in people. Our users preferred being acknowledged from two meters away but gestures were also effective from four meters. Rotating the drone towards the user elicited a higher degree of acknowledgement than without. We conclude with a set design guidelines for drone gestures. People prefer drones to acknowledge them from two meters away at the very least. People feel acknowledged by drone gestures both at short (2m) and larger distances (4m). Designers can combine orienting and salutational gestures in drone flight paths and achieve high levels of acknowledgments in users. Drone users will feel more acknowledged by drones with recognizable fronts to support the orienting gestures. To obtain valid results on acknowledgment distances drones need to fly freely, i.e. not tied to rails, but wind and even indoor ventilation can pose a challenge to accurate flight execution. Future work should look into other gestures, e.g. slowing down from long-distance travel speed, flying in a circle, or bopping up and down, and how sound, size, and approach angles of drones affect acknowledgement. It is important to note, as well, that some of our tested gestures might prove problematic in scenarios in which drone payload is sensitive to sudden displacement from a waggle, e.g. when delivering a pizza in a box.
"
Evaluating CoBlox: A Comparative Study of Robotics Programming Environments for Adult Novices,https://dl.acm.org/authorize?N657178,"
A new wave of collaborative robots designed to work alongside humans is bringing the automation historically seen in large-scale industrial settings to new, diverse contexts. However, the ability to program these machines often requires years of training, making them inaccessible or impractical for many. This paper rethinks what robot programming interfaces could be in order to make them accessible and intuitive for adult novice programmers. We created a block-based interface for programming a one-armed industrial robot and conducted a study with 67 adult novices comparing it to two programming approaches in widespread use in industry. The results show participants using the block-based interface successfully implemented robot programs faster with no loss in accuracy while reporting higher scores for usability, learnability, and overall satisfaction. The contribution of this work is showing the potential for using block-based programming to make powerful technologies accessible to a wider audience. The goal of this work was to explore ways of making industrial robot programming more accessible to people with little or no prior programming experience. Drawing on successful design strategies used to introduce young learners to the practice of programming, we created CoBlox and showed how it outperforms the most wide-spread robotics programming approaches used today. The analysis shows the CoBlox helped adult novices program more tasks successfully by decreasing time on task while maintaining quality. In addition, the participants found it easier to use and enjoyed it more. Collectively, with this work, we advance our understanding of ways to make robot programming more accessible to a wider range of users. We view robotics as merely a single example of a field in which a block-based interface can be used. This study shows the block-based approach making a robotics programming task easier for novice adults, providing an empirical basis for future work concerned with making programming accessible to all. In doing so, we contribute to the larger goal of giving people access to and control over the technologies around us.

"
Rolling-Menu: Rapid Command Selection in Toolbars Using Roll Gestures with a Multi-DoF Mouse,https://dl.acm.org/authorize?N657179,"
This paper presents Rolling-Menu, a technique for selecting toolbar items, based on the use of roll gestures with a multidimensional device, the Roly-Poly Mouse (RPM). Rolling-Menu reduces object-command transition, resulting in a better integration between command selection and direct manipulation of application objects. Selecting a toolbar item with Rolling-Menu requires rolling RPM in a predefined direction corresponding to the item. We propose a design space of Rolling-Menu that includes different roll mapping and validation modes. A first user's study, with a simple toolbar containing up to 14 items, establishes that the best version of Rolling-Menu takes, on average, up to 29% less time than the Mouse to select a toolbar item. Moreover accuracy of the selection with Rolling-Menu is above 90%. Both the validation mode and the mapping between roll direction and toolbar items influence the performance of Rolling-Menus. A second study compares the three best versions of Rolling-Menu with the Mouse to select an item in two types of multidimensional toolbars: a toolbar containing dropdown lists, and a grid toolbar. Results confirm the advantage of Rolling-Menu over a Mouse. In this work we presented different roll-based techniques for command selection in toolbars. The aim of these techniques is to minimize disruptive transitions between the working area and the toolbar. To do so, Rolling-Menus rely on the detection of roll gestures performed in the direction of the toolbar itemsto select. Based on different design dimensions, we proposed 8 versions of Rolling-Menu that we compared to the Mouse for selecting a top-level item in a Simple toolbar, or sub-items in more complex toolbars, i.e. a Dropdown and a Grid toolbar. Our user studies demonstrated that two Rolling-Menus reduce the selection time for a toplevel item or sub-item, while keeping a good or very good accuracy.

"
Effects of Enhanced Gaze Presentation on Gaze Leading in Remote Collaborative Physical Tasks,https://dl.acm.org/authorize?N657170,"
With respect to collaborative physical tasks, gaze and gestures play significant roles when referring to physical objects. In video-mediated communication, however, such nonverbal cues become ""ineffectual"" when they are presented via a 2D monitor, making video-mediated collaborative physical tasks inefficient. This study focuses on gaze cues to support remote collaborative physical tasks and uses an eye-shaped display, ""ThirdEye,"" a simple add-on display that represents a remote participant's gaze direction. ThirdEye is expected to be especially effective when used with mobile terminals. We investigated whether the ThirdEye's gaze shift is effective in leading a local observer's attention toward objects in the local environment, even when ThirdEye is presented with the actual face image of a remote person. Experimental results show that ThirdEye can lead the local participant's attention to intended objects faster than without ThirdEye. The purpose of our study was to support gaze cue in remote collaborative physical tasks, especially when participants use mobile terminals for video communication. ThirdEye is a simple add-on display that represents a remote participant's gaze direction; it was expected to be suitable for our purpose. Therefore, this study focused on investigating the effect of ThirdEye on leading an observer’s gaze. We assumed a case in which ThirdEye is used in combination with normal video communication software that shows the actual face image of a remote participant. Thus, we conducted a simple video communication experiment and compared the cases of “with ThirdEye” and “without ThirdEye.” The results show that ThirdEye was effective in leading the participants’ gaze faster to target objects, even when ThirdEye and the remote participant's face were presented together. Furthermore, the objective measures suggest that ThirdEye was cognitively effective.

"
Dream Lens: Exploration and Visualization of Large-Scale Generative Design Datasets,https://dl.acm.org/authorize?N657171,"
This paper presents Dream Lens, an interactive visual analysis tool for exploring and visualizing large-scale generative design datasets. Unlike traditional computer aided design, where users create a single model, with generative design, users specify high-level goals and constraints, and the system automatically generates hundreds or thousands of candidates all meeting the design criteria. Once a large collection of design variations is created, the designer is left with the task of finding the design, or set of designs, which best meets their requirements. This is a complicated task which could require analyzing the structural characteristics and visual aesthetics of the designs. Two studies are conducted which demonstrate the usability and usefulness of the Dream Lens system, and a generatively designed dataset of 16,800 designs for a sample design problem is described and publicly released to encourage advancement in this area. Overall, we believe Dream Lens represents a novel contribution to the area of interactively exploring large collections of 3D models, particularly those with subtle variations in their aesthetic and physical properties. Combined with contributing a dataset of 16,800 generatively designed solutions to the same design problem, we hope our work can serve to inspire future research in this area.

"
"Seeing What Is and What Can Be: On Sustainability, Respect for Work, and Design for Respect",https://dl.acm.org/authorize?N657172,"
This paper privileges visual contributions-original images and referenced materials-nearly as much as text. As such, it follows a trend towards pictorials and image intensive papers elsewhere in SIGCHI venues that have yet to find acceptance in the CHI paper tracks. The paper in both its text and its visual contributions takes up (a) ongoing questions of how designs matter-especially in relation to sustainability, (b) questions of extending notions of sustainability beyond the environment to include notions from respect for human labor to respect between nations, (c) questions of the utility of photographic methods in building design understanding and conceptualization, (d) questions of emphasis and extension for Rams' principles of good design, and (e) hypotheses about the relations between seemingly small design details and global attitudes, policy, and harmony, inspired by Allison's account of Thucydides' Trap. These are big questions. It is their ambitious character that unifies them. Things that matter: Knowing what it means for designs that matter to matter, or even if it is important to know how and if the question of what matters matters cannot be answered in a single paper. Sustainability beyond the environment: Sustainability can be foundational to design. So too can several other things. Sustainability extends beyond environmental conditions to include human costs, occupational safety, supply chains, and even sensitivity and respect between nations as illustrated here. Many other extensions are possible. Photographic method: Diagrammatic representations can provide evidence and predictions that may inform design. Photographs of high quality can help with these understandings. Taking and discussing such photographs is a design method. This paper has instances of photographs and diagrams used as evidence, prediction, empathy, and conceptualization in the service of constructing design knowledge. There are other forms of use. Shifting emphasis for good design principles: The often-cited principles of good design by Dieter Rams include less cited principles about durability, environmental sustainability, and thoughtfulness. Others in Rams’ generation wrote similarly about these design concerns. Design details and policy: I propose as an hypothesis that designers can influence even global policy by designing for respect, as a matter of sustainability. I propose also as an hypothesis that seemingly small design details can influence global tensions and harmony. These may contribute to a way forward for knowing how we can be sure that what we design matters. The last word: the path of respect is preferable to the path of conflict, as a design perspective and as a sustainable design practice.

"
"Everything We Do, Everything We Press: Data-Driven Remote Performance Management in a Mobile Workplace",https://dl.acm.org/authorize?N657173,"
This paper examines how data-driven performance monitoring technologies affect the work of telecommunications field engineers. As a mobile workforce, this occupational group rely on an array of smartphone applications to plan, manage and report on their jobs, and to liaise remotely with managers and colleagues. These technologies intend to help field engineers be more productive and have greater control over their work; however they also gather data related to the quantity and effectiveness of their labor. We conducted a qualitative study examining engineers' experiences of these systems. Our findings suggest they simultaneously enhance worker autonomy, support co-ordination with and monitoring of colleagues, but promote anxieties around productivity and the interpretation of data by management. We discuss the implications of data-driven performance management technologies on worker agency, and examine the consequences of such systems in an era of quantified workplaces. We studied the work of field engineers and the roles that new performance enhancement and management technologies play in their workplace. Although field engineers see some value in these systems, they have also induced feelings of disconnection from colleagues, and raised concerns about being monitored and remotely performance managed. Moreover, while these technologies simplified aspects of the engineer’s work, their use simultaneously led to experiences of work intensification and perceptions of greater responsibility and accountability that rendered any gain of greater job control disputable. Our findings further highlighted the range of acts of resistance engineers have developed and adopted to compensate for lost privacy. We also saw how engineers engage in new forms of work to respond to the demands of these systems in relation to the unexpected interactions they cause with colleagues and customers, and to maintain their schedules for the day. Our study provides insight into the use of monitoring and remote performance management technologies in the workplace, and sheds light into a commonplace yet understudied workplace and worker occupation.
"
Social Affordances at Play: Game Design Toward Socio-Technical Innovation,https://dl.acm.org/authorize?N657174,"
In this paper we propose that game design strategies and theories can be useful tools for supporting the design of innovative socio-technical systems aimed at supporting social co-presence. We support this proposal with an annotated portfolio of a series of research prototype games that investigate sensor affordances and configurations to sustain and enhance social co-presence. We introduce relevant theory from game studies (the magic circle; the MDA (mechanics/dynamics/aesthetics framework)) to help ground and guide the use of game design in HCI practice. We conclude with recommendations for adopting game design as a supplementary research technique, with caveats about the limits of the approach. In this paper, we claim that game design theory and techniques can be of value in tackling the design challenge of developing technologies to support and enhance copresence. We introduced a concept (‘the magic circle’) and a framework (MDA) from game studies, and used an annotated portfolio of our design work to illustrate some insights we gleaned from developing collocated social games while making use of these ideas. We concluded with some general patterns that were extrapolated from this body of work, which of course drew upon extant HCI research and theory as well. We do not mean to propose that game design theory and techniques could or should supplant what is already known, but instead, propose that they can be useful additions to a designer’s toolkit. We are currently taking what we’ve learned from this cycle of research-through-design into the creation of non-game technology to support collocated social engagement. We are constructing technical prototypes nested in activities that draw upon the design insights as well as the general patterns for creating viable support for robust and pleasurable moment-to-moment felt experience. We have come to realize many non-game encounters are ‘magic circles’ in their own right—spaces in which people engage in collective action that sustains experiences which may be instrumental, but which also are weighed and valued for their moment-by-moment texture. We are especially interested in developing co-presence support technologies to enhance mutual attention and connection, and see the work described in this paper as useful for breaking away from embedded technology biases to move toward achieving these aims. We hope our results and recommendations will be useful to others investigating ways that interactive technology can enhance the fabric of everyday co-present social life and activity, whether for work or for play.

"
A Critical Examination of Feedback in Early Reading Games,https://dl.acm.org/authorize?N657185,"
Learning games now play a role in both formal and informal learning, including foundational skills such as literacy. While feedback is recognised as a key pedagogical dimension of these games, particularly in early learning, there has been no research on how commercial games available to schools and parents reify learning theory into feedback. Using a systematic content analysis, we examine how evidence-based feedback principles manifest in five widely-used learning games designed to foster young children's reading skills. Our findings highlight strengths in how games deliver feedback when players succeed. Many of the games, however, were inconsistent and not proactive when providing error feedback, often promoting trial and error strategies. Furthermore, there was a lack of support for learning the game mechanics and a preference for task-oriented rewards less deeply embedded in the gameplay. Our research provides a design and research agenda for the inclusion of feedback in early learning games. This paper identified the importance of evaluating how feedback is currently designed in early learning games. To achieve this, we consolidated, refined and applied a comprehensive game content analysis framework, to analyse several widely-used reading games for young children. This descriptive analysis was followed by a critical evaluation of existing early learning game feedback decisions, drawing from the games and learning literature to problematize them. This research makes three main contributions. Firstly, in consolidating and reflexively applying a holistic framework for evaluating feedback in early learning game design we offer a new methodological tool. This tool was based on three existing broad frameworks, and unifies codes for feedback in learning and learning games as well as game rewards. This methodology supports a fine-grained level analysis of game feedback, serving designers and practitioners who may want to use it as a guide for design or as an evaluation tool for games in reading and other domains. In applying this tool to games within the reading domain we refined and added new codes. We encourage others utilising this tool in different domains and game genres to similarly iterate its dimensions in order to further widen its scope and relevance. Secondly, we provide a characterisation of the broad game genres for early reading. We uncovered a prioritisation of task-oriented learning over intrinsically integrated learning content within the games, with more playful and immersive features of games such as rewards and praise as well as punishments limited. This focus on task-oriented learning, as opposed to learning through play, problematizes the kind of learning taking place, the limited space for more immersive games and the tension of including games within tightly packed curricula. Additionally, our analysis showed an important division between games for learning and games for practice. The lack of instruction in some games calls for practitioners to carefully evaluate their game choices to ensure pupils have the necessary knowledge prior to game play. Finally, and most importantly, our research allows us to identify strengths and weaknesses, as well as open questions, for the future design work in the area of game feedback. Specifically, while most games supported a clear direction on where the player was going (Feed Up), there were weaknesses in how the provision of feedback during game play (Feed Back) was delivered. This gap was especially visible in how little support children were offered to recover from their errors. Given our focus and scope, our analysis was limited to the reading domain, and within this we recognise that many of the games focused on phonics. However, by examining the reading domain in depth, and articulating our methodological process, we allow other researchers to conduct similar analyses, ascertaining transferability to other domains [54], for instance to mathematics. Future research can build upon our findings to consider if the domain and its complexity, e.g. low-level skills such as phonics or basic arithmetic compared to high-level-skills such as comprehension or interpreting statistics, impact on how game feedback is designed. Equally, this work could be extended to other learning games types (beyond minigames) with more complex narratives such as immersive games where further aspects such as feedback timing may have greater importance [23]. In closing, we hope that our work will shine a spotlight on the importance of well-designed feedback for early learning games, carving out priorities that direct game design developments in this area, whilst providing a guide for practitioners to evaluate the plethora of existing learning games toward ensuring meaningful learning experiences for young learners.
"
HCI meets Material Science: A Literature Review of Morphing Materials for the Design of Shape-Changing Interfaces,https://dl.acm.org/authorize?N657186,"
With the proliferation of flexible displays and the advances in smart materials, it is now possible to create interactive devices that are not only flexible but can reconfigure into any shape on demand. Several Human Computer Interaction (HCI) and robotics researchers have started designing, prototyping and evaluating shape-changing devices, realising, however, that this vision still requires many engineering challenges to be addressed. On the material science front, we need breakthroughs in stable and accessible materials to create novel, proof-of-concept devices. On the interactive devices side, we require a deeper appreciation for the material properties and an understanding of how exploiting material properties can provide affordances that unleash the human interactive potential. While these challenges are interesting for the respective research fields, we believe that the true power of shape-changing devices can be magnified by bringing together these communities. In this paper we therefore present a review of advances made in shape-changing materials and discuss their applications within an HCI context. With the aim of accelerating the design of shape-changing devices, we have provided a review of the advances in material science from an HCI perspective. We see this approach as a road map for next generation designers that want to better understand material science and adopt shape-changing mechanisms in their work. We also believe that creating OUIs requires a redefinition of the tools we use during the design process. The tools needed for shaped-interface design need to be more expressive, like the raw and versatile materials an industrial designer might use to create complex geometries. A change like this can happen if HCI practitioners are attentive to shape-change developments from a material science perspective. This work is a step in this direction as it bridges a gap between material science, HCI and shape-change.

"
Ticket to Talk: Supporting Conversation between Young People and People with Dementia through Digital Media,https://dl.acm.org/authorize?N657187,"
We explore the role of digital media in supporting intergenerational interactions between people with dementia and young people. Though meaningful social interaction is integral to quality of life in dementia, initiating conversation with a person with dementia can be challenging, especially for younger people who may lack knowledge of someone's life history. This can be further compounded without a nuanced understanding of the nature of dementia, along with an unfamiliarity in leading and maintaining conversation. We designed a mobile application - Ticket to Talk - to support intergenerational interactions by encouraging young people to collect media relevant to individuals with dementia to use in conversations with people with dementia. We evaluated Ticket to Talk through trials with two families, a care home, and groups of older people. We highlight difficulties in using technologies such as this as a conversational tool, the value of digital media in supporting intergenerational interactions, and the potential to positively shape people with dementia's agency in social settings. We present Ticket to Talk as a novel technology that encourages and supports conversations between young people and their older relatives living with dementia. Ticket to Talk achieves this through the collection and curation of digital media related to an older relatives’ past experiences, providing talking points for young people. The project identifies the themes of ‘promoting and managing reminiscence, ‘starting and maintaining conversation’, and ‘redistributing agency’ as its findings. We discuss: the benefits and challenges in promoting and managing conversations; the methods of using digital media to stimulate intergenerational interactions; and finally, the positive and negative ramifications of using technology to influence people with dementia’s agency in social settings. Ticket to Talk can be used to position people with dementia as story tellers and advice givers, bridge conversational gaps, and reaffirm connectivity in families.
"
Falling for Fake News: Investigating the Consumption of News via Social Media,https://dl.acm.org/authorize?N657188,"
In the so called 'post-truth' era, characterized by a loss of public trust in various institutions, and the rise of 'fake news' disseminated via the internet and social media, individuals may face uncertainty about the veracity of information available, whether it be satire or malicious hoax. We investigate attitudes to news delivered by social media, and subsequent verification strategies applied, or not applied, by individuals. A survey reveals that two thirds of respondents regularly consumed news via Facebook, and that one third had at some point come across fake news that they initially believed to be true. An analysis task involving news presented via Facebook reveals a diverse range of judgement forming strategies, with participants relying on personal judgements as to plausibility and scepticism around sources and journalistic style. This reflects a shift away from traditional methods of accessing the news, and highlights the difficulties in combating the spread of fake news. This paper has shown, through an online survey and study, that social media users are aware of and encountering what can be characterised as “fake news.” A think aloud study has subsequently revealed various interpretative and argumentative strategies used by readers when asked to make judgements on the truthfulness of news presented in a social media feed. Our participants’ behaviour suggests that they assess news differently when it is presented via social media, including drawing upon a variety of formal and judgement based characteristics, potentially challenging traditionally hierarchical information provision, and demonstrating that perceived levels of interest in topics is a key factor when considering solutions to echo chambers and fake news.
"
'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions,https://dl.acm.org/authorize?N657189,"
Data-driven decision-making consequential to individuals raises important questions of accountability and justice. Indeed, European law provides individuals limited rights to 'meaningful information about the logic' behind significant, autonomous decisions such as loan approvals, insurance quotes, and CV filtering. We undertake three experimental studies examining people's perceptions of justice in algorithmic decision-making under different scenarios and explanation styles. Dimensions of justice previously observed in response to human decision-making appear similarly engaged in response to algorithmic decisions. Qualitative analysis identified several concerns and heuristics involved in justice perceptions including arbitrariness, generalisation, and (in)dignity. Quantitative analysis indicates that explanation styles primarily matter to justice perceptions only when subjects are exposed to multiple different styles---under repeated exposure of one style, scenario effects obscure any explanation effects. Our results suggests there may be no 'best' approach to explaining algorithmic decisions, and that reflection on their automated nature both implicates and mitigates justice dimensions. Algorithmic decisions are likely to become increasingly relied on for a range of decisions with potentially important repercussions for those affected. Understanding how people assess the fairness of such decisions, and how explanations might help, is therefore of increasing significance. Despite repeated calls for more transparency over how such decisions are made, there is still much to learn about what people want and need to know about algorithms in order to hold them accountable to justice. As lawmakers legislate for mandatory provision of information to decision-subjects, human-computer interaction research has much to offer in how such information should be extracted, presented and delivered. This paper suggests that people do consider justice-related aspects of algorithmic decision-making systems, much as they do for manual decision-making processes. However, depending on how and when they are deployed, explanations may or may not help individuals to evaluate the fairness of such decisions. The algorithmic nature of these systems results in an array of novel considerations which are not captured by traditional research on perceptions of justice. Conversely, creators of ML explanation systems have not typically designed them with the information needs of those individuals facing significant personal consequences of model outputs in mind. It is our hope that such concerns will instigate renewed focus on this range of important use cases for algorithmic explanations, and more broadly for HCI research to support the pursuit of justice as algorithmic decision-making systems take hold in a wide array of high-stakes domains.

"
"I Really did That: Sense of Agency with Touchpad, Keyboard, and On-skin Interaction",https://dl.acm.org/authorize?N657180,"
Input on the skin is emerging as an interaction style. At CHI 2012, Coyle and colleagues identified an increase in the sense of agency (SoA) as one benefit of skin input. However, their study only compared skin input to button presses and has not, to our knowledge, been replicated. Therefore, we had 24 participants compare skin input to both button presses and touchpad input, measuring SoA using the Libet Clock paradigm. We replicate previous findings regarding increased SoA in skin versus button input and also find that SoA for skin is significantly increased compared to touchpad input. Interview data addressing subjective experience further support these findings. We discuss agency and the experiences associated with skin input, as well as differences to input with non-skin devices. Input on the skin is emerging as an interaction style; earlier work has suggested that this is tied to the sense of agency when interacting on ones own skin. In particular, Coyle et al. [4] provided data showing increased intentional binding for on-skin interaction compared to interaction using a button. We have replicated these findings and shown that the differences in agency can also be found with other devices than buttons (here, a touchpad). We also showed that the subjective experience of interaction is not affected in a straightforward manner by these differences, calling for further work that investigates if sense of agency affects conscious user experience.
"
The Impact of Abstract vs. Concrete Feedback Design on Behavior Insights from a Large Eco-Driving Field Experiment,https://dl.acm.org/authorize?N657181,"
About 17% of the worldwide CO2-emissions can be ascribed to road transportation. Using information systems (IS)-enabled feedback has shown to be very efficient in promoting a less fuel-consuming driving style. Today, in-car IS that provide feedback on driving behavior are in the midst of a fundamental change. Increasing digitalization of in-car IS enables virtually any kind of feedback. Still, we see a gap in the empirical evidence on how to leverage this potential, raising questions on future HCI-based feedback design. To address this knowledge gap, we designed an eco-driving feedback IS and, building upon construal level theory, hypothesize that abstract feedback is more effective in reducing fuel consumption than concrete feedback. Deployed in a large field experiment with 56 participants covering over 297,000km, we provide first empirical evidence that supports this hypothesis. Despite its limitations, this research may have general implications for the design of real-time feedback. In our research, we investigate the impact of abstract versus concrete feedback on eco-driving. Based on existing research we predict that abstract feedback is more effective in reducing fuel consumption than concrete feedback. This hypothesis led the design process of an EDFIS artifact, which we evaluated in a rigorous field experiment with 56 participants that together drove over 297,000km in 21,608 trips. The EDFIS stimuli were developed based on inputs from practice and HCI research, using methods from design science research and theoretical implications from construal level theory. The results indicate that abstract feedback significantly reduces fuel consumption, while concrete feedback did mostly not have an effect, thus pointing towards confirmation of our hypothesis. This effect could not be explained by differences between the groups in respect to their eco-driving motivation, their attitude towards eco-driving nor their preference for a specific type of feedback. Hence, the research at hand is the first to demonstrate the impact of different feedback designs, namely abstract vs. concrete feedback design, on eco-driving behavior in a rigorous field experiment. Practitioners in HCI may consider the power of information abstraction in the design of visual feedback systems. There are some limitations in the research design and analysis that should be addressed in future work. Regarding research design, one limitation is the sample size and sample specificity. Though we cover quite a lot of km in driving data, for intergroup comparisons with our three groups, the sample size of 56 drivers is sub-optimal. Additionally, our sample was drawn from professional road assistance drivers. Therefore, the eco-driving motivation and potential of our participants was restricted by two key factors. First, fuel expenses are covered by the drivers’ employer. Second, in case of a customer service request, the drivers’ first priority is to get to the spot of the incidence as fast as possible, thereby disregarding eco-friendly driving. If any, however, we argue that this made it harder to find an effect. Therefore, we would in fact expect even stronger results if our research would be replicated with a sample of regular drivers. Another limitation is the design of our experimental stimuli (Figures 2a vs. 2b) that apparently differ in more than one factor from each other. This research tries to shed light on whether there is a potential effect of abstract feedback design on behavior by initially combining several construal level inducing factors. Future research will have to disentangle these factors and distinguish them from competing theories with more clarity [25]. Regarding the control stimulus, we cannot exclude a possible effect of the G-radar on fuel consumption, even though we doubt that it had an effect, as no number or color provides normative feedback to the driver. Additionally, an effect of the G-radar cannot explain the differences in fuel consumption between the groups in the treatment phase as compared to the baseline phase. Apart from tackling these shortcomings, we see further potential for future research in deepening the analyses in two ways. Firstly, additional data, like weather, traffic, or road conditions should be included as they very likely have an effect on fuel consumption. By including information on the day of travel and average trip speed, we tried to cover some of these potential influences, but they surely struggle with fuzziness and hence, should be replaced by the direct measures. Additionally, information on road conditions like slope or speed limit, are missing in our dataset. Adding such information in the analysis both, improves the validity of the investigated effects and helps to understand the determinants of eco-driving. As a second future research stream, we propose to not only look for main effects of feedback interventions, but try to understand, which specific behaviors are changed in detail. For our example, this would mean to analyze, which specific driving behavior was affected by the different feedback types and how these behaviors are correlated with the outcome variable, i.e. fuel consumption. For example, did we change acceleration behavior, idling behavior or coasting and what is the impact for each of them on fuel consumption? Our modern sensor technology gives us the possibilities to measure many different behaviors in high granularity. This could enable specific and tailored interventions in real-time. Regarding the theoretical basis, we used construal level theory as it nicely explains why abstract feedback is more effective in changing eco-driving behavior than concrete feedback, but we see that other explanations could contribute to our findings as well. For example, the visualization of the growing tree that we used may contain elements of gamification. The apples that appear on very low fuel consumption could be seen as batches often used in gamification [11,35] and it may be the motivation to “let the apples grow” that drives the user to behave as intended. Future research should challenge these alternative explanations by proper research designs under more controlled conditions, e.g. in a laboratory setting.
"
Tangible Landscape: A Hands-on Method for Teaching Terrain Analysis,https://dl.acm.org/authorize?N657182,"
This paper presents novel and effective methods for teaching about topography--or shape of terrain--and assessing 3-dimensional spatial learning using tangibles. We used Tangible Landscape--a tangible interface for geospatial modeling--to teach multiple hands-on tangible lessons on the concepts of grading (i.e., earthwork), geomorphology, and hydrology. We examined students' ratings of the system's usability and user experience and tested students' acquisition and transfer of knowledge. Our results suggest the physicality of the objects enabled the participants to effectively interact with the system and each other, positively impacting ratings of usability and task-specific knowledge building. These findings can potentially advance the design and implementation of tangible teaching methods for the topics of geography, design, architecture, and engineering. This research is unique in that it: (1) highlighted the potential of using a tangible interface–Tangible Landscape–to develop hands-on teaching tasks related to various geospatial, geographic, geological, planning, and landscape architecture topics; (2) demonstrated how using open-source GIS grants researchers the flexibility (i.e., ability to run various types of geospatial simulations) to develop specific tangible interface tasks (water flow, landforms, cut and fill); and (3) administered assessments–analogous to the tangible lessons and tasks–to measure student learning outcomes. Additionally, the observed variance in knowledge building speaks to the importance of systematically saving and recording interaction data (e.g., scanning of students’ hands to quantify amount of interaction) to accurately process, score, and assess students’ task performance. This, as well as using recorded log files to qualitatively explore interaction data will allow future researchers to uncover and understand any issues students experience when learning topics of geography, design, architecture, and engineering with tangible media. In sum, we argue that the main focus of future research should be on how geospatial learning outcomes are achieved, rather than only on what is achieved.

"
[Un]breaking News: Design Opportunities for Enhancing Collaboration in Scientific Media Production,https://dl.acm.org/authorize?N657183,"
Contemporary scientific media production requires a complex socio-technical infrastructure we call the ""Media Production Pipeline"" (MPP). Media professionals engage with researchers along the MPP to disseminate science news to the lay public. However, differing incentive structures and professional contexts frequently set researchers' values and needs at odds with those of media professionals, resulting in problematic or failed interactions. We ask the research question: what pain points in scientific media production afford opportunities for future HCI innovation? We then present a grounded theory analysis of 24 interviews with researchers and media professionals, yielding several key contributions. First, we describe two collaborative domains in scientific media production between research advocates and media outlets. Second, we characterize discrete technological gaps and pain points in both domains. Finally, we discuss implications for design and propose solutions from HCI areas like peer production, online communities, recommender systems, and online collaboration. In the modern Web 2.0 information environment, stories can go viral in the blink of an eye regardless of their legitimacy. Their authors are not only trained journalists, but also scientists, lay citizens, and powerful political and corporate interests, often pitted against each other in a battle for credibility. With a crisis of faith in mainstream media well underway, combined with an onslaught of science-decrying public figures spreading misinformation like wildfire, the institution of science is at a critical juncture. Scientists must speak up, and they must do so effectively if their voices are to be heard through the chaotic information churn of Web 2.0. This paper describes the MPP for producing scientific content in Western media systems. As we have shown, much can be done to enhance collaboration with media outlets, yet scientists’ willingness to engage is clearly a prerequisite–and a point of opinionated contention. ""Visible Scientists"" [17, 23] who operate prominently in the public sphere can potentially use their influence to affect policy/public opinion. Yet researchers risk reputations as ""show boaters"" who egotistically pander to the media or sacrifice the quality of their work to focus on career advancement through increased popularity [17]. Academics rely intensely on citations to demonstrate scientific contributions, yet media engagement is of value to society and requires real work. We believe there exists a middleground. Technology can possibly reduce the time and effort required of scientists to share newsworthy research responsibly with the public, and perhaps even improve scientific literacy rates, though it remains critical to consider how peoples’ ""folk theories"" [5] interact with reception of science news. We have suggested implications such as automatic tracking of research coverage in mass media, as well as expert contributions to credible and lay-accessible online resources–both of which yield quantitative metrics. In order to truly bridge the gap between science and society, the academy ought to not only study and build new media tools and systems, but also formally expand incentive structures to consider measures of public scholarship through high quality and impactful independent media production or mainstream media engagement.

"
ThermoKiosk: Investigating Roles for Digital Surveys of Thermal Experience in Workplace Comfort Management,https://dl.acm.org/authorize?N657184,"
Thermal comfort in shared workplaces is often contested and impacts productivity, wellbeing, and energy use. Yet, subjective and situated comfort experiences are rarely captured and engaged with. In this paper, we explore roles for digital surveys in capturing and visualising subjective experiences of comfort in situ for comfort management. We present findings from a 3-week field trial of our prototype system called ThermoKiosk, which we deployed in an open plan, shared office with a history of thermal comfort complaints. In interviews with occupants and members of facilities management, we find that the data and interactions can play an important role in initiating dialogue to understand and handle tensions, and point to design considerations for more systematically integrating them into workplace comfort practices. In this paper, we presented a study of ThermoKiosk to understand the utility of subjective thermal comfort data in workplace comfort management. Through a qualitative study with building managers and occupants, we uncover much about digital comfort surveys as a new facet in participatory comfort management. We see valuable roles for interaction design in supporting the everyday expression of subjective comfort experience, and in providing a medium for negotiating and developing the knowledge, competences, meanings, and politics required for effective collective comfort practices. For Management, the data brings into question how comfort is approached in institutional policies and management practices, but further work is required to find ways to more systemically integrate it in ways that challenge these. In establishing thermal comfort as an ‘object of concern’, these findings point to interesting potential for sustainability applications: first in engaging occupants and management with alternative approaches to thermal comfort where the energy required is more closely linked to comfort experience; and secondly, in providing a platform for the establishment of new, sustainable practices.
"
"The Privilege of Immersion: Racial and Ethnic Experiences, Perceptions, and Beliefs in Digital Gaming",https://dl.acm.org/authorize?N657195,"
People of color comprise a large proportion of the US player base, yet are systematically and grossly underrepresented in digital games. We constructed a survey to assess if players perceive this underrepresentation, how they experience these representations, and sample their beliefs about diversity and gaming. Mixed-methods analyses show significant differences between players of color and White players on perception of racial norms in gaming, effects on behavior, emotions, player satisfaction, engagement, and beliefs stemming from a lack of diversity. Players from all races-ethnicities overwhelmingly expressed a desire for greater diversity. We discuss reasons why our methodology shows higher dissatisfaction than previous research and discuss our findings in the context of industry's challenge to meet audience demands for greater racial diversity in games. White players appear experientially satisfied with their racial-ethnic representation in games, whereas players of color are employing common strategies to meet their needs under unnecessary discrimination. If players game to represent who they, engage and immerse themselves with greater freedom than life outside of gaming allows, it is those who face systematic discrimination that have the most to gain from changes in digital gaming’s norms. Our study demonstrates that White players understand some level of what players of color experience and feel they deserve equal opportunity to be immersed in games. Diversity has been shown to be wanted by players of all races-ethnicities and to benefit players of all races-ethnicities. If digital gaming intends to represent humans then it must do so carefully, as it risks harmful negative representations. Accuracy, collaboration, and information is thus paramount to addressing these issues. Much more than a low-risk high-reward industry choice, appropriate racial/ethnic representation and diversity is a social and moral demand—with repercussions on a host of biopsychosocial factors. If we have the resources, talent, technology, information, and creativity to create fantastical game worlds—to allow players to become superheroes, celebrities, imaginary creatures, and White— why can we not allow players of color to be themselves?
"
Stitching Infrastructures to Facilitate Telemedicine for Low-Resource Environments,https://dl.acm.org/authorize?N657196,"
Telemedicine can potentially transform healthcare delivery in low-resource environments by enabling extension of medical knowledge to remote locations, thus enhancing the efficiency and effectiveness of the larger healthcare infrastructure. However, empirical studies have shown mixed results at best. We present a qualitative investigation of a long-standing telemedicine program operating from Lucknow (Uttar Pradesh, India). Invoking the lenses of human infrastructure and seamful spaces, we highlight the factors that determine the success of this telemedicine program. We identify and describe three important aspects: (1) conceptualizing telemedicine as the connectedness of two nodes rather than doctors and patients alone, (2) identifying the critical 'carrying agent' (local doctors at peripheral nodes) and engaging them in program design and implementation, and (3) ensuring co-creation by engaging patients in the process. Finally, we discuss how our lenses allowed us to recognize the seams made visible through the juxtaposition of the infrastructures at the central and peripheral nodes, and to emphasize the human elements that addressed these seams for ensuring the facilitation of a successful telemedicine program. We conducted a qualitative investigation of a telemedicine program in India that has widely been considered successful in the Indian healthcare context. This program entailed the provision of tele-consultations for thyroid cancer patients from South-East India (Orissa). The central node was located in a super-specialty hospital in North India (Lucknow, Uttar Pradesh). Our findings unpacked the roles of diverse stakeholders in the human infrastructures that were engaged by this program, both at the center and the periphery. First, we laid out the role of the coordinators who ensured that a working connection was possible and information regarding patients’ conditions and doctors’ schedules was shared and managed effectively in a bidirectional flow. Second, we examined the role of the local physicians/surgeons who ensured that patients were at ease and their concerns were addressed. We also found their medical and technological skills to mature over time. Third, we studied the involvement of patients to highlight how they remained active throughout the telemedicine process (before, during, and in-between consultations) instead of being passive recipients of medical knowledge and guidelines. In addition to detailing the human infrastructures involved in this telemedicine set-up, we studied how the infrastructures at the center and the periphery were stitched together to facilitate tele-consultations. In particular, we drew attention to the seams that were appropriately addressed by the human infrastructures in this NIMR program to allow for seamless facilitation of telemedicine.
"
BIGFile: Bayesian Information Gain for Fast File Retrieval,https://dl.acm.org/authorize?N657197,"
We introduce BIGFile, a new fast file retrieval technique based on the Bayesian Information Gain framework. BIGFile provides interface shortcuts to assist the user in navigating to a desired target (file or folder). BIGFile's split interface combines a traditional list view with an adaptive area that displays shortcuts to the set of file paths estimated by our computationally efficient algorithm. Users can navigate the list as usual, or select any part of the paths in the adaptive area. A pilot study of 15 users informed the design of BIGFile, revealing the size and structure of their file systems and their file retrieval practices. Our simulations show that BIGFile outperforms Fitchett et al.'s AccessRank, a best-of-breed prediction algorithm. We conducted an experiment to compare BIGFile with ARFile (AccessRank instantiated in a split interface) and with a Finder-like list view as baseline. BIGFile was by far the most efficient technique (up to 44% faster than ARFile and 64% faster than Finder), and participants unanimously preferred the split interfaces to the Finder. We presented BIGFile, a fast navigation-based file retrieval technique where the computer is trying to gain information from the user by providing shortcuts that may help access the target faster. These shortcuts are presented in a split adaptive area of a file retrieval interface and include the estimated files or folders selected by our computationally efficient algorithm BIGFileFast, which together with the items in the current folder, maximize the expected information gain from the next user input. The interface includes the paths to the estimated items so that contextual information is provided to identify them. Users can use any shortcut in the adaptive area or simply navigate the hierarchy as usual. We first ran a pilot study to better understand users’ file structures and retrieval practices. We ran simulations demonstrating the effectiveness and accuracy of BIGFileFast compared to the AccessRank prediction algorithm in various hierarchical structures. We also ran an experiment comparing BIGFile with ARFile, a split interface using AccessRank, and with a Finderlike list view as baseline. BIGFile was up to 44% faster than ARFile and 64% faster than Finder, and users unanimously preferred the split interfaces. Future work includes improving BIGFile by adding a stability parameter and potentially repeated user behavior. We also plan to evaluate BIGFile in a longitudinal study, and explore applications of the BIGFileFast algorithm to other areas.

"
The Use of Private Mobile Phones at War: Accounts From the Donbas Conflict,https://dl.acm.org/authorize?N657198,"
Studying technology use in unstable and life-threatening conditions can help highlight assumptions of use built into technologies and foreground contradictions in the design of devices and services. This paper provides an account of how soldiers, volunteers, and civilians use mobile technologies in wartime, reporting on fieldwork conducted in Western Russia and Eastern Ukraine with people close to or participating directly in the armed conflict in the Donbas region. We document how private mobile phones and computers became a crucial but ambiguous infrastructure despite their lack of durability in extreme conditions of a military conflict, and their government and military surveillance potential. Our participants rely on a combination of myths and significant technical knowledge to negotiate the possibilities mobile technologies offer and the life-threatening reality of enemy surveillance they engender. We consider the problems of always-on always-connected devices under conditions of war and surveillance and our responsibilities as HCI practitioners in the design of social technologies. Our study demonstrates that in considerations of mobile technologies two points are important from the point of view of HCI research and design. First, any mobile technology-based solution relies on availability of infrastructures supporting telephony, and must consider who owns and controls these. Second, data sent across mobile infrastructures is always vulnerable and makes visible its producers and consumers. Such visibility can have significant and even life-threatening consequences, calling for a rethinking of design objectives for consumer communication devices. Given the rapid development of military surveillance technologies in an increasingly unstable world, what does it mean to be digitally visible? We argue that users of social media and communication technologies have a right to an understanding of surveillance practices by the state, the platforms and the military industrial complex. It is crucial to ask: How might HCI help make surveillance detection a mundane, everyday practice? How might we enable people to respond in ways that can protect and support them?
"
ConceptScape: Collaborative Concept Mapping for Video Learning,https://dl.acm.org/authorize?N657199,"
While video has become a widely adopted medium for online learning, existing video players provide limited support for navigation and learning. It is difficult to locate parts of the video that are linked to specific concepts. Also, most video players afford passive watching, thus making it difficult for learners with limited metacognitive skills to deeply engage with the content and reflect on their understanding. To support concept-driven navigation and comprehension of lecture videos, we present ConceptScape, a system that generates and presents a concept map for lecture videos. ConceptScape engages crowd workers to collaboratively generate a concept map by prompting them to externalize reflections on the video. We present two studies to show that (1) interactive concept maps can be useful tools for concept-based video navigation and comprehension, and (2) with ConceptScape, novice crowd workers can collaboratively generate complex concept maps that match the quality of those by experts. This paper presents ConceptScape, a system that generates and presents a concept map for lecture videos. We introduce a crowdsourcing workflow to engage workers to collaboratively generate a concept map by prompting them to externalize reflections on the video. We evaluate our crowdsourcing workflow on Mechanical Turk. The result shows that crowd workers collaboratively generated concept maps that match the quality of those generated by experts. In addition, the flexible task design of the workflow promotes workers to contribute more than required, while they generally perceived performing the task to be helpful for learning. We also show that watching video with an interactive concept map can support concept-based video navigation and comprehension.

"
Expressive Time Series Querying with Hand-Drawn Scale-Free Sketches,https://dl.acm.org/authorize?N657190,"
We present Qetch, a tool where users freely sketch patterns on a scale-less canvas to query time series data without specifying query length or amplitude. We study how humans sketch time series patterns --- humans preserve visually salient perceptual features but often non-uniformly scale and locally distort a pattern --- and we develop a novel matching algorithm that accounts for human sketching errors. Qetch enables the easy construction of complex and expressive queries with two key features: regular expressions over sketches and relative positioning of sketches to query multiple time-aligned series. Through user studies, we demonstrate the effectiveness of Qetch's different interaction features. We also demonstrate the effectiveness of Qetch's matching algorithm compared to popular algorithms on targeted, and exploratory query-by-sketch search tasks on a variety of data sets. In this paper, we introduced Qetch, a query-by-sketch tool for time series data. We conducted a crowd study to learn how humans sketch time series. We observed that participants often preserve and exaggerate the visually salient features of the reference time series they are sketching. We designed Qetch’s matching algorithm to consider and tolerate such distortions. As our evaluation demonstrates, Qetch’s sketchcentric design is powerful and expressive: through sketch annotations, users can effectively construct complex regularexpression queries and queries over multiple time-aligned series. Qetch outperforms standard algorithms on targeted and exploratory search tasks. Finally, we publicly release our crowd-sourced data set of sketches and source code [34].

"
Prayana: Intermediated Financial Management in Resource-Constrained Settings,https://dl.acm.org/authorize?N657191,"
We describe the design of a novel mobile phone-based application for loan management in a resource-constrained setting. In this setting, a social enterprise manages auto-rickshaw loans for drivers, taking charge of collections. The design was informed by an ethnographic study which revealed how loan management for this financially vulnerable population is a daily struggle, and loan payment is a collaborative achievement between collectors and drivers. However, drivers and collectors have limited resources to-hand for loan management. To address this, we designed Prayana, an intermediated financial management app. Prayana shares the principles of many persuasive technologies, such as education, motivation, and nudges, but is designed for users with a range of print, technical, and financial literacies and embodies the core design sensibility of enhancing users' agency. Furthermore, it does not put the onus solely on drivers to better manage their money, instead it aims to enhance the collaborative work of loan management, supporting both the drivers and collectors. In many ways, TWU represents the challenges typical to small business and NGOs operating in low resource setting, including an ad hoc mixture of process and practice built up over time, limited resources to invest in technology, and a rapidly changing business model. We, therefore, believe that our design sensibilities - to support agency, rather than to control and direct; sensitivity to the problems of formalization, and the complexity/flexibility trade-off, and the use of technology for amplification rather than automation of human work - have a wider applicability. Although not widely field tested yet, the design of Prayana is deeply grounded in the findings of an ethnographic study and an iterative design process. At each stage of the design and user testing, we found improved understanding and ease of use by participants. Furthermore, we have just started a field deployment and TWU has transferred its entire collections (currently consisting of four collectors, eight community organizers and around 200 drivers) onto Prayana, giving a clear indication of perceived benefit. This, and the fact that these methods have proved successful elsewhere [38], gives us confidence in the validity of our design. In future work, we will report on the results of the field trial, where we are collecting quantitative data (driver survey, payment data, use logs) and qualitative data (ethnographic observation of the system in use, including field agents and back office staff, interviews with drivers and their families, collectors and community organizers). Finally, we hope to introduce a social competition for drivers and collectors, as well as to release a driver’s app for the increasing number of drivers with smartphones.
"
ChromaGlasses: Computational Glasses for Compensating Colour Blindness,https://dl.acm.org/authorize?N657192,"
Prescription glasses are used by many people as a simple, and even fashionable way, to correct refractive problems of the eye. However, there are other visual impairments that cannot be treated with an optical lens in conventional glasses. In this work we present ChromaGlasses, Computational Glasses using optical head-mounted displays for compensating colour vision deficiency. Unlike prior work that required users to look at a screen in their visual periphery rather than at the environment directly, ChromaGlasses allow users to directly see the environment using a novel head-mounted displays design that analyzes the environment in real-time and changes the appearance of the environment with pixel precision to compensate the impairment of the user. In this work, we present first prototypes for ChromaGlasses and report on the results from several studies showing that ChromaGlasses are an effective method for managing colour blindness.

"
Face Value?,https://dl.acm.org/authorize?N657193,"
We are interested in increasing the ability of groups to collaborate efficiently by leveraging new advances in AI and Conversational Agent (CA) technology. Given the longstanding debate on the necessity of embodiment for CAs, bringing them to groups requires answering the questions of whether and how providing a CA with a face affects its interaction with the humans in a group. We explored these questions by comparing group decision-making sessions facilitated by an embodied agent, versus a voice-only agent. Results of an experiment with 20 user groups revealed that while the embodiment improved various aspects of group's social perception of the agent (e.g., rapport, trust, intelligence, and power), its impact on the group-decision process and outcome was nuanced. Drawing on both quantitative and qualitative findings, we discuss the pros and cons of embodiment, argue that the value of having a face depends on the types of assistance the agent provides, and lay out directions for future research. We revisited the longstanding debate on the necessity of embodiment for conversational agents in a new context–group facilitation. Consistent with previous work, we showed that the embodiment improved various dimensions of subjective perceptions of the agent, but its effect on the objective task performance was less evident. However, in the group context, we found evidence that the embodiment had positive influence on group dynamics and invited more pro-active interactions. Our qualitative results suggested these phenomenon could be potentially explained by an enhanced social presence of a agent continuously “being there”, more intuitive and pleasant interactions with multi-modalities, and higher task capabilities attributed to the more lifelike visual character. Although the cost of developing additional modalities does not always justify the benefit of embodiment, we suggest that embodiment is a valuable feature for CAs in collaborative contexts, especially when social influence such as rapport, trust and power is beneficial for the task, when the agent activities require continuous user attention, and when it involves collaboration and mixed-initiative interactions between human and the agent.
"
Coding Tactile Symbols for Phonemic Communication,https://dl.acm.org/authorize?N657194,"
We present a study to examine one's learning and processing capacity of broadband tactile information, such as that derived from speech. In Study 1, we tested a user's capability to recognize tactile locations and movements on the forearm in the presence of masking stimuli and determined 9 distinguishable tactile symbols. We associated these symbols to 9 phonemes using two approaches, random and articulation associations. Study 2 showed that novice participants can learn both associations. However, performance for retention, construction of words and knowledge transfer to recognize unlearned words was better with articulation association. In study 3, we trained novel participants to directly recognize words before learning phonemes. Our results show that novel users can retain and generalize the knowledge to recognize new words faster when they were directly train on words. Finally, Study 4 examined optimal presentation rate for the tactile symbols without compromising learning and recognition rate.
"
X-Ray Refine: Supporting the Exploration and Refinement of Information Exposure Resulting from Smartphone Apps,https://dl.acm.org/authorize?N657105,"
Most smartphone apps collect and share information with various first and third parties; yet, such data collection practices remain largely unbeknownst to, and outside the control of, end-users. In this paper, we seek to understand the potential for tools to help people refine their exposure to third parties, resulting from their app usage. We designed an interactive, focus-plus-context display called X-Ray Refine (Refine) that uses models of over 1 million Android apps to visualise a person's exposure profile based on their durations of app use. To support exploration of mitigation strategies, emphRefine can simulate actions such as app usage reduction, removal, and substitution. A lab study of emphRefine found participants achieved a high-level understanding of their exposure, and identified data collection behaviours that violated both their expectations and privacy preferences. Participants also devised bespoke strategies to achieve privacy goals, identifying the key barriers to achieving them. Many of the smartphone apps people see as vital to daily life have been—for various economic, technical, and social reasons—driven to share data to many third parties. Some of this sharing is simply a side effect of functionality integration, whilst others are deliberate attempts to harvest information about people for purposes such as marketing or analytics. The essential challenge that the work presented in this paper addresses is the fact that people remain largely unaware of this network of entities with whom their apps are sharing data. In a world where cybersecurity breaches impact millions of people on a frequent basis, this work explores whether a greater awareness of such information sharing practices could help people articulate privacy goals, and to devise practical strategies to achieve them. Evaluation of our interface prototype, X-Ray Refine, demonstrated that people could effectively and accurately deduce essential properties of their exposure from a visualisation of a privacy profile model, derived from simple static analysis of apps they used. Participants used both focus and overview elements of the interactive visualisation, spending the most time viewing the effects of particular actions (adding, removing, adjusting duration of use, and substituting apps) on their exposure. During reflection, they provided a variety of viewpoints on what they felt about apps’ data collection activities and the companies behind them. Most interestingly, people derived different strategies as they sought to achieve their idiosyncratic privacy goals. The diversity of such goals and the effectiveness with which these individuals came up with realistic strategies, highlights the potential advantages of our approach—and of interfaces, in general, that directly support user judgement and reasoning, through sensemaking, simulation, and mitigation affordances. We propose that such interfaces afford greater flexibility, transparency, and control than AI or recommender systems that take people out of the loop by making decisions for them. Finally, our study found many promising avenues for further exploration. One idea was, for example, to support participants in understanding whether there were particular data protection regulations or frameworks in place in the jurisdictions where their data were held. Another avenue for exploration involved helping people to easily make sense of the trustworthiness of third parties through a ‘data reputation score’ derived from their maturity and history of security incidents. We feel that this work has opened up a number of potentially fruitful lines of further investigation towards greater end-user awareness and control over their privacy.

"
Controlling Maximal Voluntary Contraction of the Upper Limb Muscles by Facial Electrical Stimulation,https://dl.acm.org/authorize?N657106,"
In this paper, we propose to use facial electrical stimulation to control maximal voluntary contraction (MVC) of the upper limbs. The method is based on a body mechanism in which the contraction of the masseter muscles enhances MVC of the limb muscles. Facial electrical stimulation is applied to the masseter muscles and the lips. The former is to enhance the MVC by causing involuntary contraction of the masseter muscles, and the latter is to suppress the MVC by interfering with voluntary contraction of the masseter muscles. In a user study, we used electromyography sensors on the upper limbs to evaluate the effects of the facial electrical stimulation on the MVC of the upper limbs. The experimental results show that the MVC was controlled by the facial electrical stimulation. We assume that the proposed method is useful for sports athletes because the MVC is linked to sports performance. We proposed a novel method to control maximal voluntary contraction (MVC) of the upper limb muscles by facial electrical stimulation. In the method, facial electrical stimulation is applied to the masseter muscles to enhance the MVC of the upper limb muscles by causing involuntary contraction of the masseter muscles, and to the lips to suppress the MVC by interference with voluntary contraction of the masseter muscles. The method aims to control the muscles of the whole body with a few electrodes placed on the face utilizing the body mechanism of human motor activity. We evaluated the method under the condition of isometric contraction of the upper limb muscles. The results showed that the involuntary contraction evoked by EMS to the masseter muscles increased the MVC of the upper limb muscles, and the electrical stimulation to the lips interfered with the voluntary contraction of the masseter muscles. In future work, we will investigate the method’s feasibility under more complex motor activity conditions.

"
How to Design a Digital Storytelling Authoring Tool for Developing Pre-Reading and Pre-Writing Skills,https://dl.acm.org/authorize?N657107,"
In the paper we describe an exploration into the design of an authoring tool to support the creation of multimedia stories. We explicitly targeted children with no reading or writing skills and their educators. Children in this age group often enjoy reading and creating stories together with adults and in so doing develop important pre-literacy skills. Literature suggests that when children play an active role in these activities, with a high level of engagement and interaction, there is a significant increase in their vocabulary acquisition and an improvement in their communication skills. Thus, we investigated these issues by conducting an explorative study in a pre-school class with fifteen children and three teachers. Here, we describe the emerging challenges and provide design directions for an authoring system to support the co-creation of stories for pre-literate children. Literature indicates that pre-literate children can find a benefit in co-reading and co-creating stories with adults [18], with their peers [14]. Indeed, the use of technology can amplify this experience and provide a concrete support in the development of language and communication skills [4]. In our explorative study, we considered this activity in a formal educational context, and the opportunities and challenges of designing an authoring tool for narrative skills development. The emerging challenges are described in order to inform the design process towards the implementation of a working prototype to be tested in preschools. Other research projects addressed similar issues. For instance, Q–Tales [16] focuses on the co-creation of narrative stories with an educational flavour but outside the school environment. Indeed, designing for and with schools’ stakeholders has many specific constraints as well as opportunities. The results of our study are somewhat limited by the small sample of subjects included and the curriculum, which is specific to this European country, Switzerland.
"
"Non-Native English Speakers Learning Computer Programming: Barriers, Desires, and Design Opportunities",https://dl.acm.org/authorize?N657108,"
People from nearly every country are now learning computer programming, yet the majority of programming languages, libraries, documentation, and instructional materials are in English. What barriers do non-native English speakers face when learning from English-based resources? What desires do they have for improving instructional materials? We investigate these questions by deploying a survey to a programming education website and analyzing 840 responses spanning 86 countries and 74 native languages. We found that non-native English speakers faced barriers with reading instructional materials, technical communication, reading and writing code, and simultaneously learning English and programming. They wanted instructional materials to use simplified English without culturally-specific slang, to use more visuals and multimedia, to use more culturally-agnostic code examples, and to embed inline dictionaries. Programming also motivated some to learn English better and helped clarify logical thinking about natural languages. Based on these findings, we recommend learner-centered design improvements to programming-related instructional resources and tools to make them more accessible to people around the world. This paper contributes, to our knowledge, the largest-scale qualitative investigation so far of human language on learning programming. By analyzing 840 survey responses spanning 86 countries and 74 native languages, we discovered a set of barriers faced by non-native English speakers (as reported by both native and non-native speakers), along with their desires for improving instructional materials. We then proposed learner-centered design ideas [37] for making instructional resources and tools more accessible to non-native English speakers. This work brings us closer to a future where the hopeful promise of Computer Science For All [53] broadens its scope to include people from all language backgrounds. More generally, programmers are a form of lead users [78] who are “ahead of the curve” in terms of willingness to both adopt and adapt existing resources to suit their needs. Thus, designing for novice programmers could pave the way for making other creative technical skills more accessible to people from all sorts of language and cultural backgrounds. It is hard to predict exactly what new creative skills will be in demand in the coming decades, but making sure that they will be maximally accessible to a global audience will become even more important as the world grows more interconnected. By empowering more future creators from diverse backgrounds, the artifacts that end up being created will hopefully in turn become more representative of the needs of the world.
"
The Ambient Birdhouse: An IoT Device to Discover Birds and Engage with Nature,https://dl.acm.org/authorize?N657109,"
We introduce the Ambient Birdhouse, a novel IoT design for the home that seeks to encourage awareness and discovery of birds outside. People increasingly have routines and technologies that disconnect them from nature. Moreover birds are hard to come to know, seen but not heard, heard but not seen, or simply around when we are not. The Ambient Birdhouse aims to reconcile these positions, by using local bird media to leverage people's playfulness and curiosity, calmly sustain interest over time and ultimately to garner interest and engagement in nature and conservation projects. We trialled the Ambient Birdhouse with five families. Key findings are that the playful nature of the Birdhouse has an immediate grasp on children, and through them on the rest of the family. Children were prompt to learn bird calls, and invented and played games that involved the Birdhouse. Learning strategies emerged spontaneously from family routines and arrangements, with each family creating different moments and spaces for learning. This paper introduced a novel IoT design for the home aimed at encouraging the discovery of birds, and through this, at fostering engagement in nature. The Ambient Birdhouse has been trialed with 5 families, and their insights and feedback have been gathered through interviews and a diary. Our key findings showed how participants used the Birdhouse as an opportunity to socialize, to share what they were learning and to discuss possible strategies for learning. The position in the home assigned to the Birdhouse was determinant in shaping its use, as were the particular routines in which the birdhouse was integrated. Participants offered many valuable suggestions as to what they may want to include in future versions of the Birdhouse, and how they preferred to interact with it. The bird-cards were particularly well received, as children were able to invent new ways to play with them, and to involve their parents in those games. Although assessing the learning outcomes was out of our scope, all participants noted that they were more alert to birds and more aware of their calls. They were more curious and keen to discover new ways of telling different species apart, and often took videos or pictures to share in their social networks to seek advice about a bird. Children discussed the Birdhouse with their friends at schools, and were keen to demonstrate it to Grandparents on Skype. We identified some key motivations that may be leveraged in the future to sustain engagement. Parents were happy to have their children engaged with a technology that is safer that browsing the Web or using social media, and is not isolating as a personal device. Children were keen to appropriate the playful elements of the design in ways that matched their age, skills and interests. We offered three key implications for design of IoT technologies for discovering and engaging with nature. We propose a novel perspective on gamification to embrace elements of play platforms, such as cards or bricks, rather than game elements such as leaderboards, in order to allow users to appropriate those elements into their own games. We propose to revisit probe and diary methods in the light of the great potentials of IoT design, in order to embed in the future designs the tools for learning and self-reflection that are normally confined to the academic discourse, making them accessible to the users for a true participation in design research. We finally suggest to conceptualize IoT technologies as catalysts of shared spaces for learning and socialization, rather than through the lens of their sensing and communicating capabilities, in order to foreground their potentials to foster engagement and discovery.

"
Pocket Skills: A Conversational Mobile Web App To Support Dialectical Behavioral Therapy,https://dl.acm.org/authorize?N657100,"
Mental health disorders are a leading cause of disability worldwide. Although evidence-based psychotherapy is effective, engagement from such programs can be low. Mobile apps have the potential to help engage and support people in their therapy. We developed Pocket Skills, a mobile web app based on Dialectical Behavior Therapy (DBT). Pocket Skills teaches DBT via a conversational agent modeled on Marsha Linehan, who developed DBT. We examined the feasibility of Pocket Skills in a 4-week field study with 73 individuals enrolled in psychotherapy. After the study, participants reported decreased depression and anxiety and increased DBT skills use. We present a model based on qualitative findings of how Pocket Skills supported DBT. Pocket Skills helped participants engage in their DBT and practice and implement skills in their environmental context, which enabled them to see the results of using their DBT skills and increase their self-efficacy. We discuss the design implications of these findings for future mobile mental health systems. We created Pocket Skills, a mobile web app designed to support Dialectical Behavioral Therapy (DBT). Pocket Skills guides people through DBT education and skills practice via eMarsha, a conversational agent modeled on Marsha Linehan, the developer of DBT. We conduced a 4-week field study of 73 participants to test the feasibility of using Pocket Skills to support DBT. After the study, all participants showed significant improvement in depression, anxiety, and DBT skills use. In addition, we contribute a model of how Pocket Skills supported DBT based on our qualitative analysis of open-ended questions in the exit survey. Participants reported that Pocket Skills helped them engage both in the app and in DBT in general. This engagement helped them learn the principles and practice the skills in context, helping them implement those skills in their everyday lives. Participants were therefore able to see the concrete results of implementing their DBT skills and improve their self-efficacy. We discussed the necessity of technology being available and engaging to support mental health treatment, as well as the importance of visualizing history and improvement, considering therapist needs, and supporting people without access to in-person therapy. This work therefore motivates future study, design, and development of tools to support mental health.
"
The Application and Its Consequences for Non-Standard Knowledge Work,https://dl.acm.org/authorize?N657101,"
Application-centric computing dominates human-computer interactions, yet the concept of an application is ambiguous and the impact of its ubiquity underexplored. We unpack ""the application"" through the lens of non-standard knowledge work: freelance, self-employed, and fixed-term contract workers who create knowledge in collaboration with a wide variety of stakeholders on a per-project basis. Based on interviews with fourteen participants we describe how: i) their economic value is intertwined with data and skills related to specific applications; ii) their access to this value is systematically jeopardised in collaboration due to the different application practices, preferences, and proficiencies of other stakeholders; and iii) they mitigate the costs of this compromise through cross-application collaboration strategies. We trace these experiences to common characteristics of applications, such as update processes, interface symmetries, application-document relationships, and operating system and hardware dependencies. By empirically and analytically focusing on ""the application"", we reveal the implications of the current application-centric computing paradigm and discuss how variations within this model create qualitatively different human-computer interactions. The application represents a particular way of packaging interaction with computation. We explored how the common characteristics of applications reveal themselves through the problems and opportunities experienced by non-standard knowledge workers. We found how the economic value of these workers is tied up with the applications they use based on the skills and data they invest in them and how this relationship affects the efficient, effective, and enjoyable execution of their tasks. However, the technical tendencies of application-centric computing require non-standard knowledge workers to regularly abandon their preferred choice and instead switch to applications they are unfamiliar with or which change their ability to produce. At the same time, not all applications are identical and the extent and way in which users have to adapt can vary, based on the way the application relates to the document, different versions of the same application, other applications, and the operating system. The way application characteristics vary shows how they exist on a spectrum and how alternative models can create qualitatively different user experiences. Given the central position of the application in our interaction with computation, empirically exploring the application-centric computing paradigm is imperative in order to further reveal what defines an application, how variations matter, and what alternatives are yet to be explored.
"
Digital Konditorei: Programmable Taste Structures using a Modular Mold,https://dl.acm.org/authorize?N657102,"
Digital Gastronomy (DG) is a culinary concept that enhances traditional cooking with new HCI capabilities, rather than replacing the chef with an autonomous machine. Preliminary projects demonstrate implementation of DG via the deployment of digital instruments in a kitchen. Here we contribute an alternative solution, demonstrating the use of a modular (silicone) mold and a genetic mold-arrangement algorithm to achieve a variety of shape permutations for a recipe, allowing the control of taste structures in the dish. The mold overcomes the slow production time of 3D food printing, while allowing for a high degree of flexibility in the numerous shapes produced. This flexibility enables us to satisfy chefs' and diners' diverse requirements. We present the mold's logic, arithmetic, design and special parts, the evolutionary algorithm, and a recipe, exploiting a new digital cooking concept of programmable edible taste structures and taste patterns to enrich user interaction with a given recipe. In this work we contribute a new procedure to the growing body of DG research and developments, to allow chefs to program the taste of food, enabling for local computational control of taste. We propose to use a modular mold to cast edible substances. By using a mold with a large number of shape permutations, we accelerate the 3D forming stage compared to food printing: while it can take hours to print a shape with an edible paste, casting the same shape is immediate. Yet we are aware that after casting we still need to stabilize the liquid substance, which may take significant time, depending on the method. For example, we demonstrate the use of a modular mold with a mousse recipe, which needs to be fully frozen to stabilize and solidify it. Although this takes time, there are methods to accelerate the process, such as using liquid nitrogen that we did not explore in detail.

"
Exploring the Weak Association between Flow Experience and Performance in Virtual Environments,https://dl.acm.org/authorize?N657103,"
Many studies conducted in non-virtual activities have shown that flow significantly influences performance, yet studies in virtual activities often reveal only a weak association. This paper begins by building a theoretical explanatory model, and then conducts 3 empirical studies to explore this question. Study 1 exams the mechanism of weak association in two virtual activities. Study 2 tests the effectiveness of a potential approach to strengthen this association. In Study 3 we applied our proposed model and design approach to optimize a VR tennis game. Results show that the influence of flow on performance was not significant in those virtual activities where the primary task and the operation of interactive artifacts were less congruent such that the artifacts can lead to flow experience that is independently of the primary task. Our research offers a theoretical and empirical basis on how to optimize virtual environment design and maximize positive effect of the flow experience.
"
Understanding the Mundane Nature of Self-care: Ethnographic Accounts of People Living with Parkinson's,https://dl.acm.org/authorize?N657104,"
Self-care technologies have been influenced by medical values and models. One of the values that was acritically incorporated was that self-care was medicalised, and, as a result, technologies were designed to afford use with clinicians and fit structured medical processes. This paper seeks to broaden the understanding of self-care in HCI, to acknowledge the mundane ways in which self-care is achieved. Drawing on in-depth interviews with patients and carers, and online ethnography of an online community, we describe how the self-care of Parkinson's is mundane. The fieldwork contrasts with more medicalised perspectives on self-care, thus we discuss the properties of a self-care concept that would acknowledge its mundane nature. Our hope is to sensitise designers to identify the mundane ways in which self-care is performed and, consequently, design technologies that better fit the complexities of everyday life with a chronic condition. This paper presented the everyday practices of the self-care of Parkinson’s. It was clear that numerous challenges of Parkinson’s were mundane and related with the need to practically adapt to the condition. These findings question the common assumption that self-care equates with monitoring symptoms or performing treatment and thus point to the need of approaching self-care through a different lens. Recognising the mundane nature of self-care entails understanding that performing self-care requires great work in organising and conducting activities. The activities are not isolated, but rather intertwined and deeply ingrained in the everyday life of specific people. Also, as not everything is possible at the same time, self-care is the result of negotiations and compromises that are dynamically adjusted as needed to live a life with quality.
"
EDITalk: Towards Designing Eyes-free Interactions for Mobile Word Processing,https://dl.acm.org/authorize?N657115,"
We present EDITalk, a novel voice-based, eyes-free word processing interface. We used a Wizard-of-Oz elicitation study to investigate the viability of eyes-free word processing in the mobile context and to elicit user requirements for such scenarios. Results showed that meta-level operations like highlight and comment, and core operations like insert, delete and replace are desired by users. However, users were challenged by the lack of visual feedback and the cognitive load of remembering text while editing it. We then studied a commercial-grade dictation application and discovered serious limitations that preclude comfortable speak-to-edit interactions. We address these limitations through EDITalk's closed-loop interaction design, enabling eyes-free operation of both meta-level and core word processing operations in the mobile context. Finally, we discuss implications for the design of future mobile, voice-based, eyes-free word processing interface. EDITalk is designed to bridge the gap between the user’s need for mobile eyes-free interaction with text and limitations of using existing dictation applications in trying to achieve it. Results show that EDITalk’s interaction design enables the user to achieve eyes-free word processing with high accuracy and precision levels. The prototype garnered positive feedback from all of our participants and demonstrated promising potential to design targeted solutions for specific user groups such as the visually impaired. We believe EDITalk has real potential to spur future work in the space of designing specialized eyesfree text interaction systems. This would be another step towards an exciting future in the conversational paradigm of user interactions.
"
Improving User Confidence in Concept Maps: Exploring Data Driven Explanations,https://dl.acm.org/authorize?N657116,"
Automated tools are increasingly being used to generate highly engaging concept maps as an aid to strategic planning and other decision-making tasks. Unless stakeholders can understand the principles of the underlying layout process, however, we have found that they lack confidence and are therefore reluctant to use these maps. In this paper, we present a qualitative study exploring the effect on users' confidence of using data-driven explanation mechanisms, by conducting in-depth scenario-based interviews with ten participants. To provide diversity in stimulus and approach we use two explanation mechanisms based on projection and agglomerative layout methods. The themes exposed in our results indicate that the data-driven explanations improved user confidence in several ways, and that process clarity and layout density also affected users' views of the credibility of the concept maps. We discuss how these factors can increase uptake of automated tools and affect user confidence. In this paper we report the effect of data-driven explanations on users’ confidence both in terms of their ability to understand, and explain, the layouts of automatically generated concept maps. To this aim we conducted a qualitative study using in-depth scenario-based interviews that exploited interactive, visual, explanations of constructive and reductive layout methods. During these interviews, participants used and discussed concept maps that were provided with and without our data-driven explanations. Our participants reported having stronger confidence when they used the data-driven explanations, as they provide case-specific evidence and interactivity that allows both control over the explanation’s pacing and the ability to query the underlying evidence (R1 and R2). These results were further reinforced by a frequency analysis which showed that users were proportionately more positive when discussing data-driven explanation, than when commenting on concept map layouts on their own (58% vs. 21% respectively). The participants were also provided with two different types of explanation, one for each of the layout methods. The reductive approach was based upon standard projection methods, while the constructive approach was derived from a simple agglomerative clustering technique. Our aim in providing these two methods was to enhance the diversity of the stimuli rather than perform A/B testing, as the latter would not be possible without confounding the effect of the two layout and explanation methods. The two different layout methods discussed above revealed two important design considerations that can affect the credibility of concept maps (R3 and R4). First, that layout density affects users’ perception of the clarity of the map, as it alters their ability to perceive structure (as dense packing prevents easy abstraction by users of concepts into groups). We therefore recommend that designers choose algorithms which provide variable packing densities so that they can trade screen real estate for the communication of structure and perceived clarity (R3). It should be noted that while our implementation of the agglomerative method produced dense maps in this study, this is not intrinsic to the method. In future work we plan to investigate different ways of communicating inherent structure within the map’s similarity data. Second, the two types of stimuli exposed participants’ strong preference for agglomerative or constructive approaches as opposed to the reductive projective methods in which a complex problem is presented to users and then information is gradually discarded. In particular, the repeated aggregation of pairs of concepts into clusters in the first approach means that the user’s working memory is not overloaded, which we believe greatly contributes to users’ perception of simplicity and clarity (R4). To conclude, we believe this study also highlights the need to further understand and research the underlying issues that affect user confidence when generating various visualizations using automated tools for planning, decision-making and collaborative activities.

"
Spokespeople: Exploring Routes to Action through Citizen-Generated Data,https://dl.acm.org/authorize?N657117,"
This paper presents insights from a collaboration with cycling advocates and local authorities to consider how HCI can open productive spaces for citizens to contribute to the realization of social goals. We worked with members of a walking and cycling advocacy organization to explore the potential for technology-mediated data collection to support advocacy and action taking. Based on our initial findings, we developed and deployed Spokespeople-a system to enable people who cycle to collect, curate and make visible their everyday journeys and experiences. We then worked with participants, cycling advocates and local authority transport planners to explore how citizens can contribute beyond data collection, by curating and prioritizing their experiences and exploring possible routes to action. We identify future directions for technology design to support citizens to make meaningful contributions to changes in the city through annotated routes, prioritization and community commissioning processes. This paper covers three stages of our research working with cyclists, cycling advocacy organizations and local councils to explore how citizens can not only generate data but go on to use data to create meaningful change. We have presented Spokespeople, a platform to enable cyclists to collect and curate situated journey and experiential data. We have also identified future directions for technology design to support citizens to make meaningful contributions to changes in the city. We introduce the idea of annotated routes that provide affordances for data curation, storytelling and data analysis. Finally, we also explore design challenges and opportunities for technologies to support citizen participation in processes leading to action and change that go beyond data collection. We highlight supporting issue prioritization and community commissioning processes as promising areas for further research.
"
Evaluating the Disruptiveness of Mobile Interactions: A Mixed-Method Approach,https://dl.acm.org/authorize?N657118,"
While the proliferation of mobile devices has rendered mobile notifications ubiquitous, researchers are only slowly beginning to understand how these technologies affect everyday social interactions. In particular, the negative social influence of mobile interruptions remains unexplored from a methodological perspective. This paper contributes a mixed-method evaluation procedure for assessing the disruptive impact of mobile interruptions in conversation. The approach combines quantitative eye tracking, qualitative analysis, and a simulated conversation environment to enable fast assessment of disruptiveness. It is intended to be used as a part of an iterative interaction design process. We describe our approach in detail, present an example of its use to study a new call declining technique, and reflect upon the pros and cons of our approach. This paper contributes a new mixed-method approach to measuring the disruptiveness of technology. Our new approach uses eye tracking and semi-structured interviews in a generic conversation task to offer rapid, actionable insights for designing interaction techniques that may be used in conversations. We presented an example study where we investigated techniques for declining calls in a face-to-face conversation. We were able to revisit the conversation and draw conclusions from the participants’ behavior using video and audio recordings. Using our approach enabled us to understand the impact of a new interaction technique on disruptiveness. Eye tracking revealed a significant drop in time spent looking on the phone when using the new technique. Interviews provided evidence for underlining social mechanics that affect disruptions. Designing techniques beyond existing ones to study the influence of different interaction mechanisms on conversational engagement remains an important challenge. We are eager to see how future designs will explore the design space that we merely begin to understand. As we recognize that our study is constrained by the fact that it was conducted in a lab setting, we hope that using our approach will be complemented by other studies that use alternative methods such as in-the-wild deployments of new interaction techniques. We also believe that an ethnographic study of the social acceptability of smartphone interruptions in public settings such as cafés or libraries will produce interesting insights for design. We hope that our work will inspire further developments and the creation of enhanced evaluation methods for future interaction techniques.

"
Make Yourself at Phone: Reimagining Mobile Interaction Architectures With Emergent Users,https://dl.acm.org/authorize?N657119,"
We present APPropriate -- a novel mobile design to allow users to temporarily annex any Android device for their own use. APPropriate is a small, cheap storage pod, designed to be easily carried in a pocket or hidden within clothing. Its purpose is simple: to hold a copy of the local content an owner has on their mobile, liberating them from carrying a phone, or allowing them to use another device that provides advantages over their own. Picking up another device when carrying APPropriate transfers all pertinent content to the borrowed device (using local no-cost WiFi from the APPropriate device), transforming it to give the impression that they are using their own phone. While APPropriate is useful for a wide range of contexts, the design was envisaged through a co-design process with resource-constrained emergent users in three countries. Lab studies and a subsequent deployment on participants' own devices identified key benefits of the approach in these contexts, including for security, resource sharing, and privacy. In many countries, having a mobile phone to-hand makes people feel safer [5]. In other places, however—for instance, in more resource-constrained settings—having a phone on one’s person can make someone more of a target for theft [23]. In this work we have developed and explored the concept of separating out notions of data and device. An overarching goal of our approach has been to allow the financially valuable component of a device (i.e., the phone hardware) to be disconnected from the personally valuable component (i.e., the data it holds). We have illustrated how the APPropriate approach is particularly beneficial in emergent user contexts where device security has been well documented as being a pressing issue [8, 14, 22]. This was particularly evident during evaluations in Kenya and South Africa, where many participants were worried about potential theft of their devices (Tables 1 and 2). Here, APPropriate was seen as a way to allow participants to leave their phone behind, but still have access to their data on-the-go. Our studies also showed benefits of the approach for privacy, in particular when people share devices with others. Participants were more likely to be concerned about privacy if they shared their device with other people. Our results suggest that APPropriate is a potential response to issues around privacy on shared devices – participants saw the approach as a privacy-preserving way to store content created using shared devices, and potentially safer than using on-device hiding strategies [36]. In all of our studies, participants also saw benefits of APPropriate for sharing resources, both when consumables were exhausted (e.g., data, airtime, battery), and to take advantage of better features (e.g., camera, storage, screen). The general concept of separating out data and device is already prominent in mainstream computing thanks to the cloud computing revolution (e.g., synchronising documents between devices). In our view, however, this model is currently far from seamless when applied to the complete contents of a mobile device, and the APPropriate design hints at an alternative approach.

"
Interactive Feedforward for Improving Performance and Maintaining Intrinsic Motivation in VR Exergaming,https://dl.acm.org/authorize?N657110,"
Exergames commonly use low to moderate intensity exercise protocols. Their effectiveness in implementing high intensity protocols remains uncertain. We propose a method for improving performance while maintaining intrinsic motivation in high intensity VR exergaming. Our method is based on an interactive adaptation of the feedforward method: a psychophysical training technique achieving rapid improvement in performance by exposing participants to self models showing previously unachieved performance levels. We evaluated our method in a cycling-based exergame. Participants competed against (i) a self model which represented their previous speed; (ii) a self model representing their previous speed but increased resistance therefore requiring higher performance to keep up; or (iii) a virtual competitor at the same two levels of performance. We varied participants' awareness of these differences. Interactive feedforward led to improved performance while maintaining intrinsic motivation even when participants were aware of the interventions, and was superior to competing against a virtual competitor. We proposed and evaluated interactive feedforward, a novel method to rapidly improve performance in a HIIT cycling VR exergame. Interactive feedforward is based on self competition against an improved self model of the player, such as a recording of previous gameplay. Our empirical study suggests the following conclusions, which should be considered in light of the aforementioned limitations: 1. Interactive feedforward can be effective in improving players’ performance while maintaining intrinsic motivation. 2. Interactive feedforward can still work if players are aware of the increased challenge, i.e. it does not rely on deception. 3. Interactive feedforward, and self competition in general, can be superior to competition against others, leading to higher performance, intrinsic motivation, flow and immersion. Interactive feedforward holds promise as a new method in exergames, with potential applications and opportunities in promoting positive change in people’s exercise behaviour.

"
ARcadia: A Rapid Prototyping Platform for Real-time Tangible Interfaces,https://dl.acm.org/authorize?N657111,"
Paper-based fabrication techniques offer powerful opportunities to prototype new technological interfaces. Typically, paper-based interfaces are either static mockups or require integration with sensors to provide real-time interactivity. The latter can be challenging and expensive, requiring knowledge of electronics, programming, and sensing. But what if computer vision could be combined with prototyping domain-aware programming tools to support the rapid construction of interactive, paper-based tangible interfaces? We designed a toolkit called ARcadia that allows for rapid, low-cost prototyping of TUIs that only requires access to a webcam, a web browser, and paper. ARcadia brings paper prototypes to life through the use of marker based augmented reality (AR). Users create mappings between real-world tangible objects and different UI elements. After a crafting and programming phase, all subsequent interactions take place with the tangible objects. We evaluated ARcadia in a workshop with 120 teenage girls and found that tangible AR technologies can empower novice technology designers to rapidly construct and iterate on their ideas. We have presented the design of a low-cost, toolkit for the rapid prototyping of real-time, interactive and tangible interfaces. ARcadia enables this through its built-in support for common interface elements and semantics – buttons, sliders, and knobs – and event-based control of system behavior and adjustment of continuous parameters. We have shown through our initial evaluation that ARcadia can be used productively by novice designers and programmers to create interactive musical prototypes in a short amount of time. We found that the workshop participants generally had ease in iteratively modifying their prototypes. In general, the resulting projects that the girls built would typically take hours and possibly days for even an experienced programmer to create. For example, the “Happy Birthday” AR piano, which was made over the course of 90 minutes by two firsttime programmers, would require the work of a much more experienced developer and would still likely take much longer to build. Although the workshops demonstrated ARcadia’s ease of use and ability for novices to construct interactive projects, due to space constraints and decisions we made at the beginning of the workshops the projects remained pretty limited in their functionality. Projects only incorporated the use of on or off buttons and no sliders or knobs. In the future we would like to provide an easier way for participants to construct more complex table-top interfaces. In addition, many of the workshop participants expressed a desire to use ARcadia for other projects beyond simple musical controllers.

"
Building Momentum: Scaling up Change in Community Organizations,https://dl.acm.org/authorize?N657112,"
Addressing calls in Sustainable HCI to scale up our work in HCI targeting sustainability, and the current knowledge gap of how to do this practically, we here present a qualitative study of 10 sustainability-oriented community organizations that are working to scale up their change making. They are all loosely connected to a local Transition network, meaning that they are aiming at transforming current practices in society, through local and practical action, to meet challenges related to climate change. We wanted to know how they try to scale up their change making, and what role ICT plays in enabling scaling up. The study contributes new insights about three stages of scaling up, in which ICT plays different roles. We conclude with implications for HCI for how to support community organizations in scaling up, while keeping values important for working toward a more resilient society. In this work, we have outlined how 10 sustainabilityoriented community organizations work trying to sustain and/or scale up their activities, aiming at creating change in society. We have looked at how they operate, the purpose and motivation behind their work, what obstacles they meet trying to achieve their goals, as well as what role ICT plays in supporting and/or hindering them in their work. From what we have seen, these organizations are capable of many things, but it can be hard for them to scale up on their own, especially concerning ICT use in supporting their work. They thus need to team up with others, including stakeholders with expertise in ICT and design who truly respect their values and who are in it for the long run. If we in the HCI community and society more broadly want to support the momentum that grass-roots are building up everywhere, we have a lot to offer that could help them scale up. However, scaling up needs to be more than just sharing resources; it needs to bring people together.
"
Mechanism Perfboard: An Augmented Reality Environment for Linkage Mechanism Design and Fabrication,https://dl.acm.org/authorize?N657113,"
Prototyping devices with kinetic mechanisms, such as automata and robots, has become common in physical computing projects. However, mechanism design in the early-concept exploration phase is challenging, due to the dynamic and unpredictable characteristics of mechanisms. We present Mechanism Perfboard, an augmented reality environment that supports linkage mechanism design and fabrication. It supports the concretization of ideas by generating the initial desired linkage mechanism from a real world movement. The projection of simulated movement within the environment enables iterative tests and modifications in real scale. Augmented information and accompanying tangible parts help users to fabricate mechanisms. Through a user study with 10 participants, we found that Mechanism Perfboard helped the participant to achieve their desired movement. The augmented environment enabled intuitive modification and fabrication with an understanding of mechanical movement. Based on the tool development and the user study, we discuss implications for mechanism prototyping with augmented reality and computational support. Our motivation for conducting this research was that there are many difficulties in the linkage mechanism fabrication process and it is difficult for non-experts, who lack knowledge of and experience with mechanism design, to employ it. In this paper, we have presented an augmented reality environment and a top-down approach with a novel movement input interface that enables non-experts to design and fabricate linkage mechanisms with ease. Also, we explored how the features of Mechanism Perfboard, including computational support, augmented reality, and a fabrication guide, could be involved with the linkage mechanism fabrication process. We expect this research to inspire the development of tools for kinetic mechanisms, as well as systems that use augmented reality and computational support.

"
Addressing Age-Related Bias in Sentiment Analysis,https://dl.acm.org/authorize?N657114,"
Computational approaches to text analysis are useful in understanding aspects of online interaction, such as opinions and subjectivity in text. Yet, recent studies have identified various forms of bias in language-based models, raising concerns about the risk of propagating social biases against certain groups based on sociodemographic factors (e.g., gender, race, geography). In this study, we contribute a systematic examination of the application of language models to study discourse on aging. We analyze the treatment of age-related terms across 15 sentiment analysis models and 10 widely-used GloVe word embeddings and attempt to alleviate bias through a method of processing model training data. Our results demonstrate that significant age bias is encoded in the outputs of many sentiment analysis algorithms and word embeddings. We discuss the models' characteristics in relation to output bias and how these models might be best incorporated into research. This paper systematically compares a number of popular and diverse sentiment tools, with respect to age-related bias. We find significant age-related bias among a variety of tools and commonly-used word embeddings and successfully reduce bias in a custom-built classifier. While we provide a first step in understanding how the technical characteristics of sentiment algorithms affect bias and identify one technique for reducing bias, our analysis is not exhaustive. Future work should consider additional characteristics of algorithmic models, such as the type of classifier implemented and richer model parameters. Further, researchers should consider the unique challenges of using computational techniques such as sentiment analysis to study underrepresented groups and social movements. As the “new power brokers in society,” [22] algorithms affect many aspects of life, including hiring, social policy, and finance; all of which are domains where age discrimination is common. In addition to understanding social bias in algorithms, we can use them as a lens to understand how unrecognized social bias operates at scale.
"
Identification of Imminent Suicide Risk Among Young Adults using Text Messages,https://dl.acm.org/authorize?N657125,"
Suicide is the second leading cause of death among young adults but the challenges of preventing suicide are significant because the signs often seem invisible. Research has shown that clinicians are not able to reliably predict when someone is at greatest risk. In this paper, we describe the design, collection, and analysis of text messages from individuals with a history of suicidal thoughts and behaviors to build a model to identify periods of suicidality (i.e., suicidal ideation and non-fatal suicide attempts). By reconstructing the timeline of recent suicidal behaviors through a retrospective clinical interview, this study utilizes a prospective research design to understand if text communications can predict periods of suicidality versus depression. Identifying subtle clues in communication indicating when someone is at heightened risk of a suicide attempt may allow for more effective prevention of suicide. Our study was designed to investigate temporally sensitive patterns in communication that predict acute suicidal thoughts and behaviors. By comparing communication patterns during periods immediately preceding a suicide attempt and periods of high ideation versus depression but non-suicidal periods of their life, we aim to isolate specific communication that characterizes acute suicide risk. This research provides evidence that language changes as an individual transitions from depression to suicidality (i.e., suicidal ideation and non-fatal suicide attempts), indicating an increasing level of suicide risk. Future research should explore whether individualizing the models produces better performance that is calibrated to an individual’s specific expression of language during increasing risk states. Although depression is a risk factor for suicide, research indicates that only 2-8% of individuals with a mood disorder will go on to kill themselves [2]. Therefore, depression in itself is not clinically useful for identifying high-risk individuals. Further, even if known risk factors are used to indicate high risk individuals, they cannot tell us when such individuals are at particularly elevated risk. Employing data-driven techniques, such as those developed in this study, could identify when individuals are at heightened risk and help direct appropriate resources to these individuals. As Nock et al. [29] report,“the biggest shortcoming in suicide research to date” is “the inability to dramatically decrease rates of suicidal behavior and mortality despite decades of research and associated commitment of resources.” This research may enable new ways to identify not just who is at risk for a suicide attempt, but also when a given person increases in their risk state and acutely needs services.
"
Touch Your Heart: A Tone-aware Chatbot for Customer Care on Social Media,https://dl.acm.org/authorize?N657126,"
Chatbot has become an important solution to rapidly increasing customer care demands on social media in recent years. However, current work on chatbot for customer care ignores a key to impact user experience - tones. In this work, we create a novel tone-aware chatbot that generates toned responses to user requests on social media. We first conduct a formative research, in which the effects of tones are studied. Significant and various influences of different tones on user experience are uncovered in the study. With the knowledge of effects of tones, we design a deep learning based chatbot that takes tone information into account. We train our system on over 1.5 million real customer care conversations collected from Twitter. The evaluation reveals that our tone-aware chatbot generates as appropriate responses to user requests as human agents. More importantly, our chatbot is perceived to be even more empathetic than human agents. In this paper, we first systematically study the effects of agent tones in customer care. Two tones that are beneficial for increasing user experience – passionate and empathetic – are identified accordingly. We further propose a novel deep learning based chatbot for customer care that integrates the tone information in conversations, and generates toned responses to user requests. The evaluation results suggest that our system could generate as appropriate responses as human agents. Meanwhile, the the tones embedded can be easily perceived by annotators. More importantly, it is observed that the responses generated by our system is perceived more empathetic than responses by human agents. There are many interesting and valuable directions for future work. Possible directions include studying the effects of agent tones at a finer granularity, and how the chatbot could effect the end user engagement. Meanwhile, it is also worth studying the possible extensions on our proposed model, such as a brand-aware chatbot for different brand styles.
"
Modern Bereavement: A Model for Complicated Grief in the Digital Age,https://dl.acm.org/authorize?N657127,"
The experience of grief and death is an inevitable part of life. Grief, a natural response to death, can be a challenging and emotionally taxing journey. Bereaved individuals often feel lost in a fog, unaware of resources available to them and unsure of which resources could be useful for supporting their healing process. Complicated grief, a more intense form of grief that extends beyond six months following the death of a loved one, presents both a unique challenge and a design opportunity for the HCI community. In this work, we present the results of a survey and interview study on the technological practices of complicated grievers. Based on themes found in the data, we propose a new model for complicated grief in the digital age, consisting of the following phases: Fog, Isolation, Exploration, Immersion, and Stabilization. We then present a set of design considerations for designers seeking to create tools for complicated grievers navigating their unique grief journeys. This work provides an in-depth look into how the bereaved navigate their grief in an increasingly digital world. We have introduced a behavioral model of the bereavement journey, compared online and offline grief support groups and examined the transitions the bereaved make between these support systems, and presented implications for designing systems that support the bereavement journey. In particular, we have reviewed the risk of the grief loop, and how online support groups can, if not designed carefully, contribute to prolonged suffering in bereavement. These portraits of modern grief that we have presented are by no means comprehensive. A common refrain that we heard over and over from our participants is that “everyone grieves differently”. The HCI community should heed this message and work to design technologies which respect the individualized nature of the bereavement journey. At the same time, we must provide avenues for the bereaved to connect with similar others and grow together in community. Through these efforts, we can equip grieving individuals with the tools they need to cope with their loss in the way that best fits their unique situation and goals.
"
Using Animation to Alleviate Overdraw in Multiclass Scatterplot Matrices,https://dl.acm.org/authorize?N657128,"
The scatterplot matrix (SPLOM) is a commonly used technique for visualizing multiclass multivariate data. However, multiclass SPLOMs have issues with overdraw (overlapping points), and most existing techniques for alleviating overdraw focus on individual scatterplots with a single class. This paper explores whether animation using flickering points is an effective way to alleviate overdraw in these multiclass SPLOMs. In a user study with 69 participants, we found that users not only performed better at identifying dense regions using animated SPLOMs, but also found them easier to interpret and preferred them to static SPLOMs. These results open up new directions for future work on alleviating overdraw for multiclass SPLOMs, and provide insights for applying animation to alleviate overdraw in other settings. Multiclass SPLOMs are one of hardest and least studied settings for dealing with overdraw, and there are not yet studies examining how well animation alleviates overdraw. Our work is a first step to remedying that. Specifically, we provide and demonstrate an interactive web-based tool capable of rendering of over a million points. We then studied the impact of animation on the simplest of encodings with a 69-participant user study. Participants performed better with the animated SPLOMs, found them easier to interpret and not too distracting, and preferred animated versus static SPLOMs.

"
The Context of College Students' Facebook Use and Academic Performance: An Empirical Study,https://dl.acm.org/authorize?N657129,"
The effects of Facebook on academic performance have attracted both public and scholarly attention. Prior research found that Facebook use is linked to poor academic performance, suggesting that Facebook distracts students from studying. These studies, which are primarily based on survey responses, are insufficient to uncover exactly how Facebook is used or embedded in students' studying activities. To capture unbiased, detailed use patterns and to investigate the context of Facebook use, we studied 50 college students using automatic logging and experience sampling. We analyzed the activities and attentional states of students prior to visiting Facebook. Results show that GPAs of frequent Facebook users do not suffer. Students with high GPAs spend shorter time in each Facebook session and shorter Facebook use often follows schoolwork. These results point to a possibility that potentially problematic Facebook use occurs when students are in a spree of leisure activities, not while studying. Though past studies generally point to a negative relationship between Facebook use and academic performance, our study has provided evidence that the relationship is far more nuanced. Using more precise measures of Facebook use than previous studies, we found that people who check Facebook frequently are no different in academic performance than those who check infrequently. The difference between high and low performing students appears to be in how Facebook is used each time. Low GPA students spend longer time in each use and Facebook use that follows leisure activities tend to be longer in duration. Further research is needed to understand more clearly why some students engage in brief Facebook use while others engage in prolonged use. We have suggested some directions, such as alertness and selfregulation. We have also suggested a new direction for the future investigation of social media use and academic performance in college life: examining Facebook use in leisure time.
"
"Designing Consistent Gestures Across Device Types: Eliciting RSVP Controls for Phone, Watch, and Glasses",https://dl.acm.org/authorize?N657120,"
In the era of ubiquitous computing, people expect applications to work across different devices. To provide a seamless user experience it is therefore crucial that interfaces and interactions are consistent across different device types. In this paper, we present a method to create gesture sets that are consistent and easily transferable. Our proposed method entails 1) the gesture elicitation on each device type, 2) the consolidation of a unified gesture set, and 3) a final validation by calculating a transferability score. We tested our approach by eliciting a set of user-defined gestures for reading with Rapid Serial Visual Presentation (RSVP) of text for three device types: phone, watch, and glasses. We present the resulting, unified gesture set for RSVP reading and show the feasibility of our method to elicit gesture sets that are consistent across device types with different form factors. Each type of device has its own means for input and output, which poses challenges to the consistency of interactions. Many applications are expected to work across different device types, which is why interactions need to be increasingly designed so that they are consistent and therefore easily transferable. In this paper, we demonstrate the creation of a unified gesture set that follows a user-defined elicitation approach and takes into account the transferability of gesture sets across different device types. By eliciting gesture sets for controlling reading via RSVP on wearable devices, we present the resulting gesture set for phone, watch, and glasses along with a method to design for and assess consistency. While the proposed transferability score describes the ease of transferring a gesture set from one device to another, the consistency score is a metric for a the overall goodness of a cross-device gesture set. By applying these metrics and following the three steps of 1) elicitation, 2) consolidation, and 3) validation, application designers are given a method to design, evaluate, and compare user-defined gesture sets that are consistent and easily transferable across devices with different form factors.

"
Supporting the Complex Social Lives of New Parents,https://dl.acm.org/authorize?N657121,"
One of the many challenges of becoming a parent is the shift in one's social life. As HCI researchers have begun to investigate the intersection of sociotechnical system design and parenthood, they have also sought to understand how parents' social lives can be best supported. We build on these strands of research through a qualitative study with new parents regarding the role of digital technologies in their social lives as they transition to parenthood. We demonstrate how sociotechnical systems are entangled in the ways new parents manage their relationships, build (or resist building) new friendships and ad hoc support systems, and navigate the vulnerabilities of parenthood. We discuss how systems designed for new parents can better support the vulnerabilities they internalize, the diverse friendships they desire, and the logistical challenges they experience. We conclude with recommendations for future design and research in this area. In this paper, we have provided further insight into the technological support needs of new parents, building upon previous research into the difficulties individuals experience as they transition to parenthood. Through extensive qualitative engagement, we have identified several areas where technological support is needed, particularly as governments reduce funding for social support. Our analysis of these areas provides multiple opportunities for future technological innovations that build on these known barriers that parents experience, foregrounding aspects of interpersonal care and vulnerability that are important in building and maintaining parenting relationships.
"
Algorithmic Anxiety and Coping Strategies of Airbnb Hosts,https://dl.acm.org/authorize?N657122,"
Algorithms increasingly mediate how work is evaluated in a wide variety of work settings. Drawing on our interviews with 15 Airbnb hosts, we explore the impact of algorithmic evaluation on users and their work practices in the context of Airbnb. Our analysis reveals that Airbnb hosts engage in a double negotiation on the platform: They must negotiate efforts not just to attract potential guests but also to appeal to only partially transparent evaluative algorithms. We found that a perceived lack of control and uncertainty over how algorithmic evaluation works can create anxiety among some Airbnb hosts. We present a framework for understanding this double negotiation, as well as a case study of coping strategies that hosts employ to deal with their anxiety. We conclude with a discussion of design solutions that can help reduce algorithmic anxiety and increase confidence in algorithmic systems. Software algorithms are increasingly changing how work is evaluated in an ever growing number of fields. In this paper, we explore the impact of algorithmic evaluation on workers in the context of Airbnb. Our findings from a qualitative study reveal the paradox of algorithmic transparency, and the resultant challenges in designing human-centered algorithmic evaluation systems. Algorithms are in the foreground as well as the background of different platforms. They are increasingly explicit actors in online systems but also pervasive background actors that are hard for many users to name or point out. This suggests a shift where it is not just other humans on one hand and information systems on the other, but a middle layer of seeming-human judgment and decision-making, done by algorithms, which demands more study. We argue that it is critical for designers and researchers to pay more attention to this layer especially as algorithms, ML and AI become pervasive parts of sociotechnical systems. In this paper, we discuss opportunities for designing systems that address algorithmic anxiety and foster trust between the users and the systems. We hope this work inspires future research into the impact of algorithmic evaluation on workers in other contexts.
"
More Text Please! Understanding and Supporting the Use of Visualization for Clinical Text Overview,https://dl.acm.org/authorize?N657123,"
Clinical practice is heavily reliant on the use of unstructured text to document patient stories due to its expressive and flexible nature. However, a physician's capacity to recover information from text for clinical overview is severely affected when records get longer and time pressure increases. Data visualization strategies have been explored to aid in information retrieval by replacing text with graphical summaries, though often at the cost of omitting important text features. This causes physician mistrust and limits real-world adoption. This work presents our investigation into the role and use of text in clinical practice, and reports on efforts to assess the best of both worlds---text and visualization---to facilitate clinical overview. We report on insights garnered from a field study, and the lessons learned from an iterative design process and evaluation of a text-visualization prototype, MedStory, with 14 medical professionals. The results led to a number of grounded design recommendations to guide visualization design to support clinical text overview. Clinical text overview is a challenging problem, to which data visualization has significant room to contribute. In this work, we presented a broad and extensive investigation into this question, via an iterative design approach. Our activities included (a) an initial grounding from a formative phase, based on a multidisciplinary literature review and insights from field studies that led to a list of preliminary design goals; (b) the design, development and evaluation of MedStory, a tool we designed to help assess the design principles we derived, and (c) reflections on the design of text-centered visualizations for clinical overview, reframing our initial design goals as a list of design recommendations for visualization-based systems to support clinical overview. There are also a number of relevant extensions and parallel investigations spanning from this work that have not been mentioned in the scope of our design reflections. First, extending our assessment to include other medical specialties would be an important step towards generality, as we have found some significant variations in work styles across formations. Collaboration is another relevant aspect to look into, as patient care traditionally involves coordination among several care providers, within and across specialties. From a design perspective, it would also be interesting to look at how this text-driven visualization approach can encompass the heterogeneity of the medical record to seamlessly incorporate data such as structured tables (e.g., labs) and medical imaging to the core text narrative. Ultimately, we believe there is significant room for future investigations—to which this work is an important but only initial step— and we hope to see further exploration on the topic.

"
"The SelfReflector: Design, IoT and the High Street",https://dl.acm.org/authorize?N657124,"
We describe the design of SelfReflector an internet-connected mirror that uses online facial recognition to estimate your age and play music from when it thinks you were 14 years old. The mirror was created for a specific shop (SPeX PisTOls optical boutique), within a research through design project centered on the high street as a space of vital social, economic and environmental exchange that offers a myriad of psychosocial support for people beyond a place to purchase goods. We present in detail how the design emerged as our research interests developed related to IoT and how people use the high street to experiment with, and support sense of self. We discuss SelfReflector in relation to challenges for IoT, facial recognition and surveillance technologies, mirrorness and the values of a craft approach to designing technology centering on the nature of the bespoke and 'one-off'. Our contributions through this paper are firstly SelfReflector as an innovative digital artifact, secondly a detailed description of the design process which opened up and shared much reflective thought and action through the conceptualization and making of the piece, thirdly timely reflections, on resonant issues in Design and HCI, enabled by the process of designing and making SelfReflector. Through the discussion sections we offer perspectives on: challenges to designing for IoT; limitations and inequalities to facial recognition algorithms and what this implies for IoT design and research; mirrorness as a very particular, intimate quality to reflective artifacts and the potential of this for supporting self; the merits of single function objects; the value of the bespoke and one-off; and finally the three-fold proposition for IoT on the high street represented by SelfReflector.
"
Co-constructing Family Memory: Understanding the Intergenerational Practices of Passing on Family Stories,https://dl.acm.org/authorize?N657135,"
Sharing family stories is an integral aspect of how families remember together and build a sense of connection. Yet, when generations in families are separated by large geographic and temporal distances, the everyday taken-for-granted processes of sharing family stories shift from conversational to mediated forms. To inform HCI research and practice in mediating family stories, we contribute an account of the co-constructive intergenerational social practices enacted to co-construct and interpret family stories. These practices demonstrate the agency of both storytellers and listeners as they work to discover, decipher, and reconstruct family stories. We close by drawing insights from this setting to frame key design challenges for multi-lifespan information systems mediating asynchronous, asymmetric, co-constructive and socially weighted information sharing interactions. In this paper, we contributed a detailed account of the intergenerational practices involved in passing on family memory across generations through family stories. These practices are critical components of families’ processes of coming to a shared understanding of the past. We described how family stories are collectively co-constructed by family members in fluid roles of storytellers and active listeners working to discover, decipher and reconstruct family stories. We also identify strategies and challenges that family members face when conveying life experiences, life lessons, and historical information through family stories. In our discussion, we proposed opportunities for the design of multi-lifespan information systems to account for multiple contributors, to facilitate sharing with unknown listeners, and to preserve interpretive context over time. These findings and analysis provide an important foundation for understanding and developing for human values in socio-temporal settings.
"
Seismo: Blood Pressure Monitoring using Built-in Smartphone Accelerometer and Camera,https://dl.acm.org/authorize?N657136,"
Although cost-effective at-home blood pressure monitors are available, a complementary mobile solution can ease the burden of measuring BP at critical points throughout the day. In this work, we developed and evaluated a smartphone-based BP monitoring application called textitSeismo. The technique relies on measuring the time between the opening of the aortic valve and the pulse later reaching a periphery arterial site. It uses the smartphone's accelerometer to measure the vibration caused by the heart valve movements and the smartphone's camera to measure the pulse at the fingertip. The system was evaluated in a nine participant longitudinal BP perturbation study. Each participant participated in four sessions that involved stationary biking at multiple intensities. The Pearson correlation coefficient of the blood pressure estimation across participants is 0.20-0.77 ($mu$=0.55, $sigma$=0.19), with an RMSE of 3.3-9.2 mmHg ($mu$=5.2, $sigma$=2.0). Seismo is a smartphone-based blood pressure monitoring technique that uses the built-in accelerometer to measure SCG and the smartphone’s camera to measure PPG, in order to calculate pulse transit time to estimate blood pressure. In our evaluation with nine participants, we observed that two participants exhibited signals that were too noisy for our system to produce consistent SCG signals. Further investigation into different positioning of the phone may improve the motion artifacts that occur due to someone’s hand shaking. For participants with a weaker heart beat and more fat and muscle tissues between the accelerometer and the heart impedes the transduction of the SCG. In this case, the high noise floor of the built-in accelerometer suffered in acquisition of the SCG. Of the seven participants that produced clean SCG and PPG signals for further analysis, we found the group correlation to be between 0.20-0.77 with a mean of 0.55. Although this result is a bit lower than prior work that uses similar signal source to infer pulse transit time, upon further investigation, we noted that the participant with the lowest correlation exhibited unexpected reference blood pressure measurements. For this participant, even though their pulse transit time decreased with respect to increase biking effort, their blood pressure did not follow the expected trend of increasing with increased effort. When we look at the group correlation without the worst correlated subject, the correlation becomes 0.61, comparable with most of the related work. Through this evaluation, we show that the built-in sensors of a smartphone can acquire high quality signals for use in pulse transit time based blood pressure monitoring, while also revealing some of the short comings and improvements that can potentially mitigate them.

"
Jetto: Using Lateral Force Feedback for Smartwatch Interactions,https://dl.acm.org/authorize?N657137,"
Interacting with media and games is a challenging user experience on smartwatches due to their small screens. We propose using lateral force feedback to enhance these experiences. When virtual objects on the smartwatch display visually collide or push the edge of the screen, we add haptic feedback so that the user also feels the impact. This addition creates the illusion of a virtual object that is physically hitting or pushing the smartwatch, from within the device itself. Using this approach, we extend virtual space and scenes into a 2D physical space. To create realistic lateral force feedback, we first examined the minimum change in force magnitude that is detectable by users in different directions and weight levels, finding an average JND of 49% across all tested conditions, with no significant effect of weight and force direction. We then developed a proof-of-concept hardware prototype called Jetto and demonstrated its unique capabilities through a set of impact-enhanced videos and games. Our preliminary user evaluations indicated the concept was welcomed and is regarded as a worthwhile addition to smartwatch output and media experiences. Our work introduces a smartwatch that uses lateral force feedback to enrich gaming and video experiences. We discussed the design space of this type of force feedback and its influence on human perception. Through a controlled experiment (JND study), we first investigated the minimum change in force magnitude that can be detected by users, focusing on weight and force direction. Our results revealed the JND of force magnitudes is 49% across all tested conditions, with no significant effect of weight and force direction. These results can provide useful insights for designers creating and using lateral force feedback on smartwatches. To demonstrate technical feasibility, we created a proof-of-concept prototype, composed of a small 3.5” TFT display and a pneumatic system, capable of generating thrust forces by emitting a jet of air. The orientation and location of the nozzle along the edge of the screen is controlled using several miniature motors. Finally, to demonstrate the capabilities of Jetto, we developed a set of games and videos on the device and evaluated them in a preliminary user study. Our results showed that our proposed lateral force feedback is a meaningful addition to smartwatch media experiences and output techniques.

"
ExtVision: Augmentation of Visual Experiences with Generation of Context Images for a Peripheral Vision Using Deep Neural Network,https://dl.acm.org/authorize?N657138,"
We propose a system, called ExtVision, to augment visual experiences by generating and projecting context-images onto the periphery of the television or computer screen. A peripheral projection of the context-image is one of the most effective techniques to enhance visual experiences. However, the projection is not commonly used at present, because of the difficulty in preparing the context-image. In this paper, we propose a deep neural network-based method to generate context-images for peripheral projection. A user study was performed to investigate the manner in which the proposed system augments traditional visual experiences. In addition, we present applications and future prospects of the developed system. We have described two methods using DNN to generate peripheral context-images for videos and have presented our results. With our methods, a patch size of 256 × 256 pixels allows the most natural reconstruction of images (i.e., the discriminator should see the images globally in the contextimage generating task). Our methods can generate textured images as fine as the Multiscale method [1] keeping processing speed faster than 30 fps, which is sufficient for real-time generation of videos at 30 fps. Our user study showed the peripherally projected images generated by our methods are sufficient for a positive enhancement of visual experiences. However, sequenced images generated by our methods have a flicker and noise problems caused by differences between frames. Problems on the versatility of model and consideration of temporal changes also remain. To solve these problems, further optimization of the network for our task will be useful. We expect not only our research but also that DNN will make a further contribution to HCI.

"
Substituting Motion Effects with Vibrotactile Effects for 4D Experiences,https://dl.acm.org/authorize?N657139,"
In this paper, we present two methods to substitute motion effects using vibrotactile effects in order to improve the 4D experiences of viewers. This work was motivated by the needs of more affordable 4D systems for individual users. Our sensory substitution algorithms convert motion commands to vibrotactile commands to a grid display that uses multiple actuators. While one method is based on the fundamental principle of vestibular feedback, the other method makes use of intuitive visually-based mapping from motion to vibrotactile stimulation. We carried out a user study and could confirm the effectiveness of our substitution methods in improving 4D experiences. To our knowledge, this is the first study that investigated the feasibility of replacing motion effects using much simpler and less expensive vibrotactile effects. In this paper, we have explored effective ways to convert motion effects to vibrotactile effects on a grid display in order to improve the viewers’ 4D experiences. We designed two substitution algorithms, one based on the principle of vestibular feedback and the other emphasizing intuitive match between visual and vibrotactile stimuli. A user study demonstrated the high potential of our methods, supporting the general applicability of our sensory substitution approach. As a next step, we consider two main tasks. One is to further improve our substitution algorithms, e.g., by combining the position-based rules with the derivative-based rules. The other is to compare the subjective performance of vibrotactile and motion effects as a way to gauge how much sensory substitution actually takes place with vibrotactile effects.

"
Communicating Awareness and Intent in Autonomous Vehicle-Pedestrian Interaction,https://dl.acm.org/authorize?N657130,"
Drivers use nonverbal cues such as vehicle speed, eye gaze, and hand gestures to communicate awareness and intent to pedestrians. Conversely, in autonomous vehicles, drivers can be distracted or absent, leaving pedestrians to infer awareness and intent from the vehicle alone. In this paper, we investigate the usefulness of interfaces (beyond vehicle movement) that explicitly communicate awareness and intent of autonomous vehicles to pedestrians, focusing on crosswalk scenarios. We conducted a preliminary study to gain insight on designing interfaces that communicate autonomous vehicle awareness and intent to pedestrians. Based on study outcomes, we developed four prototype interfaces and deployed them in studies involving a Segway and a car. We found interfaces communicating vehicle awareness and intent: (1) can help pedestrians attempting to cross; (2) are not limited to the vehicle and can exist in the environment; and (3) should use a combination of modalities such as visual, auditory, and physical. We proposed the use of interfaces for explicitly communicating vehicle awareness and intent to pedestrians. As part of our exploration, we conducted a design study to gain insight on designing interfaces for autonomous vehicle-pedestrian interactions. We implemented the design study findings by creating four prototypes and deploying them on a Segway and a car, and conducting two user studies to assess their usefulness in helping pedestrians make crossing decisions. We found that interfaces which communicate awareness and intent can be helpful to pedestrians attempting to cross a street. In summary, our work makes three contributions: (i) showing that autonomous vehicle interfaces that explicitly communicate vehicle awareness and intent can be helpful to pedestrians in making crossing decisions, (ii) identifying a preliminary design space that can aid future designers build interfaces that explicitly communicate awareness and intent, and (iii) presenting (in the Discussion Section) considerations for designing future interfaces that can help pedestrians interact with autonomous vehicles. We plan to expand our work and prototypes to testing with an actual autonomous vehicle, and to deployment on pedestrian’s mobile and wearable devices. We are also interested in testing our work with multiple vehicles and pedestrians where we predict that scalability will become a critical challenge. By revisiting our design space in different scaling conditions such as one-to-one, one-to-many, and many-tomany instances of vehicles and pedestrians, we can refine our findings to reflect scalability. Our work has focused on the pedestrian-centered approach to handling the autonomous vehicle-pedestrian interaction, but there are also challenges in the driver-centered approach, such as maintaining driver situational awareness, which need to be addressed. Further, we can learn from research being conducted in vehicle-to-vehicle communication. For example, Sadigh et al. [26] propose using an autonomous vehicle’s actions to communicate awareness and intent to drivers of manually-driven vehicles. The near future will force pedestrians to expand their view of vehicles, a future where they will not expect the driver (if there is one) to provide them with familiar cues. Other variables impacting future design of vehicle-pedestrian interfaces are expected to emerge from new policies governing the introduction of autonomous vehicles (such as the US Department of Transportation’s recent framework 6 ). While still preliminary, our work outlines a future path forward where the interaction flow to the pedestrian is shifting from the driver to the autonomous vehicle, and possibly drifting from static infrastructure (such as crosswalks and traffic lights) to vehicle interfaces and to the pedestrian’s mobile appliances. Our findings suggest that expecting pedestrians to rely on cues provided by movement alone will be an oversight, and that future interfaces for autonomous vehicle-pedestrian communication are an acute challenge for the interaction design community.

"
Running Out of Time: The Impact and Value of Flexibility in On-Demand Crowdwork,https://dl.acm.org/authorize?N657131,"
With a seemingly endless stream of tasks, on-demand labor markets appear to offer workers flexibility in when and how much they work. This research argues that platforms afford workers far less flexibility than widely believed. A large part of the ""inflexibility"" comes from tight deadlines imposed on tasks, leaving workers little control over their work schedules. We experimentally examined the impact of offering workers control of their time in on-demand crowdwork. We found that granting higher ""in-task flexibility"" dramatically affected the temporal dynamics of worker behavior and produced a larger amount of work with similar quality. In a second experiment, we measured the compensating differential and found that workers would give up significant compensation to control their time, indicating workers attach substantial value to in-task flexibility. Our results suggest that designing tasks which give workers direct control of their time within tasks benefits both buyers and sellers of on-demand crowdwork. Flexibility has long been assumed to be a key feature and benefit of on-demand crowdwork. This paper questions this common perception by examining whether there is sufficient flexibility in on-demand crowdwork and what it might mean to design in-task flexibility into on-demand work. Our study suggests that higher levels of flexibility can be afforded in on-demand crowdwork by providing workers more control of their work time within individual tasks. Through two randomized behavioral experiments, we find that granting more flexibility in tasks significantly influences the ways workers work and leads to higher work quantity and similar work quality, and workers also attach substantial value to the flexibility provided to them. Together, these results highlight the importance and benefits of allowing workers to control their own time in individual tasks in on-demand crowdwork.
"
Tensions of Data-Driven Reflection: A Case Study of Real-Time Emotional Biosensing,https://dl.acm.org/authorize?N657132,"
Biosensing displays, increasingly enrolled in emotional reflection, promise authoritative insight by presenting users' emotions as discrete categories. Rather than machines interpreting emotions, we sought to explore an alternative with emotional biosensing displays in which users formed their own interpretations and felt comfortable critiquing the display. So, we designed, implemented, and deployed, as a technology probe, an emotional biosensory display: Ripple is a shirt whose pattern changes color responding to the wearer's skin conductance, which is associated with excitement. 17 participants wore Ripple over 2 days of daily life. While some participants appreciated the 'physical connection' Ripple provided between body and emotion, for others Ripple fostered insecurities about 'how much' feeling they had. Despite our design intentions, we found participants rarely questioned the display's relation to their feelings. Using biopolitics to speculate on Ripple's surprising authority, we highlight ethical stakes of biosensory representations for sense of self and ways of feeling. We contribute the design and study of Ripple, a technology probe that explores an alternative engagement with emotional biosensory data displays. Instead of presenting user emotions as discrete categorical states, Ripple’s display is a highly ambiguous color-changing fabric pattern. Instead of seeking to present authoritative insights with our data display, we intended the design to invite open-ended emotional interpretation and critical questioning. Yet, our analysis of vignettes of participants’ lived experiences with Ripple point to broader tensions of affective biosensing technologies. Despite our designerly efforts to invite questioning or critique, the data display still held authority even as it fostered insecurities that may not have been warranted. Despite the display’s multifaceted ambiguity, it still reconstituted feeling in its own terms of measurement. Going forward, we suggest designers of emotional biosensing technologies continue to explore these tensions and related issues of measurement, representation, and interpretation. We have presented three theoretical lenses that helped us think about these tensions. Boehner et al.’s affect-as-interaction emphasizes the socioculturally constructed and performative nature of emotion and influenced our design approach [5]. Verbeek’s theory of technological mediation helps analyze interactions between people and technologies to consider how biosensing designs shape what counts as real or valuable [70]. Biopolitics helps consider how biosensing designs are situated within broader societal discourses of health and authority [51,52]. Our findings and analysis made us more attuned to how emotional biosensing designs can suggest new ways of feeling, for better or worse. Far from being a neutral observer, biosensing actively shapes our conception of affect, feeling, and emotion, and becomes embroiled in questions of how or what we should feel or be.

"
Communicating Algorithmic Process in Online Behavioral Advertising,https://dl.acm.org/authorize?N657133,"
Advertisers develop algorithms to select the most relevant advertisements for users. However, the opacity of these algorithms, along with their potential for violating user privacy, has decreased user trust and preference in behavioral advertising. To mitigate this, advertisers have started to communicate algorithmic processes in behavioral advertising. However, how revealing parts of the algorithmic process affects users' perceptions towards ads and platforms is still an open question. To investigate this, we exposed 32 users to why an ad is shown to them, what advertising algorithms infer about them, and how advertisers use this information. Users preferred interpretable, non-creepy explanations about why an ad is presented, along with a recognizable link to their identity. We further found that exposing users to their algorithmically-derived attributes led to algorithm disillusionment---users found that advertising algorithms they thought were perfect were far from it. We propose design implications to effectively communicate information about advertising algorithms. In this study, we contribute to understanding how communicating aspects of the algorithmic ad curation process affects users’ perception of their ad experience. Our analysis highlighted misperceptions about algorithmic omniscience which subsided when users were exposed to the inner workings of the system. This illustrates that as more ads are tailored to users via algorithmic processes, advertisers should provide users with interpretable explanations about these processes. Advertisers also need to increase the visibility of such disclosure mechanisms as the current practices fail to do so. Communicating algorithmic processes not only benefits users by providing them with a more realistic understanding of how their information is processed, but could also help advertisers to regain or increase user trust in and satisfaction with their ad experience.

"
Quadcopter-Projected In-Situ Navigation Cues for Improved Location Awareness,https://dl.acm.org/authorize?N657134,"
Every day people rely on navigation systems when exploring unknown urban areas. Many navigation systems use multimodal feedback like visual, auditory or tactile cues. Although other systems exist, users mostly rely on a visual navigation using their smartphone. However, a problem with visual navigation systems is that the users have to shift their attention to the navigation system and then map the instructions to the real world. We suggest using in-situ navigation instructions that are presented directly in the environment by augmenting the reality using a projector-quadcopter. Through a user study with 16 participants, we show that using in-situ instructions for navigation leads to a significantly higher ability to observe real-world points of interest. Further, the participants enjoyed following the projected navigation cues. In this paper, we investigated using a quadcopter-mounted projector for presenting in-situ navigation instructions as an alternative to smartphone navigation instructions. In a user study, we compared the in-situ navigation instructions to a state-ofthe-art smartphone navigation. The results show that although participants required considerably more time to complete a route using in-situ navigation instructions using a levitating projector, the participants could memorize points of interest significantly more accurately using in-situ instructions. We conclude that using in-situ navigation instructions while walking leads to a higher memorability of the surroundings. In future work, we want to investigate the social implications of using a personal quadcopter for receiving in-situ navigation instructions. Further, we want to explore more use-cases, e.g. augmenting sports or outdoor sightseeing and introducing a quadcopter-mounted projector as a smart companion for everyday scenarios.

"
P2PSTORY: Dataset of Children as Storytellers and Listeners in Peer-to-Peer Interactions,https://dl.acm.org/authorize?N657145,"
Understanding social-emotional behaviors in storytelling interactions plays a critical role in the development of interactive educational technologies for children. A challenge when designing for such interactions using technology like social robots, virtual agents, and tablets is understanding the social-emotional behaviors pertinent to storytelling-especially when emulating a natural peer-to-peer relation between the child and the technology. We present P2PSTORY, a dataset of young children (5-6 years old) engaging in natural peer-to-peer storytelling interactions with fellow classmates. The dataset consists of rich social behaviors of children without adult supervision, with each participant demonstrating being a storyteller and a listener. The dataset contains 58 video recorded sessions along with a diverse set of behavioral annotations as well as developmental and demographic profiles of each child participant. We describe the main characteristics of the dataset in addition to findings that reveal perceptual differences between adults and children when evaluating the attentiveness of listeners. P2PSTORY is a dataset of peer-to-peer storytelling interactions between children of 5-6 years old demonstrating rich social-emotional behaviors as both storytellers and listeners. The objective of this work was to capture children engaged in natural social interactions, not hindered from adult supervision or structured from constrained tasks. Children participated in multiple interactions as either a storyteller or listener, allowing for the exploration of individual variations in behaviors across interactions of either roles. Additionally, these interactions took place with familiar peers as opposed to novel adults. In these three ways, the dataset is a unique contribution to the study of human-human interactions for the design and evaluation of interactive story-listening and story-telling technologies. Furthermore, we provide initial evidence highlighting the importance of accounting for the relevance of social-emotional communicative behaviors as a function of the observer, which significantly differs in perception between an adult and a child. One of the fundamental basis of human-computer interaction research stems from Reeves’ and Nass’ work demonstrating how the human mind will respond to technology as social actors—capable of evoking the same social responses as they would with a human partner [32]. Through data-driven methods to better understand human social-emotional interactions, we can more appropriately design the human-like behaviors of interactive technologies for targeted social interactions such as storytelling.

"
Personalizing Persuasive Strategies in Gameful Systems to Gamification User Types,https://dl.acm.org/authorize?N657146,"
Persuasive gameful systems are effective tools for motivating behaviour change. Research has shown that tailoring these systems to individuals can increase their efficacy; however, there is little knowledge on how to personalize them. We conducted a large-scale study of 543 participants to investigate how different gamification user types responded to ten persuasive strategies depicted in storyboards representing persuasive gameful health systems. Our results reveal that people's gamification user types play significant roles in the perceived persuasiveness of different strategies. People scoring high in the 'player' user type tend to be motivated by competition, comparison, cooperation, and reward while 'disruptors' are likely to be demotivated by punishment, goal-setting, simulation, and self-monitoring. 'Socialisers' could be motivated using any of the strategies; they are the most responsive to persuasion overall. Finally, we contribute to CHI research and practice by offering design guidelines for tailoring persuasive gameful systems to each gamification user type. The current study investigated the relations between gamification user types and persuasive strategies for the first time in the literature. This paper makes an initial contribution to understanding how to tailor persuasive gameful systems to increase their efficacy based on how the user’s responsiveness to persuasive strategies is determined by their user type. As a secondary objective, we provide qualitative insights based on users’ comments to explain why distinct strategies may motivate behaviours for people belonging to a particular user type and demotivate others. Through our study, we uncovered the shortcomings of the untailored approach and presented design opportunities for designing persuasive gameful systems that appeal both to a broad audience and for tailoring to a particular user type. Our findings indicate that socialiser, disruptor, and player are the three gamification user types that predict most of the variability in the effectiveness of persuasive strategies and thus, must be taken into account to achieve user-type-driven tailoring. Our findings could guide designers in making informed choices on the strategies to employ and those to avoid when designing persuasive tailored gameful systems. In the future, we plan to apply the guidelines in designing and evaluating the effectiveness of actual tailored persuasive gameful systems and to validate our findings across other health behaviour domains (e.g., discouraging drug use, risky sexual behaviour, and smoking) to investigate possible variability in the persuasiveness of the strategies. We hope to explore the role of culture and personal attitude towards drinking on the persuasiveness of the strategies.
"
Your Eyes Tell: Leveraging Smooth Pursuit for Assessing Cognitive Workload,https://dl.acm.org/authorize?N657147,"
A common objective for context-aware computing systems is to predict how user interfaces impact user performance regarding their cognitive capabilities. Existing approaches such as questionnaires or pupil dilation measurements either only allow for subjective assessments or are susceptible to environmental influences and user physiology. We address these challenges by exploiting the fact that cognitive workload influences smooth pursuit eye movements. We compared three trajectories and two speeds under different levels of cognitive workload within a user study (N=20). We found higher deviations of gaze points during smooth pursuit eye movements for specific trajectory types at higher cognitive workload levels. Using an SVM classifier, we predict cognitive workload through smooth pursuit with an accuracy of 99.5% for distinguishing between low and high workload as well as an accuracy of 88.1% for estimating workload between three levels of difficulty. We discuss implications and present use cases of how cognition-aware systems benefit from inferring cognitive workload in real-time by smooth pursuit eye movements. This work investigated the influence of cognitive workload on smooth pursuit eye movements using three different trajectories with two different velocities. Using an auditory delayed digit recall N-back task to induce cognitive workload, a higher deviation of gaze points from shown trajectories is measured compared to measurements when not inducing cognitive workload. Based on our results, we create a personindependent classifier for estimating binary workload and a person-dependent classifier for distinguishing different levels of cognitive workload. While binary cognitive workload classification can be elicited in the public using smooth pursuit interfaces, private spaces benefit from person-dependent classifier calibration to determine different levels of cognitive workload. Having such a measurement modality without the need of body-worn devices goes a step towards real-time mental state estimation in ubiquitous computing environments. User interfaces can then provide intervention mechanisms to relax or help users based on their current context. Our classifier depends on eye gaze only and fits into a number of application scenarios. It can be deployed in real-world scenarios to estimate the presence of cognitive workload in real-time. Thereby, the assessment can be done contactless without the need for additional bodyworn sensors. In future work, we plan to focus on specific use cases which leverage smooth pursuit as interaction modality to provide an assessment of cognitive workload in real-time. This input will be used to adapt user interfaces of applications accordingly. This includes implementations on public and head-mounted displays using smooth pursuit as input. Furthermore, we want to evaluate the efficiency of assessing cognitive workload unconsciously in user interfaces which naturally display moving elements. This comprises monitoring tasks, which can be found in air traffic and train control system. Consequently, such systems can be benchmarked and optimized regarding their usage complexity. To complement this, further research aiming to correlate objective and subjective workload measures, such as eye movement deviations and NASA-TLX questionnaires, will be conducted. Finally, we will investigate how multiple displayed moving stimuli will affect the classification performance and subjective perception of cognitive workload. To encourage research in this area, we published the data set for further analysis by the research community on our institute’s homepage.

"
FingerPing: Recognizing Fine-grained Hand Poses using Active Acoustic On-body Sensing,https://dl.acm.org/authorize?N657148,"
FingerPing is a novel sensing technique that can recognize various fine-grained hand poses by analyzing acoustic resonance features. A surface-transducer mounted on a thumb ring injects acoustic chirps (20Hz to 6,000Hz) to the body. Four receivers distributed on the wrist and thumb collect the chirps. Different hand poses of the hand create distinct paths for the acoustic chirps to travel, creating unique frequency responses at the four receivers. We demonstrate how FingerPing can differentiate up to 22 hand poses, including the thumb touching each of the 12 phalanges on the hand as well as 10 American sign language poses. A user study with 16 participants showed that our system can recognize these two sets of poses with an accuracy of 93.77% and 95.64%, respectively. We discuss the opportunities and remaining challenges for the widespread use of this input technique. In this paper, we presents FingerPing, a active acoustic sensing technology that can recognize fine-grained hand poses by analyzing how the frequency response changes after traveling through different paths in hands. A user study with 16 participants shows that FingerPing can recognize the tap locations at 12 phalanges and 10 poses from ASL with an accuracy of 93.77% and 95.64% respectively.

"
Frames and Slants in Titles of Visualizations on Controversial Topics,https://dl.acm.org/authorize?N657149,"
Slanted framing in news article titles induce bias and influence recall. While recent studies found that viewers focus extensively on titles when reading visualizations, the impact of titles in visualization remains underexplored. We study frames in visualization titles, and how the slanted framing of titles and the viewer's pre-existing attitude impact recall, perception of bias, and change of attitude. When asked to compose visualization titles, people used five existing news frames, an open-ended frame, and a statistics frame. We found that the slant of the title influenced the perceived main message of a visualization, with viewers deriving opposing messages from the same visualization. The results did not show any significant effect on attitude change. We highlight the danger of subtle statistics frames and viewers' unwarranted conviction of the neutrality of visualizations. Finally, we present a design implication for the generation of visualization titles and one for the viewing of titles. In this study, we identified general frames for visualization title and studied the influence of titles on the recall, perception of bias, and attitude change. Visualization title frames matched pre-established news frames with an addition of openended frames and statistic frames. Statistic frames referred to variables, trends, and values, and occurred most frequently in slanted titles. The results showed that the slanted frames in titles influenced the perceived main message of a visualization without impacting the perceived bias. Informed by bias assimilation, we further examined how people’s pre-existing attitudes effect the composition and interpretation of visualization titles. We observed that many participants wrote slanted titles that matched their attitude even when explicitly instructed to write neutral titles. The results did not show a significant effect of attitude-consistency or the slant of the title on the perception of bias nor on attitude change, mainly because people viewed the information as impartial and indicated little attitude change. Our findings suggest the influence of titles on visualization interpretation and reveal the lack of awareness on the potential bias introduced by the title. We conclude by suggesting how the frames can be used in automated generation of visualizations and how people could interact with visualization titles for improved comprehension of the data. Our results are directly applicable to social media sites such as Twitter, Imgur, and Reddit where visualizations are currently being shared as a standalone material. It is uncertain whether our results can be transferred to platforms where visualizations are presented along with longer accompanying text, such as online news sites. We propose continuing the study in the context of news articles for future work.
"
Typing on an Invisible Keyboard,https://dl.acm.org/authorize?N657140,"
A virtual keyboard takes a large portion of precious screen real estate. We have investigated whether an invisible keyboard is a feasible design option, how to support it, and how well it performs. Our study showed users could correctly recall relative key positions even when keys were invisible, although with greater absolute errors and overlaps between neighboring keys. Our research also showed adapting the spatial model in decoding improved the invisible keyboard performance. This method increased the input speed by 11.5% over simply hiding the keyboard and using the default spatial model. Our 3-day multi-session user study showed typing on an invisible keyboard could reach a practical level of performance after only a few sessions of practice: the input speed increased from 31.3 WPM to 37.9 WPM after 20 - 25 minutes practice on each day in 3 days, approaching that of a regular visible keyboard (41.6 WPM). Overall, our investigation shows an invisible keyboard with adapted spatial model is a practical and promising interface option for the mobile text entry systems. Invisible keyboard is a desirable design option. Our research on invisible keyboard answered a set of questions through 3 experiments: Are users able type on an invisible keyboard? If so, how will users type differently? What technology can be applied to support it? What is its performance? First, Experiment 1 showed it was possible to type on an invisible keyboard. Users had strong memory recall on key positions: they could correctly recall key positions relative to the Qwerty layout even when keys were completely invisible and with 1 or 2 finger typing. On the other hand, their touch point distributions exhibited different patterns compared with those on a visible keyboard. Second, we examined whether adapting the spatial model of the decoder would improve the performance of the invisible keyboard. We derived the adapted spatial model for the invisible keyboard from our study data, integrated it into a stateof-the-art decoder with a language model, and systematically evaluated it. Experiment 2 showed this method was very effective. It increased the typing speed of the invisible keyboard by 11.5% over the baseline invisible keyboard which used the default spatial model (i.e., unadapted) and simply hid the keys. We disclose the parameters of the derived spatial model (in Appendix), which are available for use by other researchers. Third, Experiment 3 showed typing on the invisible keyboard was easy-to-learn, practical and promising. It was remarkable that the input speed of the invisible keyboard was approaching the visible keyboard after 60 - 75 minutes practice in 3 days (20 - 25 minutes on each day). No significant difference was observed between the speed of the invisible keyboard on the last day (37.9 WPM) and the speed of a regular visible keyboard (41.6 WPM). Typing on an invisible keyboard was also easy to learn. The input speed of the invisible keyboard increased from 31.3 WPM on day 1 to 37.9 WPM on day 3, showing a rapid learning process. Overall, our investigation shows that an invisible keyboard with adapted spatial model is a practical and promising interface option for the mobile text entry systems.

"
Fairness and Accountability Design Needs for Algorithmic Support in High-Stakes Public Sector Decision-Making,https://dl.acm.org/authorize?N657141,"
Calls for heightened consideration of fairness and accountability in algorithmically-informed public decisions-like taxation, justice, and child protection-are now commonplace. How might designers support such human values? We interviewed 27 public sector machine learning practitioners across 5 OECD countries regarding challenges understanding and imbuing public values into their work. The results suggest a disconnect between organisational and institutional realities, constraints and needs, and those addressed by current research into usable, transparent and 'discrimination-aware' machine learning-absences likely to undermine practical initiatives unless addressed. We see design opportunities in this disconnect, such as in supporting the tracking of concept drift in secondary data sources, and in building usable transparency tools to identify risks and incorporate domain knowledge, aimed both at managers and at the 'street-level bureaucrats' on the frontlines of public service. We conclude by outlining ethical challenges and future directions for collaboration in these high-stakes applications. Researchers should be wary of assuming, as seems often the case in current discourse, that those involved in the procurement and deployment of these systems are necessarily naïve about challenges such as fairness and accountability in the public sector’s use of algorithmic decision support. This assumption sits particularly uncomfortably with the value attributed to participatory design and action research in HCI and information systems [33, 6]. While those involved in acquiring these technologies for the public sector might not be prime candidates for developing new statistical technologies for understanding bias and outputs in complex models, this does not mean that they do not care or do not try to tackle ethical issues that they perceive. Indeed, as well as the individual perspectives in this paper, some public agencies are already developing their own in-house ethical codes for data science activities [30]. Yet issues like fairness have been shown to come with technically difficult to reconcile, or even irreconcilable trade-offs—something well-demonstrated by Alexandra Chouldechova’s impossibility theorem illustrating that independently plausible formal definitions of fairness can be statistically incompatible with one another [13], or concerns raised that explanation facilities might work better for some outputs than for others [20]. Reconciling these harder boundaries and issues within messy organisational contexts will present a major challenge to research uptake in this field in the coming years. Where to go from here? We believe that the challenges we outlined above—dealing with changing data, better understanding discretion and the augmentation of model outputs, better transmission of social practices and improved communication of nuanced aspects of performance—sit amongst a range of promising areas for future interdisciplinary collaboration. The implicit and explicit assumptions of proposed solutions to both these challenges and to the broader issues must be stresstested in real situations. This presents important questions of methodology. Domain-specific, organisational and contextual factors are crucial to closely consider in the context of interventions intended to improve the fairness and accountability of algorithmic decision-support. The institutional constraints, high stakes and crossed lines of accountability in the public sector arguably presents even more reason to do so. Only so much can be learned from studying systems in vitro, even with access to impressive quantities of relevant, quality data with which to experiment. Those interested in transformative impact in the area of fair and accountable machine learning must move towards studying these processes in vivo, in the messy, socio-technical contexts in which they inevitably exist. Interventions will have to cope with institutional factors, political winds, technical lock-in and ancient, withering infrastructure head on, as they would have to in the real world. Researchers will have to facilitate the navigation of contested values, and will not always have the freedom of seeking the types of accountability or fairness that they feel most comfortable with. Such challenges should be embraced. To enable this, trust will need to be built between public bodies and researchers; trust that is currently being endangered by ‘gotcha!’–style research that seeks to identify problematic aspects of algorithmic systems from afar without working collaboratively to understand the processes by which they came about and might be practically remedied. Action research is a core methodology that would support these aims [6], but the combination of high stakes and a wariness that researchers might be spending more effort looking for algorithmic harms than offering help to fix it might make public agencies reluctant to open up to research interventions. Rarely have the issues HCI concerns itself with been as directly involved in steering choices related to the use of governmental power as much as they are today. As we involve more advanced decision-support, and even decision-making, systems in the workings of the state, this field might even be the ‘difference that makes a difference’ to the rights and freedoms of vulnerable societal groups. We believe that making this difference is possible, but only in close collaboration with different disciplines, practitioners and affected stakeholders. Future research must engage with not only with the new questions and avenues of exploration such research brings, but also the practical constraints that come with studying politically charged settings and developing workable social and technical improvements within them.

"
PEP (3D Printed Electronic Papercrafts): An Integrated Approach for 3D Sculpting Paper-Based Electronic Devices,https://dl.acm.org/authorize?N657142,"
We present PEP (Printed Electronic Papercrafts), a set of design and fabrication techniques to integrate electronic based interactivities into printed papercrafts via 3D sculpting. We explore the design space of PEP, integrating four functions into 3D paper products: actuation, sensing, display, and communication, leveraging the expressive and technical opportunities enabled by paper-like functional layers with a stack of paper. We outline a seven-step workflow, introduce a design tool we developed as an add-on to an existing CAD environment, and demonstrate example applications that combine the electronic enabled functionality, the capability of 3D sculpting, and the unique creative affordances by the materiality of paper. We introduced printed electronic papercrafts by 3D sculpting. Leveraging electronically enhanced papercrafts with 3D sculpting can enable new creative possibilities and we presented a set of fabrication techniques with a design editor to afford such prototyping using today’s technology. By inserting functional layers into a stack of plain paper, we can develop interactive 3D paper prototypes by cutting and gluing sheets of paper. Our software supports designing the integration process with a set of functionality (actuation, sensing, display, and communication). We believe the unique affordances of paper prototypes can be further extended, and in this paper we explored and demonstrated what’s possible today by combining paper with electronic components and leveraging such combination with 3D sculpting.

"
Between Grassroots and the Hierarchy: Lessons Learned from the Design of a Public Services Directory,https://dl.acm.org/authorize?N657143,"
There is a growing interest in HCI research studying technology for citizen engagement in civic issues. We are now seeing issues around technologies for empowerment and participation, long discussed in HCI literature, appropriated and formalised in government legislation. In the UK, recent reforms stipulate that community-based service information should be published in continuously updated, collaboratively designed and maintained, online platforms. We report on a qualitative study where we worked with stakeholders involved in the collaborative design, development and implementation of such a platform. Our findings highlight tensions between the grassroots desire to innovate and local governments' rigid compliance with statutory obligation. We pose a series of challenges and opportunities for HCI researchers engaged in the design of civic technologies to consider going forward, addressing issues of engagement in policy, measures of participation and tools for enabling participatory processes in public institutions. In this paper, we have reported on a 12-month study where we participated in the design, development and implementation of a Local Offer digital platform. We saw how new government legislation became a locus for conflict around individual interpretations and aspirations, concerns and suspicions. We have highlighted tensions that arise when new legislation is enacted and translated through the values of grassroots organisations and citizen groups working with inflexible bureaucratic hierarchies and discussed opportunities for the role of HCI in this context. In doing so, we assert that such opportunities cannot be pursued purely as a means for governments to save money by providing tools for ‘streamlining’ service provision. Rather efforts should support citizens and governments alike to share and make sense of their diverse perspectives. Taking a more critical stance, however, where local government cannot fulfil its fundamental responsibility to citizens, we should use our position as ‘middle-out’ mediators and observers to narrate the struggles of potentially marginalised or disenfranchised groups in the production of government services. In doing so, we might call into question the logic and failures of neo-liberal government, and demonstrate and evidence successful workarounds. This is our responsibility as civic actors—to both create and record the ways in which digital technologies are shaping local government with and for their citizens. The work presented here is just one example of that record. Finally, we present a short summary of the main implications that we derive from our work, that HCI researchers should: (i) support citizen evaluation of local government consultation and evidence their role in dialogue; (ii) identify opportunities to influence legislation and policy before it is handed down; (iii) design tools to document and support the perseverance of grassroots workers and activists and (iv) reflect upon and embrace their own middle-out position.
"
Social Influences on Executive Functioning in Autism: Design of a Mobile Gaming Platform,https://dl.acm.org/authorize?N657144,"
Most studies of executive function (EF) in Autism Spectrum Disorder (ASD) focus on cognitive information processing, emphasizing less the social interaction deficits core to ASD. We designed a mobile game that uses social and nonsocial stimuli to assess children's EF skills. The game comprised three components involving different EF skills: cognitive flexibility (shifting/inference), inhibitory control, and short-term memory. By recruiting 65 children with and without ASD to play the mobile game, we investigated the potential of such platforms for capturing important phenotypic characteristics of individuals with autism. Results highlighted between-diagnostic-group differences in playing patterns with children with ASD showing broad patterns of EF deficits, but with relative strengths in nonsocial short-term memory, and preserved response to emotional inhibition cues. We showed the system could predict IQ, an important target for clinical treatment, towards the goal of developing platforms to act as long-term, efficient, and effective behavioral biomarkers for ASD. We developed a mobile video game designed to explore and quantify executive functioning skills in children with ASD, separately considering social and nonsocial performance so as to disentangle broader patterns of cognitive deficit from the social deficits specific to the disorder. We found that there were specific diagnosis by performance class interactions, such as stronger non-social short-term memory in ASD as compared to their social short-term memory, but also preserved areas of ability in children with ASD, such as inhibitory response to angry faces. These results may be informative at a theoretical level regarding areas of strengths and weakness at a group level for children with ASD. At the same time, with an interest in creating a prototype for longer-term monitoring of clinically-relevant change, we designed the system to be engaging for participants in a narrow time window (e.g. one time a day), so as to balance the increased enthusiasm children with ASD may have for digital platforms against the potential for problematic game play. We showed, using both standard psychological approaches as well as machine learning methods, that patterns of performance on our delivered tasks were associated with both developmental level and IQ, moving us towards the goal of developing video game behavioral biomarkers for clinically relevant targets for ASD. Physical patterns of play, derived via accelerometer and gyroscope readings provided an extra layer of interpretation which may be exploited in the future to further improve phenotypic prediction accuracy. As a proof-of-concept, this work highlights social and nonsocial EF performance asymmetries in ASD, suggesting that digital systems modeling clinicallyrelevant features may need to consider pathology interactions. While a great deal of future work needs to be done to better assess the social and nonsocial asymmetry, this study represents a step towards targeting mobile video game development to specific characteristics of mental health conditions, with the end goal of developing more usable daily monitoring systems for children with ASD.

"
A Large Inclusive Study of Human Listening Rates,https://dl.acm.org/authorize?N657255,"
As conversational agents and digital assistants become increasingly pervasive, understanding their synthetic speech becomes increasingly important. Simultaneously, speech synthesis is becoming more sophisticated and manipulable, providing the opportunity to optimize speech rate to save users time. However, little is known about people's abilities to understand fast speech. In this work, we provide the first large-scale study on human listening rates. Run on LabintheWild, it used volunteer participants, was screen reader accessible, and measured listening rate by accuracy at answering questions spoken by a screen reader at various rates. Our results show that blind and low-vision people, who often rely on audio cues and access text aurally, generally have higher listening rates than sighted people. The findings also suggest a need to expand the range of rates available on personal devices. These results demonstrate the potential for users to learn to listen to faster rates, expanding the possibilities for human-conversational agent interaction. In this work, we presented the frst large-scale study of human listening rates, with the aim of informing the optimization of speech rate for conversational agents. By conducting a volunteer-based online study, we were able to reach a larger participant pool than previous studies. By making it accessible, we also reached a larger number of people with visual impairments, many of whom had experience with fast, synthetic speech. The study results show that people with visual impairments are typically the fastest listeners, in particular those exposed to screen readers at a young age. These results suggest that in optimizing conversational agent speech rate, an expanded set of speech rates should be considered, as well as tailoring to the individual user and content. More importantly, this work demonstrates that people with disabilities have incredible abilities and personal experiences which can inspire design, as previous research shows. A main takeaway of this project is to not view people with visual impairments primarily as consumers of assistive technologies; rather, recognize that they can inspire new avenues for humanconversational agent interactions. Recognizing important contributions of blind people beyond their necessary perspective for accessibility improvements is an important step toward further integrating blind people into research and design.
"
Storyboard-Based Empirical Modeling of Touch Interface Performance,https://dl.acm.org/authorize?N657256,"
Touch interactions are now ubiquitous, but few tools are available to help designers quickly prototype touch interfaces and predict their performance. For rapid prototyping, most applications only support visual design. For predictive modelling, tools such as CogTool generate performance predictions but do not represent touch actions natively and do not allow exploration of different usage contexts. To combine the benefits of rapid visual design tools with underlying predictive models, we developed the Storyboard Empirical Modelling tool (StEM) for exploring and predicting user performance with touch interfaces. StEM provides performance models for mainstream touch actions, based on a large corpus of realistic data. We evaluated StEM in an experiment and compared its predictions to empirical times for several scenarios. The study showed that our predictions are accurate (within 7% of empirical values on average), and that StEM correctly predicted differences between alternative designs. Our tool provides new capabilities for exploring and predicting touch performance, even in the early stages of design. Storyboard Empirical Modelling (StEM) is a drag-and-drop tool that allows designers to quickly prototype touch interactions and explore their performance implications. StEM relies on the Touch-Action database, a crowd-sourced data corpus that provides empirical characterisations of tap, point, drag, swipe, scale and rotate touch interactions across a wide range of device types, hand grips, and screen orientations. Although we developed the TADB corpus primarily as the foundation for predictions with StEM, the corpus is available on the web for other researchers and practitioners. We carried out several evaluations of StEM, and showed that its predictions are accurate within an average of 7% difference from empirical values, and never worse than 13%, across a variety of scenarios – and substantially better than existing tools. StEM provides new capabilities for designers and researchers who need to understand user performance with touch interfaces at any stage in the design process.
"
Adding Force Feedback to Mixed Reality Experiences and Games using Electrical Muscle Stimulation,https://dl.acm.org/authorize?N657257,"
We present a mobile system that enhances mixed reality experiences and games with force feedback by means of electrical muscle stimulation (EMS). The benefit of our approach is that it adds physical forces while keeping the users' hands free to interact unencumbered-not only with virtual objects, but also with physical objects, such as props and appliances. We demonstrate how this supports three classes of applications along the mixed-reality continuum: (1) entirely virtual objects, such as furniture with EMS friction when pushed or an EMS-based catapult game. (2) Virtual objects augmented via passive props with EMS-constraints, such as a light control panel made tangible by means of a physical cup or a balance-the-marble game with an actuated tray. (3) Augmented appliances with virtual behaviors, such as a physical thermostat dial with EMS-detents or an escape-room that repurposes lamps as levers with detents. We present a user-study in which participants rated the EMS-feedback as significantly more realistic than a no-EMS baseline. We demonstrated a fully mobile system that empowers Mixed Reality games and experiences with mid-air force feedback by means of electrical muscle stimulation. Our system, built around the HoloLens headset, and actuates the users’ wrists, biceps, triceps and shoulder muscles. The main benefit of our approach is that it leaves users’ hands free, thus allowing users to interact unencumbered—not only with virtual objects, but also with physical objects in their surroundings, such as props and appliances. EMS opens novel interaction opportunities in MR Besides the direct implications for increased realism in MR gaming, EMS might uncover new terrains for augmented passive objects and appliances. For instance, an appliance that is augmented with EMS might have more potential if we think of using it daily. Unlike RetroFab [52] that complements the appliance with updated hardware UI, an EMSaugmented appliance allows updating the UI of a device by merely updating the software (i.e., the EMS side). Also, our tangible dial that automatically recalled the last position, i.e., the cup in the “walkthrough”, points to another strength of exploring EMS in MR. EMS might assist in aligning virtual and physical realities to prevent inconsistent states often introduced by physical props (as debated in [33,28]). While previous methods solved this by mechanically coupling or actuating the props (e.g., mechanically constrained tangible dials [28]), EMS allows for everyday handheld objects to move without instrumentation.

"
Learning from the Veg Box: Designing Unpredictability in Agency Delegation,https://dl.acm.org/authorize?N657258,"
The Internet of Things (IoT) promises to enable applications that foster a more efficient, sustainable, and healthy way of life. If end-users are to take full advantage of these developments we foresee the need for future IoT systems and services to include an element of autonomy and support the delegation of agency to software processes and connected devices. To inform the design of such future technology, we report on a breaching experiment designed to investigate how people integrate an unpredictable service, through the veg box scheme, in everyday life. Findings from our semi-structured interviews and a two-week diary study with 11 households reveal that agency delegation must be warranted, that it must be possible to incorporate delegated decisions into everyday activities, and that delegation is subject to constraint. We further discuss design implications on the need to support people's diverse values, and their coordinative and creative practices. In this paper, we report the findings of a qualitative study, through semi-structured interviews and a two-week diary study with 11 households in the UK, which seeks to understand how people manage a veg box scheme as an instance of an inherently unpredictable service. In particular, we focus on how people manage agency delegation and integrate the veg box into their everyday life. Our findings suggest that agency delegation must be warranted, that it must be possible to incorporate delegated decisions into everyday activities, and that delegation is subject to constraint. We consider the potential impact of these social organisational issues on the design of a future AIoT supporting food-based practices in the home, and the challenges of making agency delegation accountable to meal planning, persons’ schedules, food-centred values, adaptation and innovation, and the social division of labour in which computational agency will ultimately be embedded.
"
Knotation: Exploring and Documenting Choreographic Processes,https://dl.acm.org/authorize?N657259,"
Contemporary choreographers often interact directly with dancers when exploring their ideas, but lack adequate tools for capturing and documenting their work. Although our first study of choreographers and dancers revealed diverse strategies for recording choreographic fragments, we found that they all worked in terms of constraints, which they represented via spatial diagrams, as movement qualities or with their own personal notation system. This led to the design of Knotation, a mobile pen-based tool that lets choreographers sketch their own representations of choreographic ideas and render them interactive. In study two, Knotation served as a technology probe to support the contrasting practices of three professional choreographers. We revised Knotation based on their input, and ran a third structured observation study with six professional choreographers. Knotation easily supported both dance-then-record and record-then-dance strategies. Participants used and appropriated Knotation's advanced features, including the combination of interactive timelines and floorplan diagrams, to represent and explore complex choreographic structures. Our goal is to design interactive digital tools that support exploration and documentation of choreographic ideas, without enforcing a particular creative process. We began by observing a professional choreographer and several dancers, which highlighted both the diversity in how they represent choreographic fragments, as well as the need for capturing and annotating movement constraints. We designed Knotation, a mobile pen-based tool that offers a lightweight method for sketching choreographic ideas with embedded images and video. Users can sketch their own personal representations of the dance, and add various forms of interaction to further explore their ideas. Using Knotation v1 as a technology probe revealed contrasting strategies for capturing movement, constraints, or both. Knotation v2 explicitly supports interactive timelines and floorplans, as well as incorporates participants’ suggestions. We then demonstrated how Knotation v2 successfully supported opposite choreographic approaches (dance-then-record and record-then-dance), and allowed users a wide range of expression, at varying levels of formality. In future, we plan to explore the collaborative potential of Knotation. Other promising directions include extending support for personal sublanguages; letting users create their own instrument palettes [3]; and assessing results with the Creativity Support Index [10].

"
A Data-Driven Analysis of Workers' Earnings on Amazon Mechanical Turk,https://dl.acm.org/authorize?N657250,"
A growing number of people are working as part of on-line crowd work. Crowd work is often thought to be low wage work. However, we know little about the wage distribution in practice and what causes low/high earnings in this setting. We recorded 2,676 workers performing 3.8 million tasks on Amazon Mechanical Turk. Our task-level analysis revealed that workers earned a median hourly wage of only ~$2/h, and only 4% earned more than $7.25/h. While the average requester pays more than $11/h, lower-paying requesters post much more work. Our wage calculations are influenced by how unpaid work is accounted for, e.g., time spent searching for tasks, working on tasks that are rejected, and working on tasks that are ultimately not submitted. We further explore the characteristics of tasks and working patterns that yield higher hourly wages. Our analysis informs platform design and worker tools to create a more positive future for crowd work. We used the log data of 2,676 workers performing 3.8 million tasks on Amazon Mechanical Turk to understand worker hourly wages. Our task-level analysis revealed a median hourly wage of ~$2/h, validating past self-report estimates. Only 4% of workers earn more than $7.25/h, justifying concerns about non-payment of the minimum wage. We characterize three sources of unpaid work that impact the hourly wage (i.e., task search, task rejection, task return). We further explore the characteristics of tasks and working patterns that yield higher hourly wages.
"
Double-sided Printed Tactile Display with Electro Stimuli and Electrostatic Forces and its Assessment,https://dl.acm.org/authorize?N657251,"
Humans can perceive tactile sensation through multimodal stimuli. To demonstrate realistic pseudo tactile sensation for the users, a tactile display is needed that can provide multiple tactile stimuli. In this paper, we have explicated a novel printed tactile display that can provide both the electrical stimulus and the electrostatic force. The circuit patterns for each stimulus were fabricated by employing the technique of double-sided conductive ink printing. Requirements for the fabrication process were analyzed and the durability of the tactile display was evaluated. Users' perceptions of a single tactile stimulus and multiple tactile stimuli were also investigated. The obtained experimental results indicate that the proposed tactile display is capable of exhibiting realistic tactile sensation and can be incorporated by various applications such as tactile sensation printing of pictorial illustrations and paintings. Furthermore, the proposed hybrid tactile display can contribute to accelerated prototyping and development of new tactile devices. In this paper, we proposed a hybrid tactile display which can provide “electrical stimulus” and “electrostatic force.” We also proposed prototyping technique that fabricating the hybrid tactile display using a double-sided inkjet printing. Our prototyping technique enable easy and inexpensive fabrication of the experimental device and facilitates future work in the haptics field. We evaluated the user experience of the tactile sensations using combinations of electrical stimulus and electrostatic force. According to the results, tactile sensation is influenced by the interaction between electrical stimulus and electrostatic force with various frequency conditions. The proposed hybrid tactile display using an electrical stimulus and an electrostatic force presents a more realistic tactile presentation that has richer information than tactile feedback with a simple stimulus. Finally, we showed a variety of novel applications for our conductive inkjet printing technique. A part of this study was presented in ACM UIST’17 demo [19].

"
RecipeScape: An Interactive Tool for Analyzing Cooking Instructions at Scale,https://dl.acm.org/authorize?N657252,"
For cooking professionals and culinary students, understanding cooking instructions is an essential yet demanding task. Common tasks include categorizing different approaches to cooking a dish and identifying usage patterns of particular ingredients or cooking methods, all of which require extensive browsing and comparison of multiple recipes. However, no existing system provides support for such in-depth and at-scale analysis. We present RecipeScape, an interactive system for browsing and analyzing the hundreds of recipes of a single dish available online. We also introduce a computational pipeline that extracts cooking processes from recipe text and calculates a procedural similarity between them. To evaluate how RecipeScape supports culinary analysis at scale, we conducted a user study with cooking professionals and culinary students with 500 recipes for two different dishes. Results show that RecipeScape clusters recipes into distinct approaches, and captures notable usage patterns of ingredients and cooking actions. This paper presents RecipeScape, an interactive system for analyzing hundreds of recipes for a single dish by visualizing summaries of their structural patterns. Our user study with cooking professionals and culinary students demonstrates that RecipeScape provides data-driven evidence to usual and unusual ingredients and cooking actions, common and exotic recipes, and different approaches to cooking a dish. There are a number of directions for possible future studies. As an immediate next step, we plan to extend this work to video-based recipes and how-to videos. Examining rich context embedded in videos in aggregate could uncover trends and patterns, and analyzing them at scale will be able to provide answers to questions text recipes at scale cannot.

"
Fostering Commonfare. Infrastructuring Autonomous Social Collaboration,https://dl.acm.org/authorize?N657253,"
Recently, HCI scholars have started questioning the relationship between computing and political economy, with both general analyses of such relationships, and specific design cases describing design interventions. This paper contributes to this stream of reflections, and argues that IT designers and HCI scholars can critically engage with the contemporary phase of capitalism by infrastructuring the emergence of new institutional forms of autonomous social collaboration through IT projects. More specifically, we discuss strategies and tactics that are available for IT designers embracing an activist agenda while infrastructuring autonomous social collaborations. We draw on empirical data from an H2020 EU funded project -- Commonfare -- that seeks to foster the emergence of alternative forms of welfare provision rooted in social collaboration. In this context, we discuss how the necessary multiple relations that unfold in a project with such ambitions shape both the language and the technologies of the project itself. The analysis of long-term evolution and short-term dynamics of the project can be developed in terms of strategic boundaries and tactical negotiations that are entailed by the practices of the actors involved in the project to manage relations with each other, coping with tensions and concerns, and contributing to the process of infrastructuring. When referring to our main research question – how can HCI scholars and designers understand the configuration of participation in a large scale infrastructuring project rooted in political economy? – we can return to literature that stresses the relevance of the installed base [30, 32, 45] and the processes of configuration of participation [47]. Indeed, much of the work in this first year of the Commonfare project has served to continually iterate, through tactical negotiations, on our understanding of the installed base as a strategic boundary for action that follows. For any given effort toward a point of infrastructure, the way in which all parties participate in the process of participatory design has involved tailoring of their configuration through tactical negotiations. Reflecting on our main research question, the first point to stress is that designers, in a case like Commonfare, are continuously engaging in tactical actions of appropriation of the installed base, be they technological artefacts or legally binding documents, in order to make space for deviations based on the empirical work done. Moreover, at any iteration, or point of infrastructure, what were previously tactical actions become more fixed, strategic elements, and the installed base on which further tactical actions are built. This points towards a need for designers wishing to engage in ambitious infrastructuring projects to understand that the relationships will evolve over time, and that while there are strategic actions that can be taken in terms of planning research activities, that the activities are then interpreted and enacted by partner organisations in different contexts (both in terms of the organisation’s expertise and background, as well as the particularities of any given country). As a result, the heterogeneous nature of the reports requires a higher level of communication and consideration, an overhead that is expected as the number of partners responsible for the project grows. Therefore, the concepts of strategies and tactics allow for a renewed understanding of the relations between the installed base and the configuration of participation, in which the way participation is configured allow for tactical negotiations on the installed base, being it technological artefacts or the understanding of the social context, and the results of such negotiations rebuild the installed base as renewed strategic boundaries for further design. Returning to the key differences of Bødker et al. [4], the position of us as authors who also act within the consortium have given us a perspective on the relationships between the consortium and itself, the consortium and the EC, and the consortium and participants. This tension is one between the bottom-up ideals of Commonfare as a participatory design project that seeks to tactically respond to the changing circumstances, and the strategic space of the project as defined by the funding rules and expectations of the EC. Moreover, participatory design, in this case, is one of the ways through which the autonomy of social collaboration is fostered, as it entails possibilities for peoples’ experiences and perspectives to influence the technology they will engage with, instead of being heteronomated by technologies built by more powerful actors. The problem of the configuration of participation in specific cases remains, and in our case has been addressed by adjusting the project language, and not only including inputs for the design of the materiality of digital technologies. Reflecting on political economy and HCI, we have explicitly sought to address issues of political economy. In particular, the changes in the welfare state, where we have shown how the actions available to designers looking to support “real utopias” involve the intermixing relations among: institutional actors, the relevant social subjects, and a diverse set of organisations, including NGOs. These different relations constitute the forms of social collaboration through which autonomy from the dictates of heteronomation could take shape [16]. This is visible not only in the consortium composition but also, and more importantly, in the kind of bottom-up, autonomous, welfare practices that we have collected and that we are supporting. Finally, we have shown how it is possible for HCI scholars and IT designers to engage in large scale participatory infrastructuring processes embedding a progressive agenda, and that implies a combination of strategic and tactical actions by the different social aggregates with which researchers build relations. The concepts of strategies and tactics, therefore, provide a useful lens through which to understand the types of actions that establish the relationships among the different actors involved in and around Commonfare, as well as to move the project towards its release goals. Our use of the installed base as the strategic iterative result of these actions, together with an understanding of the tactical changes in the configuration of participation, is a promising direction through which design projects can be described and further developed, also involving the detail of socio-technical choices connected to the strategic and tactical movements of the actors involved.
"
Designing for Student Interactions: The Role of Embodied Interactions in Mediating Collective Inquiry in an Immersive Simulation,https://dl.acm.org/authorize?N657254,"
Advances in mobile and wireless technologies provide new possibilities for supporting K-12 learning activities that can be spatially distributed in the classroom, for example in jointly investigating a scientific phenomenon. Such technologies have an impact on the ways in which students engage with one another, and with the quality of their engagement with the activity itself. This paper uses an embodied approach to understand the patterns of interactions between students (e.g., student-to-student, student-to-teacher) and with computational media within the environment (e.g., student-to-device, student-to-large display), in relation to students' real-time meaning making as they engage in collective inquiry in an immersive simulation environment. The design-based research study consists of two iterations tested in an authentic school setting. We found that increased student-to-student interactions was accompanied by improved observational accuracy and higher quality student explanations constructed. The design implications of the research findings are discussed. This paper explored broader patterns of multimodal communication as an accompanying analysis to traditional multimodal interaction analysis. This approach was applied to an investigation on the role of embodied interaction in mediating collective inquiry processes in mixed reality environments, wherein representative patterns of interaction and dominant modes of communications in the environment were revealed. We found that increased student-to-student interactions was accompanied by improved observational accuracy and higher quality student explanations.
"
LoopMaker: Automatic Creation of Music Loops from Pre-recorded Music,https://dl.acm.org/authorize?N657265,"
Music loops are seamlessly repeatable segments of music that can be used for music composition as well as backing tracks for media such as videos, webpages, and games. They are regularly used by both professional musicians as well as novices with very little experience in audio editing and music composition. The process of creating music loops can be challenging and tedious, particularly for novices. We present LoopMaker, an interactive system that assists users in creating and exploring music loops from pre-recorded music. Our system can be used in a semi-automatic mode in which it refines a user's rough selection of a loop. It can also be used in a fully automatic mode in which it creates a number of loops from a given piece of music and interactively allows the user to explore these loops. Our user study suggests that our system makes the loop creation process significantly faster, easier, and more enjoyable than manual creation for both novices and experts. It also suggests that the quality of these loops are comparable to manually created loops by experts. We have presented an interactive system to assist novices and experts in creating music loops. Our user studies helped identify tedious and challenging parts of the loop creation process. Our system automates these parts, allowing users to concentrate on the more creative aspects of the process. These studies suggest that loops created using our system are significantly easier to create and are of comparable quality to those created by experts. We aim to explore other aspects of audio content creation that enable users to concentrate on the creative aspects of the process.

"
Doppio: Tracking UI Flows and Code Changes for App Development,https://dl.acm.org/authorize?N657266,"
Developing interactive systems often involves a large set of callback functions for handling user interaction, which makes it challenging to manage UI behaviors, create descriptive documentation, and track code revisions. We developed Doppio, a tool that automatically tracks and visualizes UI flows and their changes based on source code. For each input event listener of a widget, e.g., onClick of an Android View class, Doppio captures and associates its UI output from a program execution with its code snippet from the codebase. It automatically generates a screenflow diagram organized by the callback methods and interaction flow, where developers can review the code and UI revisions interactively. Doppio, as an IDE plugin, is seamlessly integrated into a common development workflow. Our studies show that our tool is able to generate quality visual documentation and helped participants understand unfamiliar source code and track changes. We present Doppio, a tool that automatically tracks and visualizes UI flows and their changes based on source code elements and their revisions. We integrate Doppio, as an IDE plugin, seamlessly into a development workflow to generate interactive screenflow diagrams organized by the callback methods and input sequences. We tested Doppio on a range of open source projects, which present compelling results on visual documentation. We also evaluated Doppio with 16 professional developers and gained positive feedback.
"
“It's not actually that horrible”: Exploring Adoption of Two-Factor Authentication at a University,https://dl.acm.org/authorize?N657267,"
Despite the additional protection it affords, two-factor authentication (2FA) adoption reportedly remains low. To better understand 2FA adoption and its barriers, we observed the deployment of a 2FA system at Carnegie Mellon University (CMU). We explore user behaviors and opinions around adoption, surrounding a mandatory adoption deadline. Our results show that (a) 2FA adopters found it annoying, but fairly easy to use, and believed it made their accounts more secure; (b) experience with CMU Duo often led to positive perceptions, sometimes translating into 2FA adoption for other accounts; and, (c) the differences between users required to adopt 2FA and those who adopted voluntarily are smaller than expected. We also explore the relationship between different usage patterns and perceived usability, and identify user misconceptions, insecure practices, and design issues. We conclude with recommendations for large-scale 2FA deployments to maximize adoption, focusing on implementation design, use of adoption mandates, and strategic messaging. We presented our exploration of Duo 2FA adoption and usage in the heterogeneous context of an American university. Taken as a whole, these results show that even though most users found Duo annoying, they also found it easy to use and, in some cases, easier than they had expected. We see that experience with 2FA and CMU Duo often led to positive perceptions, sometimes translating into 2FA adoption for other accounts and that the differences between those required to adopt 2FA and those who adopted voluntarily were smaller than expected. We found that experiencing negative consequences, from disrupted tasks to email lockout, or frequent smaller issues with Duo (e.g. not having your phone nearby) led to more negative perceptions, as did behaviors that reduced access to convenience features (e.g. using multiple and public computers hinders the use of the “remember me” option). We identified misconceptions that led to a limited use of this option, insecure practices that 2FA can help identify and mitigate (e.g. credential sharing), and design issues with the Duo platform and CMU’s implementation. Our findings led us to identify approaches to help improve user experience and motivate current and future adoption of 2FA. We provided recommendations to those considering 2FA adoption, focusing on implementation design, adoption mandates, and strategic messaging.

"
"Am I a Bunny?: The Impact of High and Low Immersion Platforms and Viewers' Perceptions of Role on Presence, Narrative Engagement, and Empathy during an Animated 360° Video",https://dl.acm.org/authorize?N657268,"
This study used both quantitative and qualitative data to assess whether a High Immersion viewing platform (virtual reality headset) elicits stronger feelings of narrative engagement and empathy compared to a Low Immersion platform (smartphone) when viewing an animated 360° video. In line with prior research, participants (N = 65) reported greater feelings of presence in the High Immersion condition compared to Low Immersion. However, immersive condition was not significantly related to narrative engagement or empathy. Interview responses revealed that participants' perceptions of their role in the film experience (i.e., Character, Observer, or Other/Not Sure) varied and were significantly related to narrative engagement. Participants who saw themselves as a Character (versus Observer) reported higher narrative engagement and empathy. Findings suggest that although a more immersive viewing platform can enhance presence during a 360° video experience, a clear understanding of viewer role is both difficult to achieve and critical to story comprehension and empathy. This study showed that, contrary to expectations, viewing a 360° video on a VR HMD with more immersive features did not necessarily encourage better comprehension of the story, involvement with the narrative, or empathy with the characters compared to a less immersive smartphone platform. As technologies become increasingly immersive, the need for research on ways to help viewers engage with the stories and characters also increases. This research suggests that at least one criteria for engagement and empathy is for participants to have a clear grasp of their role within the experience. Immersive technology has vast potential for helping viewers experience new places and points of view. Creators of content such as 360° video viewed through immersive platforms will need to develop a set of tools that can be used to help viewers understand their intended role.

"
Making Sense of Blockchain Applications: A Typology for HCI,https://dl.acm.org/authorize?N657269,"
Blockchain is an emerging infrastructural technology that is proposed to fundamentally transform the ways in which people transact, trust, collaborate, organize and identify themselves. In this paper, we construct a typology of emerging blockchain applications, consider the domains in which they are applied, and identify distinguishing features of this new technology. We argue that there is a unique role for the HCI community in linking the design and application of blockchain technology towards lived experience and the articulation of human values. In particular, we note how the accounting of transactions, a trust in immutable code and algorithms, and the leveraging of distributed crowds and publics around vast interoperable databases all relate to longstanding issues of importance for the field. We conclude by highlighting core conceptual and methodological challenges for HCI researchers beginning to work with blockchain and distributed ledger technologies. This paper presents a detailed mapping and examination of emerging applications of blockchain technologies, in an effort to chart the space for the HCI community. We present a typology of seven classes of blockchain applications: underlying infrastructure; currency; financial services; proof-as-a-service; property and ownership; identity management and governance. We propose that these applications present some fundamental human challenges, related to financialization, procedural trust, algorithmic governance and the front-end interactions with such an infrastructural technology. As the HCI community develops a better understanding, we encourage researchers to hold blockchain applications to account, find ways to engage and design blockchain with participants, and expand the current imagination.
"
"""Accessibility Came by Accident"": Use of Voice-Controlled Intelligent Personal Assistants by People with Disabilities",https://dl.acm.org/authorize?N657260,"
From an accessibility perspective, voice-controlled, home-based intelligent personal assistants (IPAs) have the potential to greatly expand speech interaction beyond dictation and screen reader output. To examine the accessibility of off-the-shelf IPAs (e.g., Amazon Echo) and to understand how users with disabilities are making use of these devices, we conducted two exploratory studies. The first, broader study is a content analysis of 346 Amazon Echo reviews that include users with disabilities, while the second study more specifically focuses on users with visual impairments, through interviews with 16 current users of home-based IPAs. Findings show that, although some accessibility challenges exist, users with a range of disabilities are using the Amazon Echo, including for unexpected cases such as speech therapy and support for caregivers. Richer voice-based applications and solutions to support discoverability would be particularly useful to users with visual impairments. These findings should inform future work on accessible voice-based IPAs. With the increasing adoption of voice-controlled conversational interfaces and home-based IPAs, including people with disabilities in the design of these technologies is critical. To understand current use, we analyzed 346 Amazon Echo reviews that mentioned a user with a disability and interviewed 16 blind and visually impaired participants who owned a home-based IPA. The first study showed that users with a range of disabilities are using the Amazon Echo, including for unexpected cases such as speech therapy and support for caregivers. Study 2 provided a more in-depth analysis of one specific group—users who are blind or visually impaired—with findings reflecting the first study as well as emphasizing the efficiency of the devices for a variety of tasks, difficulties with discovering new functionality, and the desire for richer voice-only applications. However, accessibility challenges related to speech input and output still exist (Study 1), along with issues with the device ecosystem (both studies). As exploratory research, these findings should inform future work on accessible voice-based IPAs.
"
Conveying the Perception of Kinesthetic Feedback in Virtual Reality using State-of-the-Art Hardware,https://dl.acm.org/authorize?N657261,"
Including haptic feedback in current consumer VR applications is frequently challenging, since technical possibilities to create haptic feedback in consumer-grade VR are limited. While most systems include and make use of the possibility to create tactile feedback through vibration, kinesthetic feedback systems almost exclusively rely on external mechanical hardware to induce actual sensations so far. In this paper, we describe an approach to create a feeling of such sensations by using unmodified off-the-shelf hardware and a software solution for a multi-modal pseudo-haptics approach. We first explore this design space by applying user-elicited methods, and afterwards evaluate our refined solution in a user study. The results show that it is indeed possible to communicate kinesthetic feedback by visual and tactile cues only and even induce its perception. While visual clipping was generally unappreciated, our approach led to significant increases of enjoyment and presence. VR got to a point, where most of all the visual consumer hardware has made huge steps. On the other hand, there is a tendency towards direct interaction using tracked controllers, where users stand or even walk in reality. This additional degree of freedom also leads to new challenges concerning the mismatch of real and virtual world. Compared to the use of indirect interaction, like playing with a gamepad, users expect haptic or kinesthetic feedback when touching virtual objects. The current hardware is very limited displaying haptic features while the controller in the user’s hands as well as vibration is the only available haptic modality. We used the state-of-the-art-hardware, implemented different haptic representations using vibration and pseudo-haptics. Our pseudo-haptic manipulation go much farther than prior reported ones, and can lead to obvious breaks with proprioception, but thereby increase their expressiveness. We measured the influence on immersion, enjoyment and perception related items, which were determined in a workshop with VR researchers. We also collected qualitative feedback on how the available channels should be designed and improved. Improving the software implementation based on the suggestions, we found a strong influence of pseudo-haptic effects, while vibration was most of all seen as a supportive channel for visual effects. In addition, we found a very promising interaction between visual and vibration feedback for the communication of kinesthetic feedback. According to our participants, the combination of visual and vibration feedback is sufficient to communicate kinesthetic feedback. We therefore argue, that when being implemented well, kinesthetic feedback can not only be used to increase immersion, but also to increase enjoyment by becoming part of the game play.
"
A Bermuda Triangle?,https://dl.acm.org/authorize?N657262,"
User experience (UX) evaluation is a growing field with diverse approaches. To understand the development since previous meta-review efforts, we conducted a state-of-the-art review of UX evaluation techniques with special attention to the triangulation between methods. We systematically selected and analyzed 100 papers from recent years and while we found an increase of relevant UX studies, we also saw a remaining overlap with pure usability evaluations. Positive trends include an increasing percentage of field rather than lab studies and a tendency to combine several methods in UX studies. Triangulation was applied in more than two thirds of the studies, and the most common method combination was questionnaires and interviews. Based on our analysis, we derive common patterns for triangulation in UX evaluation efforts. A critical discussion about existing approaches should help to obtain stronger results, especially when evaluating new technologies. To analyze the current state of UX evaluation in academia, we systematically identified 280 relevant papers, out of which 100 papers were finally selected for full review. In the continuously growing number of papers over the years 2010-2016, an increasing diversity in this inherently multi-dimensional field can be found. This is, of course, an asset: a product may (at different stages of development) benefit from both “macro”- and “micro”-perspective evaluations [57]. However, we see that many of the challenges reported in earlier meta-reviews still remain, such as the weak links between theory and evaluation, little attention to expectations in UX, and a tendency towards self-defined questionnaires and post-use evaluation. Progress could be identified in the use of triangulation, by inclusion of more methods as well as a larger number of studies performed in field contexts. Thus, we can perhaps see land on the horizon, but conclude that UX evaluation currently still remains sort of a Bermuda Triangle, often depending on personal perceptions of UX rather than aggregated theory. We see that interest and efforts in the UX field still persist, and we look forward to further work. Areas which need to be addressed are evaluation approaches to multi-device experiences, machine learning, upcoming technology for virtual experiences, and addressing expectations in UX. Method triangulation needs to be used more coherently; for stronger results in UX studies, we recommend to integrate and structure data better. For example, a well-defined structuring of observational data, improved cross-analysis of qualitative and quantitative data, and a solid definition, which aspect of UX to evaluate, will bring results forward.

"
Designing Future Social Wearables with Live Action Role Play (Larp) Designers,https://dl.acm.org/authorize?N657263,"
Designing wearable technology that supports physical and social engagement in a collocated setting is challenging. In this research, we reached out to an expert community of crafters of social experiences: larpers (live action role players). Larpers and larp designers have a longstanding tradition of designing and making use of a variety of elements, such as costumes, physical objects, environments, and recently also digital artifacts. These are crafted in support of co-experience values that we argue can inform the design of social wearables. We engaged in a co-design process with a game designer and co-founder of a larp production company, and embedded the resulting social wearables in a larp. Here, we present the results of this design and implementation process, and articulate design affordances that resonate with our larp designer' values. This work may inspire and inform researchers and designers creating wearable technology that is aimed at supporting collocated engagement. We set out to explore the potential for wearables to augment collocated social interaction, by engaging in a Research through Design process to create wearables embedded in a larp, working closely with a larp designer. With the help of our larp design expert, we identified promising roles for the technology to support larp designers’ co-experience values of immersion and the experience of the many: tracking the characters’ scores and preferred physical interaction style, supporting in-person and in-character expression and communication, connection with one’s character, and mitigating the need for out of character communication. We took the concept of social affordances and articulated some to guide the design process towards these roles and co-experience values: social signaling, spectator sensitivity, social appropriateness, and emotional resonance. To translate these into design choices and features, we made use of concepts in HCI for the design of technology in collocated social spaces, such as the spectator experience and concepts in proxemics. These choices were frequently discussed with our expert larp designer, to make sure that they would fit and support the larp’s magic circle of play. The completed designs, novel in the domain of larps [41] embodied well the values of immersion and the experience of the many, performing functions including keeping track of and displaying scores, indicating preferred physical interaction styles, supporting players’ expressivity and communication with others, and enhancing players’ connection with their own characters. Reflecting about the positive reception of our designs, Michael commented that “larpers want to use technology in their games as long as the technology supports the immersion of the game.” In fact players adapted the wearables to suit their own narrative purposes, using them in unanticipated ways. Many of the affordance patterns and principles we developed and tested in this play situation have potential to apply to the design of wearables outside the context of play. We are currently drawing upon the lessons learned in this project, to create wearables that support interaction in everyday non-game contexts.
"
Antibiotic-Responsive Bioart: Exploring DIYbio as a Design Studio Practice,https://dl.acm.org/authorize?N657264,"
Our work links hybrid practices from biology, fine arts, and design in a studio setting to support materially-oriented engagement with biotechnology. Using autoethnographic methods, we present our two-year process of converting an HCI studio into a BSL-1 (biosafety level 1) facility, our iterative development of low-cost tools, and our own self-reflexive experimentation with (DIY)bio protocols. Insights from this work led us to design a weeklong bioart course, whereby junior highschool students creatively ""painted"" with bacteria and antibiotic substances, digitally designed stencils from the resulting petri dish images, and screenprinted them onto physical artifacts. Our findings reveal the nuances of working with biological, analog, and digital materials in a design studio setting. We conclude by reflecting on DIYbio studio as a gathering of diverse actors who work with hybrid materials to give physical form to matters of concern. This paper framed DIYbio—hands-on work with biology outside of professional settings—as a design studio practice. We presented our autoethnographic process whereby our multidisciplinary team set up a BSL-1 design studio, practiced biology protocols, and iteratively designed low-cost tools to support this work. Insights from our self-reflexive practice led us to develop a bioart course that combined techniques from microbiology, fine arts, and digital design to create antibiotic-responsive microbial art artifacts. Our approach embraced design studio culture through open-ended activities, access to a variety of design media, frequent reflection and feedback from peers, and work on projects that materialized heterogeneous concerns. Above all, we hope to have shown that linking hybrid practices in a studio setting is a powerful approach for HCI because it offers new ways to engage with issues. This approach can be applied to support meaningful discourse around the intellectual and social challenges poses by emerging biotechnologies.
"
More Than a Show: Using Personalized Immersive Theater to Educate and Engage the Public in Technology Ethics,https://dl.acm.org/authorize?N657275,"
Devising strategies to engage the public in discussions around the design and development of technology is critical to building a future that works for everyone. This paper presents a novel case study, an immersive theater experience, ""Quantified Self,"" that combines aspects of design fiction and user enactments to construct a public engagement opportunity about technology ethics. Our audience supplied their social data (Facebook, Twitter...) and received a personalized experience where they interacted with a narrative and technology exhibits. We used a design model targeting goals of engagement, education, and discussion. Here we overview the design and production of Quantified Self and report on the results (240 participants over 6 performances) and findings from audience surveys (n=179/240) and cast/crew interviews (n=15/22). We found our approach attracted a wide audience interested in different elements of the show. Affordances and challenges of our model are discussed in detail. In this paper, we presented and a discussed a case study using immersive theater to engage, educate, and bring together technical and non-technical participants into data ethics discussions. Aiming for a balance between technical and artistic methods, and fixed and improvised modes of interaction, we developed Quantified Self: Immersive Theater and Data Experience. We found that mixing immersive theater with interactive, social-media-driven technology exhibits created opportunities for multiple forms of engagement, although many users wanted even greater interaction and personalization. Drawing from this work, we hope to see future research incorporate elements to engage non-technologists in ethical technology design and discussions.
"
CatAR: A Novel Stereoscopic Augmented Reality Cataract Surgery Training System with Dexterous Instruments Tracking Technology,https://dl.acm.org/authorize?N657276,"
We propose CatAR, a novel stereoscopic augmented reality (AR) cataract surgery training system. It provides dexterous instrument tracking ability using a specially designed infrared optical system with 2 cameras and 1 reflective marker. The tracking accuracy on the instrument tip is 20 µm, much higher than previous simulators. Moreover, our system allows trainees to use and to see real surgical instruments while practicing. Five training modules with 31 parameters were designed and 28 participants were enrolled to conduct efficacy and validity tests. The results revealed significant differences between novice and experienced surgeons. Improvements in surgical skills after practicing with CatAR were also significant. To our knowledge, CatAR is the first AR microsurgery simulator and also the first system using real instruments as the user interface. This system not only provides high spatial resolution stereoscopic AR images with realistic haptic feedback, but also tracks the surgical instruments with ultrahigh accuracy (20 µm) in real time. CatAR can discriminate surgical performance between different experience levels and can become a new assessment tool for surgical proficiency. The 3D motions during practice are recorded, and could be crucial training data sets for AI surgery in the future. The next step of this study is to investigate the capability of skill transfer using AR technology. Feedback from the participants will enable a physics simulation of the capsule and lens material, more realistic rendering effects, and more advanced modules to practice.

"
"You Watch, You Give, and You Engage: A Study of Live Streaming Practices in China",https://dl.acm.org/authorize?N657277,"
Despite gaining traction in North America, live streaming has not reached the popularity it has in China, where live- streaming has a tremendous impact on the social behaviors of users. To better understand this socio-technological phenomenon, we conducted a mixed methods study of live streaming practices in China. We present the results of an online survey of 527 live streaming users, focusing on their broadcasting or viewing practices and the experiences they find most engaging. We also interviewed 14 active users to explore their motivations and experiences. Our data revealed the different categories of content that was broadcasted and how varying aspects of this content engaged viewers. We also gained insight into the role reward systems and fan group-chat play in engaging users, while also finding evidence that both viewers and streamers desire deeper channels and mechanisms for interaction in addition to the commenting, gifting, and fan groups that are available today. We empirically examined user’s live streaming practices in China to better understand such social-technological phenomenon, which has a large and ever-growing market, and different content types, compared to those in North America. By understanding the motivations, practices, social interactions that occur beyond the streams, and the important factors of engagement, we identified that the unique challenges and viewing behaviors encountered with Chinese live streams can be used as blueprint for the future of North American and European live streaming platforms and services. Our work also outlined the importance of expanding thinking about the current demographics of users, live streaming within educational and finance applications, and the needs to support deeper and richer interactions between viewers and streamers.

"
Bolt: Instantaneous Crowdsourcing via Just-in-Time Training,https://dl.acm.org/authorize?N657278,"
Real-time crowdsourcing has made it possible to solve problems that are beyond the scope of artificial intelligence (AI) within a matter of seconds, rather than hours or days with traditional crowdsourcing techniques. While this has led to an increase in the potential application domains of crowdsourcing and human computation, problems that require machine-level speeds---on the order of milliseconds, not seconds---have remained out of reach because of the fundamental bounds of human perception and response time. In this paper, we demonstrate that it is possible to exceed these bounds by combining human and machine intelligence. We introduce the look-ahead approach, a hybrid intelligence workflow that enables instantaneous crowdsourcing systems (i.e., those that can return crowd responses within mere milliseconds). The look-ahead approach works by exploring possible future states that may be encountered within a short time horizon (e.g., a few seconds into the future) and prefetching crowd worker responses to these states. We validate the efficacy and explore the limitations of our approach on the Bolt system, which consists of an arcade-style game (Lightning Dodger) that we formally model as a Markov Decision Process (MDP). When the MDP reward function is unspecified---as in many real-world tasks---the look-ahead approach enables just-in-time (JIT) training of the agent's policy function. Through a series of crowd worker experiments, we demonstrate that the look-ahead approach can outperform the fastest individual worker by approximately two orders of magnitude. Our work opens new avenues for hybrid intelligence systems that are as smart as people, but also far faster than humanly possible. We have introduced the look-ahead approach to facilitate the creation of Bolt, the first instantaneous crowdsourcing system. These systems use real-time crowdsourcing in a hybrid intelligence workflow to provide just-in-time training of automated agents, achieving final response latencies on the order of milliseconds (median of 2ms), instead of seconds, hours, or days. Our experiments with crowd workers demonstrate that a two order of magnitude speedup from the baseline response time is possible via our approach, and accuracy improvements are attainable via aggregation. While these results demonstrate the promise of the look-ahead approach and open avenues for instantaneous crowd-powered systems, scaling to larger state spaces using probabilistic sampling and queuing techniques remains an exciting and open problem for future work to explore.
"
Convey: Exploring the Use of a Context View for Chatbots,https://dl.acm.org/authorize?N657279,"
Text messaging-based conversational systems, popularly called chatbots, have seen massive growth lately. Recent work on evaluating chatbots has found that there exists a mismatch between the chatbot's state of understanding (also called context) and the user's perception of the chatbot's understanding. Users found it difficult to use chatbots for complex tasks as the users were uncertain of the chatbots' intelligence level and contextual state. In this work, we propose Convey (CONtext View), a window added to the chatbot interface, displaying the conversational context and providing interactions with the context values. We conducted a usability evaluation of Convey with 16 participants. Participants preferred using chatbot with Convey and found it to be easier to use, less mentally demanding, faster, and more intuitive compared to a default chatbot without Convey. The paper concludes with a discussion of the design implications offered by Convey. In this work, we added a context view called Convey to the top of the chatbot interface to help users have an understanding of the mental-state of the chatbot during the conversation (helping users and chatbot be on the same page) while sustaining the familiarity of the text-based messaging interface. Moreover, Convey adds the benefits of a form-based user interface by enabling entry of precise input through the interactive elements. The results from a 16-participant user study demonstrated that participants perceived chatbot with Convey to be faster and easier to use. Convey is generalizable to chatbots in any domain, and in future, we expect Convey to be integrated and offered by many chatbot-hosting platforms.

"
Confronting Social Criticisms: Challenges when Adopting Data-Driven Policing Strategies,https://dl.acm.org/authorize?N657270,"
Proponents of data-driven policing strategies claim that it makes policing organizations more effective, efficient, and accountable and has the potential to address some policing social criticisms (e.g. racial bias, lack of accountability and training). What remains less understood are the challenges when adopting data-driven policing as a response to these criticisms. We present results from a qualitative field study about the adoption of data-driven policing strategies in a Midwestern police department in the United States. We identify three key challenges police face with data-driven adoption efforts: data-driven frictions, precarious and inactionable insights, and police metis concerns. We demonstrate the issues that data-driven initiatives create for policing and the open questions police agents face. These findings contribute an empirical account of how policing agents attend to the strengths and limits of big data's knowledge claims. Lastly, we present data and design implications for policing. For the informants at MMPD, challenges of adopting datadriven strategies in policing are experienced as they negotiate the sociotechnical visions for their work, some of which were explicitly meant to confront social criticisms of policing. In this paper, we have made the following contributions: identified three key challenges in adopting of data-driven policing; identified how data can support and constrain police work and accountability; provided design implications for data use in police accountability.
"
Weaving Lighthouses and Stitching Stories: Blind and Visually Impaired People Designing E-textiles,https://dl.acm.org/authorize?N657271,"
We describe our experience of working with blind and visually impaired people to create interactive art objects that are personal to them, through a participatory making process using electronic textiles (e-textiles) and hands-on crafting techniques. The research addresses both the practical considerations about how to structure hands-on making workshops in a way which is accessible to participants of varying experience and abilities, and how effective the approach was in enabling participants to tell their own stories and feel in control of the design and making process. The results of our analysis is the offering of insights in how to run e-textile making sessions in such a way for them to be more accessible and inclusive to a wider community of participants. Our process has involved careful design decisions and planning, considering what is accessible, affordable and modular – to allow for ownership and creativity. We delivered a series of workshops where every participant embraced participatory making. All participants completed highly personal e-textile art pieces that expressed stories of their own choosing using texture, shape, and sound. The rich qualitative data collected throughout the workshops provides evidence of learning (each participant was able to create and test soft circuits), of creativity and self-expression (the art pieces are highly individual, and some of the participants altered or expanded the design brief), that participants exceeded their own expectations (e.g. using tools or techniques they initially said they could not handle), and that they took pride in their art pieces – not only when demonstrating them in the showcase events, but also when sharing them with friends and family in their homes.
"
This Changes Sustainable HCI,https://dl.acm.org/authorize?N657272,"
More than a decade into Sustainable HCI (SHCI) research, the community is still struggling to converge on a shared understanding of sustainability and HCI's role in addressing it. We think this is largely a positive sign, reflective of maturity; yet, lacking a clear set of aims and metrics for sustainability continues to be the community's impediment to progressing, hence we seek to articulate a vision around which the community can productively coalesce. Drawing from recent SHCI publications, we identify commonalities that might form the basis of a shared understanding, and we show that this understanding closely aligns with the authoritative conception of a path to a sustainable future proffered by Naomi Klein in her book emphThis Changes Everything. We elaborate a set of contributions that SHCI is already making that can be unified under Klein's narrative, and compare these categories of work to those found in past surveys of the field as evidence of substantive progress in SHCI. What we have presented in this paper is bound to cause controversy. We are under no illusions: a proportion of the community will disagree with the details of the vision set forth; and some will undoubtedly object to the project attempted herein of articulating a shared vision to begin with. We felt strongly, however, that it was important to illustrate the clarity that comes with pinning down a vision of SHCI going forward, and to invite the community to discuss this together. While we drew from many other voices in the field in formulating this vision, we also recognize that we have clearly argued in favor of one of the two dominant views represented by SHCI. We do not aim to be dictatorial; but equally we do not believe that all conceptions of sustainability have equal merit, and we fear that simultaneous pursuit of multiple ends may be counter-productive. Further, we understand this to be a controversial position that may, in fact, go against the view presented by Silberman et al. [94] which we (the authors) had contributed to only a few years ago. Unfortunately, the ‘procrastination penalty’ applies to SHCI as it does to climate change. We have spent so long debating definitions of sustainability and SHCI, and being indecisive on a direction to pursue, that to have any reasonable chance of affecting change, we must make a bold and radical decision on a future course of action. Our own experiences in participating in numerous SHCI workshops and struggling to develop a shared community knowledge base [53] have convinced us that these mechanisms are not enough to bring the SHCI community together, so instead we have attempted a different tactic here: to elaborate a vision. We are, in doing so, advocating the privileging of a particular perspective on sustainability that a) is sufficiently radical to enable us to affect root causes of unsustainability, and b) is focused enough to enable us to conceive of research activities to undertake in concert as a community. Despite having spent so many years peeling back layers of the onion to discover added complexities and interconnections that make a singular definition of sustainability so elusive, there do appear to be points of leverage within Klein’s conception of a sustainable future which, if we were to focus on these, would address the many and varied manifestations of unsustainability that would seem to motivate all researchers doing SHCI work. Finally, we contend that ‘doing good’—the nebulous aim that unifies all definitions of sustainability [4, 59]—is a noble goal, but not especially useful for orienting an HCI research agenda that challenges existing norms of HCI research (don’t all researchers think they are contributing to a better world?), particularly when it requires each individual researcher to define what ‘good’ looks like. Without a clear vision that can be communicated to HCI more broadly, sustainability, and SHCI by extension, are easily dismissed as meaningless— as useful only in demarcating one’s inclusion in a a counterculture (e.g. tree-huggers) within HCI. Indeed, the moralizing connotations of the term sustainability when ill-defined has the potential to create a problematic us-versus-them culture within HCI, with us playing the role of nuisance or ‘deviant’ [52]. We hope that articulating a pragmatic vision of what HCI (notably, not just SHCI) needs to do to stay relevant in the project of realizing a sustainable future enables those who had been put off by sustainability rhetoric to understand the purpose of designing for sustainability within HCI, and why they too should be involved in the effort.
"
Use the Right Sound for the Right Job: Verbal Commands and Auditory Icons for a Task-Management System Favor Different Information Processes in the Brain,https://dl.acm.org/authorize?N657273,"
Design recommendations for notifications are typically based on user performance and subjective feedback. In comparison, there has been surprisingly little research on how designed notifications might be processed by the brain for the information they convey. The current study uses EEG/ERP methods to evaluate auditory notifications that were designed to cue long-distance truck drivers for task-management and driving conditions, particularly for automated driving scenarios. Two experiments separately evaluated naive students and professional truck drivers for their behavioral and brain responses to auditory notifications, which were either auditory icons or verbal commands. Our EEG/ERP results suggest that verbal commands were more readily recognized by the brain as relevant targets, but that auditory icons were more likely to update contextual working memory. Both classes of notifications did not differ on behavioral measures. This suggests that auditory icons ought to be employed for communicating contextual information and verbal commands, for urgent requests. Taken together, the current work contributes by showing that auditory notifications can be evaluated and functionally discriminated for how they are processed by the brain for information. This has implications for the operational context as well as design. Choices for which notifications to use for which purpose can be based not only in terms of response times and discrimination accuracy, which is not necessarily the operational objective, but in terms of how the notifications are: (1) detected against the auditory scene, (2) discriminated against other notification targets, (3) likely to capture attention, and (4) capable of updating contextual working memory. To date, most studies have questioned whether verbal commands or auditory icons serve better as notifications, namely in terms of how well they elicit a speeded and accurate response. The current findings suggest that this question, while well-intentioned, is misplaced. Our results demonstrate that verbal commands and auditory icons have different qualities. While verbal commands are better discriminated against other notifications, auditory icons can update contextual working memory with less effort. Practically speaking, this suggests that verbal icons are ideally used for time-critical information where there is no leeway for ambiguity, e.g., collision warnings. Meanwhile, auditory icons are likely to be more effective in communicating contextual information, such as entry into a poorly maintained road section or changing weather conditions. In other words, verbal commands and auditory icons should be used as complementary (and not competing) notifications. Previous research has recommended using auditory icons to notify users of environmental events [17, 32]. More specifically, auditory icons have been suggested to enhance situational awareness [1, 31]. For example, a walking sound can more effectively indicate a nearing pedestrian. In addition, auditory icons might be favored because it is believed that they can be processed in parallel to other auditory events [1]. These findings so far converge with our current results and interpretation. Nonetheless, there are works that do not. For example, contrary to our current believe, that verbal commands capture attention, some work have shown that certain auditory icons (i.e., car horn) result in significantly faster response times (e.g., [24]). We might account for this by the fact that some auditory icons are overlearned to indicate danger. It should be noted that verbal processing is known to differ for different word classes (i.e., verbs, nouns) [58, 67]. The current study only uses nouns for verbal commands and, thus, future studies should verify whether verbal commands attract attention preferentially for all word classes, relative to auditory icons. In this work, we present EEG/ERP evidence that discriminates for how auditory icons and verbal commands are processed by the brain. Nonetheless, we do not doubt that nuances in how auditory notifications are engineered could ultimately render an auditory icon attention-grabbing and/or a verbal command more suited for communicating context. Our current results contribute by providing a starting point for understanding what type of sounds ought to be employed for which purposes, bearing in mind the brain’s likely response to them. The participants in Experiment 1 possessed neither a language proficiency for the verbal commands nor an expert understanding of the operational tasks that the notifications indicated. Therefore, the EEG/ERP differences (i.e., P2, P3b) found between verbal commands and auditory icons can be considered as general differences between the two notification classes. In contrast, Experiment 2 was performed on professional truck drivers in a highly realistic test environment. A comparison between the two experiments reveals that these differences in brain responses scale with realism and user proficiency. Thus, the current approach of evaluating notification designs on the basis of brain responses is robust, even when behavioral responses do not differ. Notifications that are first designed in sterile lab environments could also be evaluated for the EEG/ERP responses that they elicit. This would narrow down the candidates for deployment and validation in high fidelity simulation environments or field-testing. Besides this, EEG/ERP methods could also be used to discriminate between different instantiation of the same target notification. One example would be to determine the preferability of semantically comparable verbal commands, such as tank or fuel. To conclude, the current work suggests that verbal commands and auditory icons serve different purposes, at least from the standpoint of how they are processed by the brain. Thus, evaluations that directly compare them in terms of performance measures might not be appropriate. This might also explain the mixed evidence from previous studies in support of either auditory notifications. The growing accessibility of brain recording methods (i.e., EEG) mean that the current approach can be used to support finer functional discriminations for notifications and can be effectively deployed, even in challenging deployment scenarios such as high fidelity truck simulators.
"
Iris: A Conversational Agent for Complex Tasks,https://dl.acm.org/authorize?N657274,"
Today, most conversational agents are limited to simple tasks supported by standalone commands, such as getting directions or scheduling an appointment. To support more complex tasks, agents must be able to generalize from and combine the commands they already understand. This paper presents a new approach to designing conversational agents inspired by linguistic theory, where agents can execute complex requests interactively by combining commands through nested conversations. We demonstrate this approach in Iris, an agent that can perform open-ended data science tasks such as lexical analysis and predictive modeling. To power Iris, we have created a domain-specific language that transforms Python functions into combinable automata and regulates their combinations through a type system. Running a user study to examine the strengths and limitations of our approach, we find that data scientists completed a modeling task 2.6 times faster with Iris than with Jupyter Notebook. In this paper, we show how conversational agents can draw on human conversational strategies to combine commands together, allowing them to assist us with tasks they have not been explicitly programmed to support. We showcase these ideas in Iris, an agent designed to help users with data science and machine learning tasks. More broadly, our work demonstrates how simple models of conversation can lead to surprisingly complex emergent outcomes.
"
"Explaining Viewers' Emotional, Instrumental, and Financial Support Provision for Live Streamers",https://dl.acm.org/authorize?N657285,"
On live streams, viewers can support streamers through various methods ranging from well-wishing text messages to money. In this study (N=230) we surveyed viewers who had given money to a streamer. We identified six motivations for why they gave money to their favorite live streamer. We then examined how factors related to viewer, streamer, and viewer-streamer interaction were associated with three forms of social support provision: emotional, instrumental, and financial support. Our main findings are: parasocial relationship was consistently correlated with all three types of social support, while social presence was only related with instrumental and financial support; interpersonal attractiveness was associated with emotional and instrumental support and lonely people were more likely to give instrumental support. Our focus on various types of social support in a live streaming masspersonal platform adds a more detailed understanding to the existing literature of mediated social support. Furthermore, it suggests potential directions for designing more supportive and interactive live streaming platforms. Is giving money to one’s favorite live streamer a charitable donation, compensation for entertainment/education services, or form of social support? When asking people who have already given money to streamers in the past, our qualitative results found that viewers had wide range of reasons on why they gave money to streamers in the past. Some viewed the financial exchange as extremely transactional (e.g., paying for what I received as a service) while others associated emotional connections and care with their monetary contribution. Using a social support lens to distinguish emotional, instrumental, and financial support, we asked participants how willing they would be to offer these different types of social support in the future. Our statistical models indicate that social presence is an indicator of tangible support intentions—both instrumental and financial—and parasocial relationships are a positive predictor of all three support types. Our focus on live streaming as a unique, masspersonal platform for varied types of social support provision builds on literature of social support in mediated social systems and especially adds to our understanding of tangible support provision (i.e., instrumental and financial support), which is relatively understudied compared to information or emotional support. With more individuals becoming part time or full time live streamers, as well as the growing utilization of live streaming for online shopping experiences, fundraising, and education, we hope that our findings will be the starting point of increased interest in the tangible aspects of social support from both applied and theoretical perspectives.

"
Making the News: Digital Creativity Support for Journalists,https://dl.acm.org/authorize?N657286,"
This paper reports the design and first evaluations of new digital support for journalists to discover and examine crea-tive angles on news stories under development. The support integrated creative news search algorithms, interactive crea-tive sparks and reusable concept cards into one daily work tool of journalists. The first evaluations of INJECT by jour-nalists in their places of work to write published news sto-ries revealed that the journalists generated new angles on existing stories rather than new stories, changed their writ-ing behaviour, and reported evidence that INJECT use had the potential to increase the objectivity and the boldness of journalism methods used. As such, journalist use of INJECT was effective, but not to the degree that the tool was designed to support. Journalist use of INJECT also provided more effective support for writing feature stories rather than news stories, although this finding might have been influenced by INJECT’s data layer, which was only composed of past news rather than current social media information. However, writing feature stories will still require journalists to exhibit more creative thinking than during the evaluations, by applying more creativity skills when using INJECT than was reported. Results revealed that use of INJECT offered more support to journalists when discovering rather than examining [26]. It appeared to contribute to journalists being more ambitious and to overcome biases by retrieving topic information from alternative news sources and discovering new stances from which to investigate news topics. However, this support for discovering also led journalists to compare INJECT, sometimes unfavorably, to Google search, a comparison reinforced by some of the journalists using Google search after INJECT to seek more information on new angles. At least some of the journalists might have expected INJECT to discover the information that was needed, rather than just to offer creative support with this information, as INJECT was designed to do. Although subsequent versions of INJECT now embed Google search to support journalists, to help to distinguish between discovering a new angle with INJECT capabilities and retrieving more information afterwards, journalists will still need to apply more creativity skills to exploit INJECT’s capabilities effectively. The reported uses of INJECT did not appear to deliver and/or encourage sufficient use of these skills. Results also revealed that use of INJECT supported some of the journalists to recognize known news information at the time that news stories are being written. Indeed, different news organizations have identified the potential of INJECT to unlock their own news archives for creative use, to enhance productivity as well as creativity, but only if INJECT can support journalists to recognize [5] related news quickly when writing stories. New recommendation features will be needed to support such recognition.
"
Visual ODLs: Co-Designing Patient-Generated Observations of Daily Living to Support Data-Driven Conversations in Pediatric Care,https://dl.acm.org/authorize?N657287,"
Teens with complex chronic illnesses have difficulty understanding and articulating symptoms such as pain and emotional distress. Yet, symptom communication plays a central role in clinical care and illness management. To understand how design can help overcome these challenges, we created a visual library of 72 sketched illustrations, informed by the Observations of Daily Living framework along with insights from 11 clinician interviews. We utilized our library with storyboarding techniques, free-form sketching, and interviews, in co-design sessions with 13 pairs of chronically-ill teens and their parents. We found that teens depicted symptoms as being interwoven with narratives of personal and social identity. Teens and parents were enthusiastic about collaboratively-generated, interactive storyboards as a tracking and communication mechanism, and suggested three ways in which they could aid in communication and coordination with informal and formal caregivers. In this paper, we detail these findings, to guide the design of tools for symptom-tracking and incorporation of patient-generated data into pediatric care. In this paper, we introduce Visual ODLs and examine its use as design artifacts to elicit complexities of symptomrelated communication faced by teens with complex chronic illnesses. Through co-design activities to envision digital storyboarding technology with Visual ODLs, we generated storyboards representing the teens' reconstructions of their felt experiences. The material presence of Visual ODLs provided scaffolds to structure in-depth discussions about how to foster communication with both family and clinical caregivers using patient-defined and patient-generated data. Our future work will examine approaches to semiautomated tracking in the space of collaborative construction of Visual ODLs and storyboarding. A central design tension includes the need to reduce the burden placed on a user experiencing illness, while enabling full expressive capabilities to capture and represent their felt experience.

"
Improving Discoverability and Expert Performance in Force-Sensitive Text Selection for Touch Devices with Mode Gauges,https://dl.acm.org/authorize?N657288,"
Text selection on touch devices can be a difficult task for users. Letters and words are often too small to select directly, and the enhanced interaction techniques provided by the OS -- magnifiers, selection handles, and methods for selecting at the character, word, or sentence level -- often lead to as many usability problems as they solve. The introduction of force-sensitive touchscreens has added another enhancement to text selection (using force for different selection modes); however, these modes are difficult to discover and many users continue to struggle with accurate selection. In this paper we report on an investigation of the design of touch-based and force-based text selection mechanisms, and describe two novel text-selection techniques that provide improved discoverability, enhanced visual feedback, and a higher performance ceiling for experienced users. Two evaluations show that one design successfully combined support for novices and experts, was never worse than the standard iOS technique, and was preferred by participants. Text selection on touch devices can be a difficult task, and even with several augmentations (e.g. callout magnifiers, selection handles, or word / sentence / paragraph snapping), the task can still frustrate users. The introduction of forcesensitive touchscreens has added another enhancement to text selection (using pressure for different selection modes); however, these modes are difficult to discover and many users continue to struggle with accurate selection. In this paper we reported on an investigation of the design of touch-based and pressure-based text selection mechanisms, and describe a design framework and two novel text-selection techniques that provide improved discoverability and enhanced visual feedback using a ”mode gauge” visualization, and have design features intended to support a higher performance ceiling for experts as well as smooth transitions from novice use. We carried out two studies to evaluate the new techniques. The first study showed that the new techniques (and one in particular, called FS) performed well compared to the standard iOS technique, were never worse than the standard method, and were strongly preferred by participants. A second study that focused on discoverability showed that the gauge visualization was instrumental in helping users explore the range of functionality provided by the new techniques, and that the pressure features of the standard technique were difficult to discover. Our work shows the value of careful attention to design details and underlying principles such as affordances, discoverability, and transitions - even for seemingly-simple interactions like text selection.
"
Vibrational Artificial Subtle Expressions: Conveying System's Confidence Level to Users by Means of Smartphone Vibration,https://dl.acm.org/authorize?N657289,"
Artificial subtle expressions (ASEs) are machine-like expressions used to convey a system's confidence level to users intuitively. So far, auditory ASEs using beep sounds, visual ASEs using LEDs, and motion ASEs using robot movements have been implemented and shown to be effective. In this paper, we propose a novel type of ASE that uses vibration (vibrational ASEs). We implemented the vibrational ASEs on a smartphone and conducted experiments to confirm whether they can convey a system's confidence level to users in the same way as the other types of ASEs. The results clearly showed that vibrational ASEs were able to accurately and intuitively convey the designed confidence level to participants, demonstrating that ASEs can be applied in a variety of applications in real environments. In this paper, we proposed a novel type of ASE that uses vibration, called vibrational ASEs. We implemented the vibrational ASEs on a smartphone and conducted experiments to confirm whether they can convey a system’s confidence level to users in the same way as the other types of ASEs. The results clearly showed that vibrational ASEs were able to accurately and intuitively convey the designed confidence level to the participants. On the basis of the results of this study, we found a wide variety of future research directions. First, we would like to investigate the effectiveness of combining various kinds of main protocols and different ASEs because this study was focused on only the combination of verbal suggestions and vibrational ASEs. We can currently use various kinds of ASEs, e.g., auditory, visual, motion, and vibrational ones, so considering appropriate combinations of various kinds of main protocols and different ASEs would expand the applicability of the ASEs. Second, we are also considering expanding the variety of expressions of vibrational ASEs. Specifically, we are considering using “increasing ASEs” as a new variation of ASE and different transition patterns for increasing or decreasing ASEs in linear or logarithmic ways. Based on former studies of vibration information in human-computer interaction [26,27], various on/off patterns or different frequencies of vibrations would be possible candidates. These new expressions could convey a system’s confidence levels in much more detail or other kinds of information to users. In parallel with the above rather fundamental investigation, we are also curious about concrete applications of vibrational ASEs. For example, the vibrations could be implemented not only in smart devices but in the handles or levers that users are holding. This means that vibrational ASEs can be applied for most driving or operating consoles, e.g., cars, trains, planes, heavy machines, and cranes. Considering how to apply vibrational ASEs into these realistic applications would be our next target, e.g., a car navigation system expressing decreasing vibrational ASEs together with speech like “This route A would be faster than route B if there was no snow on it” in a cold morning in winter. In this study, we focused on a situation in which a user is holding a smartphone. We are now curious about much more ubiquitous situations, like when a user is wearing smart glasses and a smart watch and holding a smartphone. In this situation, there would be a suitable information modality for each device, e.g., visual ASEs are good for glasses, so it is very exciting to consider how to apply various kinds of ASEs in such situations.
"
Investigating the Effect of the Multiple Comparisons Problem in Visual Analysis,https://dl.acm.org/authorize?N657280,"
The goal of a visualization system is to facilitate dataset-driven insight discovery. But what if the insights are spurious? Features or patterns in visualizations can be perceived as relevant insights, even though they may arise from noise. We often compare visualizations to a mental image of what we are interested in: a particular trend, distribution or an unusual pattern. As more visualizations are examined and more comparisons are made, the probability of discovering spurious insights increases. This problem is well-known in Statistics as the multiple comparisons problem (MCP) but overlooked in visual analysis. We present a way to evaluate MCP in visualization tools by measuring the accuracy of user reported insights on synthetic datasets with known ground truth labels. In our experiment, over 60% of user insights were false. We show how a confirmatory analysis approach that accounts for all visual comparisons, insights and non-insights, can achieve similar results as one that requires a validation dataset. Comparing a visualization to a mental image is akin to performing a statistical test, thus repeated interpretation of visualizations is susceptible to the MCP. In this work we attempted to empirically characterize this. We presented an experiment based on synthetically generated datasets that enabled us to assess the correctness of user reported insigths. We showed that by not accounting for all visual comparisons made during visual data exploration, false discovery rates will be inflated even after validating user insights with further statistical testing. We demonstrated that a confirmatory approach that addresses this can provide similar statistical guarentees to one that uses a validation dataset.

"
Passenger Trip Planning using Ride-Sharing Services,https://dl.acm.org/authorize?N657281,"
Ride-sharing can potentially address transportation challenges such as traffic congestion and air pollution by letting drivers share their cars unused capacity with a number of passengers. However, even though multiple ride-sharing services exist and HCI research has investigated various aspects of their use, we still have limited knowledge on how passengers use ride-sharing services to plan their trips. In this paper, we study how passengers use existing services to support the activity of planning a trip. We report from a qualitative study where we participated in 5 rides and conducted interviews with 19 passengers about their use and opinions towards ride-sharing services. We found that planning a ride involves comparing individual preferences across a number of services which enabled participants to support finding a trip and handle challenges such as privacy and trust. Further, we discuss these findings and their implications for future HCI research in ride-sharing. In this paper, we presented an empirical study of 19 passengers of ride-sharing and their use of ride-sharing services to plan their trips. Through analysis of a mixedmethods study with 5 ride participations, 5 planning observations and 10 semi-structured interviews we identified six themes that describe how ride-sharing passengers are planning their trips and which preferences they have for a specific ride. Our findings describe how our participants didn't only use individual services to plan their rides. Surprisingly, they planned their rides across a number of ride-sharing services which enabled them to handle some of the challenges they had in planning their rides such as finding a ride with the right price and with a trusted driver. To inspire further research in HCI with ride-sharing, we have discussed our findings under three headings of facilitating detour and partial rides, supporting passenger trust and privacy, and interaction with multiple services. We have discussed the implications of our findings and suggested that HCI research consider multi-device interaction as an important addition to ride-sharing services. While we believe that our study provides insights into how passengers use ride-sharing platforms, we also acknowledge that driver experiences would be valuable to provide different perspectives on topics such as multi-device interaction. We therefore suggest further work investigating these dynamics, for example, how drivers and passengers engage in collaborative ride-sharing.
"
Infrastructuring the Solidarity Economy: Unpacking Strategies and Tactics in Designing Social Innovation,https://dl.acm.org/authorize?N657282,"
Solidarity organizations in Europe are committed to building a more socially just society through a better configuration of democracy, politics and economy. In this paper, we describe our efforts to contribute to the socio-political designed innovation of solidarity movements through the establishment of a research lab embedded in, and operating within, the solidarity economy. We describe three cases that span the polarities of everyday and expert design, and contribute to the scaling out of social innovations. We use these cases to exemplify the strategies and tactics that emerge from the ongoing negotiation of 'infrastructuring' work with solidarity organizations. Finally, we discuss how guerilla infrastructuring, designing coalitions, and spanning design polarities can contribute to HCI and design for social innovation more generally. In this work, we conceptualized SMs as spaces of designed social innovation in which designers of various competences creatively action their collective design capacities. Crucially, we emphasized the necessary negotiations of diverse strategies for the creation of common tactics that allow such innovations to materialize. Through OLA, we reported on our own negotiations for engaging in technologically infrastructuring such alternative political and economic realities. In the discussion of this paper we call researchers to mirror the innovation of SMs and creatively action our own design capacities by engaging in guerilla infrastructuring, creating design coalitions and spanning design polarities, for an HCI concerned with radically alternative, more socially just societies and economies.
"
Exploring Co-design with Breastfeeding Mothers,https://dl.acm.org/authorize?N657283,"
Designing mobile applications for breastfeeding mothers can be challenging; creating spaces to foster co-design -- when a mother's primary focus is on her child rather than on design activities - is even more so. In this paper we discuss the development of the Milk Matters mobile application, a tool developed to motivate women to donate their surplus breast milk to the local milk bank. We look at the importance of different approaches to understanding the mothers, comparing workshops, surveys, and cultural probes. Through our work we identify three factors to consider when co-designing with and for mothers: 1) interrupted interactions 2) elements that might distract a baby and 3) the importance of empowering mothers through positive reinforcement. Based on these factors we examine our methodological approaches, suggesting ways to make future research with breastfeeding mothers more productive. In this paper we used a co-design approach to elicit the requirements and needed features for an application used by breastfeeding mothers. Design considerations included: 1) interrupted interactions 2) elements that might distract a baby and 3) the importance of empowering mothers through positive reinforcement. This approach has been especially important in this context, where the constraints of nursing young children affect how mothers interact with their phones and with Milk Matters, as well as their participation and availability for this project. Thus far the mobile application has met the expectations of both Milk Matters and their donor mothers, based on the predominantly positive feedback we received. Co-design as an approach seeks to bring to the surface some of the design constraints and considerations that might be obvious to the end users (e.g. breastfeeding mothers), but would not be apparent to the average software designer or HCI researcher. These non-functional requirements can make the difference between applications with very little adoption and those that are actually taken up and used by the target population. While it is too soon to establish whether this approach has succeeded, our initial surveys and cultural probes indicate that we are on the right track.

"
The Dream is Collapsing: The Experience of Exiting VR,https://dl.acm.org/authorize?N657284,"
Research on virtual reality (VR) has studied users' experience of immersion, presence, simulator sickness, and learning effects. However, the momentary experience of exiting VR and transitioning back to the real-world is not well understood. Do users become self-conscious of their actions upon exit? Are users nervous of their surroundings? Using explicitation interviews, we explore the moment of exit from VR across four applications. Analysis of the interviews reveals five components of experience: space, control, sociality, time, and sensory adaptation. Participants described spatial disorientation, for example, regardless of the complexity of the VR scene. Participants also described a window across which they exit VR, for example mentally first and then physically. We present six designs for easing or heightening the exit experience, as described by the participants. Based on these findings, we further discuss the ?moment of exit' as an opportunity for designing engaging and enhanced VR experiences. We explored the moment of exit in VR. Other aspects of the experience of VR are well understood, from presence to the aftereffects of VR. However, the momentary experience of exiting VR is not well understood and may yet represent a new experiential space that can be designed for. We explored the moment of exit across four different VR experiences. Twenty-four participants described aspects of spatial awareness, control, sociality, sensory adaptations and details of the precise temporal moment of exit. Participants also described changes to VR (both within the virtual and the real environments) that may both heighten and lessen the experience of exiting VR and speculated as to the experience of these. With a better understanding of the moment of exit in VR, it may become possible for the experience of VR to be extended beyond the view within the headset. Some research is exploring this already, with haptic extensions [3] for example. However, with a clearer understanding we can design transitions that heighten scary experiences, facilitate spatial security through gradual exits, or better control the users’ lasting impression of the experience.

"
Common Barriers to the Use of Patient-Generated Data Across Clinical Settings,https://dl.acm.org/authorize?N657295,"
Patient-generated data, such as data from wearable fitness trackers and smartphone apps, are viewed as a valuable information source towards personalised healthcare. However, studies in specific clinical settings have revealed diverse barriers to their effective use. In this paper, we address the following question: are there barriers prevalent across distinct workflows in clinical settings to using patient-generated data? We conducted a two-part investigation: a literature review of studies identifying such barriers; and interviews with clinical specialists across multiple roles, including emergency care, cardiology, mental health, and general practice. We identify common barriers in a six-stage workflow model of aligning patient and clinician objectives, judging data quality, evaluating data utility, rearranging data into a clinical format, interpreting data, and deciding on a plan or action. This workflow establishes common ground for HCI practitioners and researchers to explore solutions to improving the use of patient-generated data in clinical practices. This paper contributes an understanding of barriers to the use of patient-generated data in clinical settings, derived from a synthesis of existing literature and interviews conducted with thirteen healthcare professionals from several common clinical roles. Our findings suggest that, while the specific challenges pertaining to the use of patient-generated data vary considerably across clinical settings, these barriers occur along stages of a common workflow. We thus proposed a six-stage workflow model of patient-generated data use, which includes stages relating to data capture, quality, utility, structure, interpretation, and finally application in a plan of action. Based on this model, we discussed potential ways that these barriers might be addressed through the design of tools for improved data capture (to support later clinical use), improved interpretability by clinicians, and support for joint sense-making with patients. Finally, we discussed the role of the increased use of patient-generated data in the shift towards participatory care, in particular the need to consider changes in clinical workflows and IT systems. An important outcome of the use of patient-generated data in clinical settings is the increased collaboration between doctor and patient in managing care. This has the twin benefits of reducing patient dependence on the clinician, thereby empowering the patient to improve their health and wellbeing. We do not claim that our findings are sufficient for understanding precise workflows in individual clinical settings, but the broad range of clinical settings does afford an understanding of the broader use of patient-generated data. By providing future HCI research with pathways to address these barriers, our findings can engender improved collaboration between patient and clinician in decision-making, as well as improved clinical outcomes.
"
Analyzing the Effect of Avatar Self-Similarity on Men and Women in a Search and Rescue Game,https://dl.acm.org/authorize?N657296,"
A crucial aspect of virtual gaming experiences is the avatar: the player's virtual self-representation. While research has demonstrated benefits to using self-similar avatars in some virtual experiences, such avatars sometimes produce a more negative experience for women. To help researchers and game designers assess the cost-benefit tradeoffs of self-similar avatars, we compared players' performance and subjective experience in a search and rescue computer game when using two different photorealistic avatars: their own self or a friend, and when playing either a social (rescuing people) or a nonsocial (rescuing gems) version of the game. There was no effect of avatar appearance on players' performance or subjective experience in either game version, but we also found that women's experience with self-similar avatars was no more negative than men's. Our results suggest that avatar appearance may not make a difference to players in certain game contexts. In this work, we present findings from a study comparing performance and subjective experience in a search and rescue game where participants played as an avatar that looked like either themselves or a friend. We predicted that using a self-similar avatar would improve players’ performance and subjective experience, especially in a version of the game with a social element (rescuing people instead of gems). In addition, we predicted that males would have a more positive experience when playing as a self-similar avatar than females. We found no effect of avatar appearance on performance or experience, even in the version of the game with a social element, suggesting that avatar appearance may not matter much to players in games like ours that do not include a substantial amount of direct social interaction, However, we also found no difference between the subjective experience of males and females when playing as a self-similar avatar, indicating that the negative effects of self-similar avatars for females observed in previous work may simply be due to the use of lower fidelity avatars that do a poor job of representing physical features that matter most to female players. These findings demonstrate the importance of carefully considering the extent to which high fidelity self-similar avatars align with the purpose and structure of an interactive experience before deciding whether it is worth the investment of time and money to implement them.

"
Supporting Rhythm Activities of Deaf Children using Music-Sensory-Substitution Systems,https://dl.acm.org/authorize?N657297,"
Rhythm is the first musical concept deaf people learn in music classes. However, hearing loss limits the amount of information that allows a deaf person to evaluate his or her performance and stay in sync with other musicians. In this paper, we investigated how a visual and vibrotactile music-sensory-substitution device, MuSS-Bits++, affects rhythm discrimination, reproduction, and expressivity of deaf people. We conducted a controlled study with 11 deaf children and found that most participants felt more confident wearing the device in vibration mode even when it did not objectively improve their accuracy. Furthermore, we studied how MuSS-Bits++ can be used in music classes at deaf schools and what challenges and opportunities arise in such a setting. Based on these studies, we discuss insights and future directions that support the design and development of music-sensory-substitution systems for music making. In this paper, we investigated how MuSS-Bits++, a musicsensory-substitution systems for music-making, affects rhythm perception. We first interviewed a music teacher from a deaf school and identified that communicating rhythm information is a challenging task. In a controlled study, we found that participants with musical background perceived MuSS-Bits++’s vibrations as making the task easier, though no significant difference in their performance was observed. We argued that this could be due to two reasons: (1) they need time to learn how to use the feedback from MuSS-Bits++, or (2) the mapping we provided is not matching the mapping they need to improve their accuracy. Furthermore, in music lessons that were conducted by a music teacher from a deaf school, the teacher observed, and the participants agreed, that it was easier for the participants to understand the music teacher’s instructions. Furthermore, collaborative synchronization activities among three participants with the music teacher were observed to work better with MuSS-Bits++ compared to the same activity without MuSS-Bits++. We believe that music-sensorysubstitution systems hold a lot of potential for music-making and hope that this work will help other researchers to develop our vision further.

"
Traces: Studying a Public Reactive Floor-Projection of Walking Trajectories to Support Social Awareness,https://dl.acm.org/authorize?N657298,"
Walking trajectories have been used to understand how users interact with public displays. However, it has not yet been studied how displaying them in-situ could affect users' awareness about others' presence and activities. We present the study of an interactive public floor-projection called Traces. Traces projects the walking trajectories of individuals as they pass through the lobby of a university building. We investigated Traces through a 6 week in-field study. Our results outline how different uses and understandings of Traces contributed towards its appropriation as a glanceable display for social awareness. We outline design suggestions that future designers should consider to support social awareness with public displays. Our study of Traces has explored how social awareness and feelings of connection within a community can be enhanced by in-situ visualisation of low-level human activity. By supporting multiple different ways to interact, use of Traces was sustained over the study. This allowed rich appropriation, but without the display dominating the main use of the space. Traces allowed individuals to explore activities and social practices of the Design Factory, and appropriate a range of personal uses from relatively basic data. These, enhancing curiosity and engaging a connection with the lived environment and others in it.

"
Exploring Accessible Smartwatch Interactions for People with Upper Body Motor Impairments,https://dl.acm.org/authorize?N657299,"
Smartwatches are always-available, provide quick access to information in a mobile setting, and can collect continuous health and fitness data. However, the small interaction space of these wearables may pose challenges for people with upper body motor impairments. To investigate accessible smartwatch interactions for this user group, we conducted two studies. First, we assessed the accessibility of existing smartwatch gestures with 10 participants with motor impairments. We found that not all participants were able to complete button, swipe and tap interactions. In a second study, we adopted a participatory approach to explore smartwatch gesture preferences and to gain insight into alternative, more accessible smartwatch interaction techniques. Eleven participants with motor impairments created gestures for 16 common smartwatch actions on both touchscreen and non-touchscreen (bezel, wristband) areas of the watch and the user's body. We present results from both studies and provide design recommendations. We presented two studies to explore accessible smartwatch interactions for people with upper body motor impairments. In the first study, we assessed the accessibility of existing smartwatch interactions and found that participants experienced challenges performing existing gestures, including taps. In the second study, we explored alternative smartwatch interactions by asking participants to elicit gestures on the touchscreen and non-touchscreen areas for 16 common smartwatch actions. We found that the physical abilities of the participants influenced location preferences, such as the desire to choose non-touchscreen locations close to dominant hand, and that the small touchscreen size created the need to explore alternative gestures for some standard actions (e.g., pinch-to-zoom). Lastly, we presented design guidelines for more accessible smartwatch input.
"
Pseudonymous Parents: Comparing Parenting Roles and Identities on the Mommit and Daddit Subreddits,https://dl.acm.org/authorize?N657290,"
Gender equality between mothers and fathers is critical for the social and economic wellbeing of children, mothers, and families. Over the past 50 years, gender roles have begun to converge, with mothers doing more work outside of the home and fathers doing more domestic work. However, popular parenting sites in the U.S. continue to be heavily gendered. We explore parenting roles and identities on the platform Reddit.com which is used by both mothers and fathers. We draw on seven years of data from three major parenting subreddits-Parenting, Mommit, and Daddit-to investigate what topics parents discuss on Reddit and how they vary across parenting subreddits. We find some similarities in topics across the three boards, such as sleep training, as well as differences, such as fathers talking about custody cases and Halloween. We discuss the role of pseudonymity for providing parents with a platform to discuss sensitive parenting topics. We conclude by highlighting the benefits of both gender-inclusive and role-specific parenting boards. This work provides a roadmap for using computational techniques to understand parenting practices online at large scale.
"
Experiential Augmentation: Uncovering The Meaning of Qualitative Visualizations when Applied to Augmented Objects,https://dl.acm.org/authorize?N657291,"
As we move toward commercial usage of ubiquitous computing and augmented reality, it is important to think about how computing should communicate with us when it is distributed in our environment. This paper proposes that qualitative indexical visualizations based on learned understanding of physical phenomena (Experiential Augmentation) can enhance our interaction design language and aid digital interfaces in communicating in a real-world context. We present a study that gathers data on how participants interpret such visualizations, and propose a model with which to analyze their responses. Finally, we also give a set of design recommendations for those interested in creating similar augmentations.
"
Enabling the Participation of People with Parkinson's and their Caregivers in Co-Inquiry around Collectivist Health Technologies,https://dl.acm.org/authorize?N657292,"
While user participation is central to HCI, co-inquiry takes this further by having participants direct and control research from conceptualisation to completion. We describe a co-inquiry, conducted over 16 months with a Parkinson's support group. We explored how the participation of members might be enabled across multiple stages of a research project, from the generation of research questions to the development of a prototype. Participants directed the research into developing alternative modes of information provision, resulting in 'Parkinson's Radio' -- a collectivist health information service produced and edited by members of the support group. We reflect on how we supported participation at different stages of the project and the successes and challenges faced by the team. We contribute insights into the design of collectivist health technologies for this group, and discuss opportunities and tensions for conducting co-inquiry in HCI research. In this paper we described a co-inquiry conducted in collaboration with members of a Parkinson’s support group. We outlined the generation of a research challenge, which focused on creating accessible, offline information resources that allow people to share experiences of Parkinson’s and ask questions surrounding a range of topics. Our reflections provide unique insights into the challenges and successes that co-inquiry can elicit. Further work is required to explore how systems might support the creation of community information services in the future. The fact that participants were not involved in the data collection or analysis during the project is a limitation of this work, yet a reality of working with participants who might be prone to fatigue or ill health. However, future work of this kind could consider lightweight methods for enhancing engagement within these types of research activities. There is a possibility that encouraging participants to take and share notes during and between workshops could be an approach to take in the future.
"
"Who Provides Phishing Training?: Facts, Stories, and People Like Me",https://dl.acm.org/authorize?N657293,"
Humans represent one of the most persistent vulnerabilities in many computing systems. Since human users are independent agents who make their own choices, closing these vulnerabilities means persuading users to make different choices. Focusing on one specific human choice -- clicking on a link in a phishing email -- we conducted an experiment to identify better ways to train users to make more secure decisions. We compared traditional facts-and-advice training against training that uses a simple story to convey the same lessons. We found a surprising interaction effect: facts-and-advice training works better than not training users, but only when presented by a security expert. Stories don't work quite as well as facts-and-advice, but work much better when told by a peer. This suggests that the perceived origin of training materials can have a surprisingly large effect on security outcomes. Phishing remains one of the more widely used methods for exploiting human vulnerabilities today. Training users to recognize and avoid clicking on links in phishing emails is a large and important business today. We compared two major methods of conducting this training: providing facts-andadvice about phishing, or providing stories of previous victims of phishing. While both can contain the same lessons for end users, we found a surprising interaction effect: facts-andadvice led to lower click rates when appearing to come from an expert, but stories led to lower click rates when appearing to come from peers rather than experts. We discussed competing potential explanations for this interaction effect, but cannot concretely explain it
"
Unpacking Perceptions of Data-Driven Inferences Underlying Online Targeting and Personalization,https://dl.acm.org/authorize?N657294,"
Much of what a user sees browsing the internet, from ads to search results, is targeted or personalized by algorithms that have made inferences about that user. Prior work has documented that users find such targeting simultaneously useful and creepy. We begin unpacking these conflicted feelings through two online studies. In the first study, 306 participants saw one of ten explanations for why they received an ad, reflecting prevalent methods of targeting based on demographics, interests, and other factors. The type of interest-based targeting described in the explanation affected participants' comfort with the targeting and perceptions of its usefulness. We conducted a follow-up study in which 237 participants saw ten interests companies might infer. Both the sensitivity of the interest category and participants' actual interest in that topic significantly impacted their attitudes toward inferencing. Our results inform the design of transparency tools. We believe our results will be valuable in designing the next generation of transparency tools and setting best practices for inferencing. Participants in Study 1 were more comfortable with targeting based on all site visitors’ aggregate interests, rather than interests inferred about them in particular. Assuming user-specific targeting continues, these results suggest the need for clearer notice about such practices. Explanations about targeting on users’ own demographics or interests were considered more informative than the explanation which only mentioned an algorithm. That is, participants found the general explanation of an algorithm doing the personalization opaque. Several participants asked to know what information was being used in the algorithm and where it had been collected. We thus suggest that future transparency tools more clearly identify the method and parameters of targeting, not just that targeting is occurring. The power of big data rests in its ability to unearth hidden correlations buried in large amounts of data. This can lead to an advertisement being targeted based on interests that, to humans, seem unrelated to the ad, yet accurately capture the algorithm’s training data and its inherent biases. In contrast, our participants considered advertising through straightforward inferences to be more useful than advertising through inferences about unrelated interests. Participants also found explanations about straightforward inferences more informative. Many participants felt that the logical jump from an interest in one topic to an interest in an unrelated product necessitated further explanation or justification. Future research could investigate whether greater transparency about the steps from an inference to personalization improves perceptions of the usefulness of targeting or informativeness of privacy notices. We also compared reactions to an ad explicitly placed on a site and an ad targeted via an algorithm. Our participants found the first method more fair, but the second more useful, in large part because it seemed more likely to show relevant ads. This tension between fairness and usefulness echoes prior work [46]. In response, future transparency tools, both self-regulatory (e.g., AdChoices) and community-developed (e.g., browser plugins), could provide users with step-by-step explanations of inferences made about them and how those inferences are used for targeting. The particular interest categories on which an ad was targeted (rather than just that targeting was based on prior browsing) should be revealed. These recommendations contrast with widely used vague explanations. In Study 2, inference topics were not equal in participants’ eyes. The 160 Google AdWords interest topics that we presented to participants in Study 2 led to a gradient, not a bimodal distribution, of comfort, in contrast to companies’ policies that only grant special consideration to a small list of highly sensitive topics [17]. Future work should critically re-examine the bifurcation of topics into sensitive and nonsensitive categories, perhaps creating a targeting-privacy calculus that takes the inference’s sensitivity into account both when making an inference and targeting based on that topic. While some companies are open about what topics they use for personalization [18], our detailed results on topic sensitivity can also inform industry-wide best practices for making inferences about topics. The range of comfort ratings across inferences suggests that users may benefit from more fine-tuned controls over which inferences about them are used to target ads. Additionally, ad buyers should be informed of the sensitivity of different categories so they can make more informed purchasing decisions to avoid alienating customers [39, 41]. Echoing prior work [7], we found that the accuracy of an inferred interest also plays a major role in user comfort. Regardless of the sensitivity of the topic, participants were more comfortable with accurate inferences being used for personalizing their online experience. This finding harkens back to the idea of privacy distortion, which specifies that inaccurate information about an individual is as much of a privacy violation as accurate information. While some companies have created privacy dashboards, our findings about the importance of inferencing accuracy on user attitudes emphasizes the need for improved privacy dashboards and greater access. Participants also reported different levels of comfort with their interests being known by different social relations. Since ads may reflect a user’s private browsing history or sensitive interests [2], different ad settings could be available at work, on shared computers, and at certain times of day.

"
"Inaccuracy Blindness in Collaboration Persists, even with an Evaluation Prompt",https://dl.acm.org/authorize?N657205,"
The tendency to believe and act on others' misinformation is documented in much prior work. This paper focuses on inaccuracy blindness, the tendency to take a collaborator's poor information at face value, which reduces problem-solving success. We draw on social psychological research from the 1970s showing that evaluative rating scales can prompt a change in perspective. In a series of studies, we prototyped and tested an evaluation prompt meant to encourage skepticism in participant detectives trying to identify a serial killer. In tests of the prototype, the prompt was partially successful in inducing skepticism (Exp. 1), but a larger study (Exp. 2) showed that, despite the evaluation prompt, participants' inaccuracy blindness persisted. This work, and the literature more generally, shows that the tendency to be misled by collaborators' inaccurate information remains a strong phenomenon that is hard to counteract and remains a significant challenge for the CHI community. Online collaborations and workgroups that include strangers, revolving membership, partners or advisors with social credentials but perhaps unknown expertise, and work done over distributed time and space, are more prevalent than ever. These interactions in today’s world are unavoidable and their value is undeniable, but the risks of having collaborators who provide sloppy, false, misleading, or inaccurate information have increased as well. Although there are documented real world cases of the impact of poor collaboration (including collaboration failures that did not prevent the 911 attacks), and prior work has called out the problem of misinformation from others [9, 19] and identified the existence of inaccuracy blindness in small collaborations [13], the current work advances our understanding of inaccuracy blindness and contributes to the CHI community a new idea for remediation and controlled experimentation on the remedy. This work demonstrates inaccuracy blindness persisted despite a prompt that went well beyond a simple warning—it required the participants to attend to and evaluate their partner’s information. Thus we document the strength of inaccuracy blindness in collaborations and challenge our community to address this problem. Doing so at the level required might risk undermining collaboration ties, so it is a difficult challenge but would be one important step in solving the larger misinformation problem faced by our society.
"
Huggable: The Impact of Embodiment on Promoting Socio-emotional Interactions for Young Pediatric Inpatients,https://dl.acm.org/authorize?N657206,"
Most hospitals make efforts to provide socio-emotional support for patients and their families during care. In order to expand the service provided by certified child life specialists, we created a social robot and a virtual avatar that augment part of the care CCLS offers to patients by engaging pediatric patients in playful interactions and promoting their socio-emotional wellbeing. We ran a randomized controlled trial in a form of a Wizard-of-Oz study at a local pediatric hospital to study how three different interactive media (a plush teddy bear, a virtual agent on a screen, and a social robot) influence the pediatric patient's affect, joyful play, and social interactions with others. Behavioral analyses of verbal utterance transcriptions and children's physical behavior revealed that the social robot is most effective in producing socially energetic conversations as well as increasing positivity and promoting multi-party interactions. The virtual avatar was socially engaging but children tended to attend more exclusively to a virtual avatar and were less responsive to others. The plush toy was least engaging of the three interventions, but children touched it the most. Based on these findings, we recommend use cases for each agent appropriate for individual pediatric patients' health conditions and needs. These analyses of behavioral data suggest the benefit of deploying a physically embodied social robot in pediatric inpatient-care contexts on young patients'; social and emotional wellbeing. Many young children in a hospital suffer psychological distress from isolation and loneliness. The negative affect often leads to uncooperative behavior toward medical staff and poor health outcomes. In order to expand the service provided by certified child life specialists, we developed interactive technologies and ran an experimental study that investigates the application of three companion-like agents as part of pediatric in-patient care context. With the developed system, a randomized clinical study was conducted to study the impact of a social robot, a virtual avatar and a plush toy on social and emotional engagement between the patient, the child life specialist, and co-present families. The behavioral analyses of recorded video footage and verbal utterance transcriptions revealed that a social robot could facilitate socially energetic and positive conversations more effectively than a virtual character and a plush toy. Children’s gaze and touch behavior also support that a physically embodied social robot promotes the child-clinical staff-family member interactions better than a virtual avatar or a plush toy. These findings are significant because the increased positive emotion and social engagement are associated with positive patient outcomes. On the other hand, a virtual avatar received more exclusive attention from children, which could potentially benefit patients by distracting them from painful on-going medical procedures. Finally, children who do not feel well enough to engage in social interactions could gain comfort from physically engaging with the plush toy. Based on our results, we suggest that social robots could potentially play a significant role in improving young patients hospital experience by positively engaging children as well as reducing their feelings of isolation through various types of playful social interactions. These findings provide an important foundation to guide the ongoing development of effective pediatric-companion technologies for hospitalized children and their families to augment CCLS and to improve patient’s socio-emotional well-being.
"
Forte: User-Driven Generative Design,https://dl.acm.org/authorize?N657207,"
Low-cost fabrication machines (e.g., 3D printers) offer the promise of creating custom-designed objects by a range of users. To maximize performance, generative design methods such as topology optimization can automatically optimize properties of a design based on high-level specifications. Though promising, such methods require people to map their design ideas--often unintuitively--to a small number of mathematical input parameters, and the relationship between those parameters and a generated design is often unclear, making it difficult to iterate a design. We present Forte, a sketch-based, real-time interactive tool for people to directly express and iterate on their designs via 2D topology optimization. Users can ask the system to add structures, provide a variation with better performance, or optimize internal material layouts. Users can globally control how much to 'deviate' from the initial sketch, or perform local suggestive editing, which interactively prompts the system to update based on the new information. Design sessions with 10 participants demonstrate that Forte empowers designers to create and explore a range of optimized designs with custom forms and styles.

"
PolarTrack: Optical Outside-In Device Tracking that Exploits Display Polarization,https://dl.acm.org/authorize?N657208,"
PolarTrack is a novel camera-based approach to detecting and tracking mobile devices inside the capture volume. In PolarTrack, a polarization filter continuously rotates in front of an off-the-shelf color camera, which causes the displays of observed devices to periodically blink in the camera feed. The periodic blinking results from the physical characteristics of current displays, which shine polarized light either through an LC overlay to produce images or through a polarizer to reduce light reflections on OLED displays. PolarTrack runs a simple detection algorithm on the camera feed to segment displays and track their locations and orientations, which makes PolarTrack particularly suitable as a tracking system for cross-device interaction with mobile devices. Our evaluation of PolarTrack's tracking quality and comparison with state-of-the-art camera-based multi-device tracking showed a better tracking accuracy and precision with similar tracking reliability. PolarTrack works as standalone multi-device tracking but is also compatible with existing camera-based tracking systems and can complement them to compensate for their limitations. We have presented PolarTrack, a novel camera-based approach to detect and track mobile devices inside the capture volume by exploiting display polarization. We described the physical characteristic of current display technologies that result in blinking displays in a camera feed when a linear polarization filter rotates in front of a color camera. PolarTrack exploits this characteristic and uses a computationally inexpensive algorithm to segment displays from the background using differential images and a naïve weighted sliding window approach. We show how open source multi-device tracking pipelines can use the resulting binary images for device detection and multi-device tracking. We evaluated PolarTrack’s tracking quality, showing higher accuracy compared to a stateof-the-art open source system. Lastly, and to make PolarTrack reproducible, we provide a low-cost approach to building a PolarTrack system around a Kinect V2 camera from off-the-shelf components.

"
Revisiting “Hole in the Wall” Computing: Private Smart Speakers and Public Slum Settings,https://dl.acm.org/authorize?N657209,"
Millions of homes worldwide enjoy access to digital content and services through smart speakers such as Amazon's Echo and Google's Home. Promotional materials and users' own videos typically show homes that have many well-resourced rooms, with good power and data infrastructures. Over the last several years, we have been working with slum communities in India, whose dwellings are usually very compact (one or two rooms), personal home WiFi is almost unheard of, power infrastructures are far less robust, and financial resources put such smart speakers out of individual household reach. Inspired by the ""hole in the wall"" internet-kiosk programme, we carried out workshops with slum inhabitants to uncover issues and opportunities for providing a smart-speaker-type device in public areas and passageways. We designed and deployed a simple probe that allowed passers-by to ask and receive answers to questions. In this paper, we present the findings of this work, and a design space for such devices in these settings. Smart speaker appliances are fast becoming commonplace in homes worldwide. Among the many advantages of speech, its text-free nature potentially makes it especially useful for the lower-literate, such as the many so-called emergent users living in resource-constrained areas of the world. The often challenging nature of these environments, however, means that standard, in-home smart speakers—devices that require constant power and a reliable, high-bandwidth internet connection—are typically beyond reach for emergent users. The hole-in-the-wall computing concept has long been seen as a highly valuable resource for the demonstration, learning and awareness of screen-based technologies in emergent user communities. As a first step to identify whether this type of installation would be beneficial for speech-based technology, we conducted a Wizard-of-Oz investigation in two slum areas of Mumbai. We have described the promising results of this probe, and related our findings to the highly-regarded hole-inthe-wall literature. We have demonstrated the opportunities and values of conversational systems—imagined in the Western world for single family use in a domestic home—for public slum settings. Of course, we are not claiming the impact or longer-term insights of the hole-in-the-wall project. Rather, we argue that a key take-away from our work is that it is clearly worth investing the resources to create a deployable conversational speech system for the sorts of unsupervised learning and interaction benefits seen in that project. Our work provides evidence, then, that building publicly accessible conversational systems could lead to people experimenting with speech in a way that will not just be beneficial for any public “voice displays,” but also in promoting awareness and community learning about this new modality that individuals might then use on their own device as this becomes viable.

"
"Values, Identity, and Social Translucence: Neurodiverse Student Teams in Higher Education",https://dl.acm.org/authorize?N657200,"
To successfully function within a team, students must develop a range of skills for communication, organization, and conflict resolution. For students on the autism spectrum, these skills mirror the social, communicative, and cognitive experiences that can often be challenging for these learners. Since instructors and students collaborate using a mix of technology, we investigated the technology needs of neurodiverse teams comprised of autistic and non-autistic students. We interviewed seven autistic students and five employees of disability services in higher education. Our analysis focused on technology stakeholder values, stages of small-group development, and Social Translucence -- a model for online collaboration highlighting principles of visibility, awareness, and accountability. Despite motivation to succeed, neurodiverse students have difficulty expressing individual differences and addressing team conflict. To support future design of technology for neurodiverse teams, we propose: (1) a design space and design concepts including collaborative and affective computing tools, and (2) extending Social Translucence to account for student and group identities. Our research motivates the need for HCI researchers and designers to support development of more inclusive sociotechnical environments for teamwork. Throughout the team stages, successful team projects leverage team member strengths to form a cohesive team. Autistic students described ways that technology can act as a mediator to provide them support and structure in navigating the challenging environment of higher education. There is a need to protect privacy of these individuals, while supporting equity within the team. By incorporating the notion of identity into the design of socially translucent systems, technologies can give people control over disclosure and mechanisms to advocate for their needs in accessible, respectful, and discrete ways. Our tailored design space for teamwork can be used to explore more socially translucent ideas by considering stakeholder values as they are negotiated across team stages. Future research should prototype and evaluate team-based technologies for neurodiverse teams to refine our design space and design concepts. This advancement can help mediate interactions among teammates, peers, and instructors, and ultimately, support neurodiverse adults as they pursue their goals in higher education.

"
T-Cal: Understanding Team Conversational Data with Calendar-based Visualization,https://dl.acm.org/authorize?N657201,"
Understanding team communication and collaboration patterns is critical for improving work efficiency in organizations. This paper presents an interactive visualization system, T-Cal, that supports the analysis of conversation data from modern team messaging platforms (e.g., Slack). T-Cal employs a user-familiar visual interface, a calendar, to enable seamless multi-scale browsing of data from different perspectives. T-Cal also incorporates a number of analytical techniques for disentangling interleaving conversations, extracting keywords, and estimating sentiment. The design of T-Cal is based on an iterative user-centered design process including interview studies, requirements gathering, initial prototypes demonstration, and evaluation with domain users. The resulting two case studies indicate the effectiveness and usefulness of T-Cal in real-world applications, including daily conversations within an industry research lab and student group chats in a MOOC. We have presented T-Cal, an interactive visualization for assisting the analysis of team communication and collaboration. T-Cal displays the conversation data captured in team messaging platforms in multiple scales with a calendar-based visualization. We have also introduced ThreadPulse for showing conversational threads in multiple levels of detail. By involving expert users, we derived a set of design requirements and iteratively refined the system in a three-stage design process. For evaluation, we reported two case studies to illustrate the effectiveness and usefulness of T-Cal with realistic datasets in two different applications. In the future, we plan to pursue the comparative analysis of conversation data from two teams. Moreover, we want to focus on the event-based analysis in data, e.g., understanding when a problem was proposed, how it evolved, and when it was solved, by integrating other sources of data. Finally, we plan to further evaluate T-Cal in different application domains and conduct a long-term deployment study.
"
HapCube: A Wearable Tactile Device to Provide Tangential and Normal Pseudo-Force Feedback on a Fingertip,https://dl.acm.org/authorize?N657202,"
Haptic devices allow a more immersive experience with Virtual and Augmented Reality. However, for a wider range of usage they need to be miniaturized while maintaining the quality of haptic feedback. In this study, we used two kinds of human sensory illusion of vibration. The first illusion involves creating a virtual force (pulling sensation) using asymmetric vibration, and the second involves imparting compliances of complex stress-strain curves (i.e. force-displacement curves of mechanical keyboards) to a rigid object by changing the frequency and amplitude of vibration. Using these two illusions, we developed a wearable tactile device named HapCube, consisting of three orthogonal voicecoil actuators. Four measurement tests and four user tests confirmed that 1) a combination of two orthogonal asymmetric vibrations could provide a 2D virtual force in any tangential directions on a finger pad, and 2) a single voicecoil actuator produced pseudo-force feedback of the complex compliance curves in the normal direction. In this study, we showed that two asymmetric vibrations can be synthesized to generate two dimensional virtual forces in a plane. We also demonstrated that it is possible to provide distinctive normal feedback based on compliance curves with a small-sized voicecoil actuator. We hope that the knowledge will be a cornerstone for designing useful haptic device with vibration and human sensory illusion.

"
Keppi: A Tangible User Interface for Self-Reporting Pain,https://dl.acm.org/authorize?N657203,"
Motivated by the need to support those managing chronic pain, we report on the iterative design, development, and evaluation of Keppi, a novel pressure-based tangible user interface (TUI) for the self-report of pain intensity. In-lab studies with 28 participants found individuals were able to use Keppi to reliably report low, medium, and high pain as well as map squeeze pressure to pain level. Based on insights from these evaluations, we ultimately created a wearable version of Keppi with multiple form factors, including a necklace, bracelet, and keychain. Interviews indicated high receptivity to the wearable design, which satisfied additional user-identified needs (e.g., discreet and convenient) and highlighted key directions for the continued refinement of tangible devices for pain assessment. This paper reports on the development of Keppi, a novel pressure-based user input device for self-reporting scalar values — in this case, of pain intensity. Constructing three versions of Keppi to meet a variety of identified design considerations, hardware constraints, and user preferences, we illustrated the feasibility, reliability, and usability of our approach to support the momentary self-assessment of pain levels through an unobtrusive and natural tangible interaction. In doing so, we additionally identified a range of factors (e.g., from a user’s hand strength and pain type to general aesthetic and ergonomic issues) that can help guide others working in this space or on similar problems. Overall, our findings provide a number of implications for the continued development and evaluation of such self-assessment tools.
"
"Data, Data Everywhere, and Still Too Hard to Link: Insights from User Interactions with Diabetes Apps",https://dl.acm.org/authorize?N657204,"
For those with chronic conditions, such as Type 1 diabetes, smartphone apps offer the promise of an affordable, convenient, and personalized disease management tool. However, despite significant academic research and commercial development in this area, diabetes apps still show low adoption rates and underwhelming clinical outcomes. Through user-interaction sessions with 16 people with Type 1 diabetes, we provide evidence that commonly used interfaces for diabetes self-management apps, while providing certain benefits, can fail to explicitly address the cognitive and emotional requirements of users. From analysis of these sessions with eight such user interface designs, we report on user requirements, as well as interface benefits, limitations, and then discuss the implications of these findings. Finally, with the goal of improving these apps, we identify 3 questions for designers, and review for each in turn: current shortcomings, relevant approaches, exposed challenges, and potential solutions. Sessions with 16 users interacting with representative UI designs for diabetes self-help apps have been analyzed to see how well they meet users’ needs. We have drawn attention to two principal areas of failure: excessive cognitive demands on users to extract value; and the need for emotional sensitivity given the affective potential of these interactions. Cognitively, these apps require too much effort to make sense of data and locate meaningful insights, exposing users to visual confusion and cognitive overload. Emotionally, the complex relationship users have with their data appears inadequately considered. We have also proposed 3 questions for designers to advance these tools so that they can serve a more meaningful role in people’s lives. If the purpose of such apps is variously: to provide a digital tool for periodic troubleshooting of specific problems; recording diverse data for interaction with a health care provider; and to give the patient broad overviews of collected data; then one may consider these apps tolerably successful. Our participants were generally comfortable browsing through and understanding the significance of individual data entries, and in most instances, given a little time for close examination, could understand data within graphs and charts. Yet, as this study has illustrated, users’ day-to-day needs appear somewhat different. We have presented evidence from the literature that the majority of diabetes care is self-care, and that patients should be enabled to independently make frequent well-informed care decisions. Based on these premises, the current study gives evidence that current diabetes apps are inadequate for such goals. Given the number of apps based on a narrow range of interaction and UI paradigms, one must ask why so many app developers continue to deliver apps that fail to adequately address users’ problems, require significant daily effort to assemble representative data, show debatable improvements in outcomes, and have low adoption rates. While the desire to avoid medical regulation is a factor, perhaps it is also because they adhere to a model that is too closely tied to clinical requirements and conventions that focus on a mediated session, and thereby are ill suited to actual user requirements and expectations. We posit that this is not just a matter of adding new ways for patients to record more data, automation of data entry alone, more attractive color schemes, or even more visually appealing designs and interactions. Rather there is a need to reconsider how to help users draw value from real and often noisy diabetes data. Furthermore, there must be realistic assessment of available cognitive expenditure and emotional resilience given the contexts and frequency of usage. In summary, despite some tangible benefits from these UIs, we appear to have a widespread and repeated failure to understand user requirements combined with a lack of willingness to challenge established conventions. We suggest that the three posed questions should be answered so that we can move towards more effective and sensitive systems for health management.
"
Rewire: Interface Design Assistance from Examples,https://dl.acm.org/authorize?N657215,"
Interface designers often use screenshot images of example designs as building blocks for new designs. Since images are unstructured and hard to edit, designers typically reconstruct screenshots with vector graphics tools in order to reuse or edit parts of the design. Unfortunately, this reconstruction process is tedious and slow. We present Rewire, an interactive system that helps designers leverage example screenshots. Rewire automatically infers a vector representation of screenshots where each UI component is a separate object with editable shape and style properties. Based on this representation, the system provides three design assistance modes that help designers reuse or redraw components of the example design. The results from our quantitative and user evaluations demonstrate that Rewire can generate accurate vector representations of interface screenshots found in the wild and that design assistance enables users to reconstruct and edit example designs more efficiently compared to a baseline design tool. In this paper, we presented Rewire, a system that automatically infers a semantic vector-based representation of interface shapes from a pixel-based input screenshot. Rewire provides new forms of design assistance to ease the adaptation of example screenshots directly in designs. If designers can save time from recreating interface elements, they would potentially have more time to consider alternative designs, which would lead them to better final products [11]. We believe that systems like Rewire can enable us to explore new forms of intelligent design assistance enabling new possibilities in user interface design.

"
Uncertainty Visualization Influences how Humans Aggregate Discrepant Information,https://dl.acm.org/authorize?N657216,"
The number of sensors in our surroundings that provide the same information steadily increases. Since sensing is prone to errors, sensors may disagree. For example, a GPS-based tracker on the phone and a sensor on the bike wheel may provide discrepant estimates on traveled distance. This poses a user dilemma, namely how to reconcile the conflicting information into one estimate. We investigated whether visualizing the uncertainty associated with sensor measurements improves the quality of users' inference. We tested four visualizations with increasingly detailed representation of uncertainty. Our study repeatedly presented two sensor measurements with varying degrees of inconsistency to participants who indicated their best guess of the ""true"" value. We found that uncertainty information improves users' estimates, especially if sensors differ largely in their associated variability. Improvements were larger for information-rich visualizations. Based on our findings, we provide an interactive tool to select the optimal visualization for displaying conflicting information. In this paper, we conducted a study to understand how uncertainty visualizations influence humans’ choice of internal models of information aggregation. We showed that presenting uncertainty information improves users’ inference when the inconsistency between two measurements increases and when the measurements’ associated reliabilities are dissimilar. We recommend to use a point estimate visualization only for measurements with little inconsistency or non-diverging reliabilities. The higher the inconsistency or difference in reliability, the more humans benefit from uncertainty information. Based on the interpretation of our results, we recommend to use a dot plot to communicate uncertainty information for conflicting information. To better illustrate our results and make them easily explorable for designers and other researchers, we implemented and contribute an interactive tool that allows to visualize conflicting information. In addition to the visualizations, it shows the optimal combined value and a prediction of what users might actually choose as the combined value. We hope that our research will inspire research in other application areas of HCI to understand whether the results are applicable to communicating uncertain data in other contexts than sensor measurements. As we choose sensor measurements as a general scenario, we are confident that the results are applicable in other areas. In future work, we hope to extend the interactive tool to support more different types of visualizations. Further, we are interested in understanding how the difference between systematic and random error and larger inconsistencies than tested in this experiment influence humans’ inference.
"
"Content is King, Leadership Lags: Effects of Prior Experience on Newcomer Retention and Productivity in Online Production Groups",https://dl.acm.org/authorize?N657217,"
Organizers of online groups often struggle to recruit members who can most effectively carry out the group's activities and remain part of the group over time. In a study of a sample of 30,000 new editors belonging to 1,054 English WikiProjects, we empirically examine the effects of generalized prior work-productivity experience (measured by overall prior article edits), prior leadership experience (measured by overall prior project edits), and localized prior work-productivity experience (measured by pre-joining article edits on a project) on early retention and productivity. We find that (1)generalized prior work-productivity experience is positively associated with retention, but negatively associated with productivity (2) prior leadership experience is negatively associated with both retention and productivity, and (3) localized prior work-productivity experience is positively associated with both retention and productivity within that focal project. We then discuss implications to inform the designs of early interventions aimed at group success. In this study, we explore the effects of prior experience of new volunteers on their early retention and productivity in the group they join with the understanding that early identification of group failures can help community moderators intervene in a timely manner and craft the group for success. We found that certain kinds of prior experience have positive effects on newcomer retention and productivity whereas other kinds of prior experience have negative effects. Specifically, we carried out the study on a sample of 30,000 new editors to 1,054 WikiProjects, which are groups dedicated to building content around specific topic areas. This platform allowed us to measure prior experience in multiple dimensions and potential interactions between them which could generalize to other communities with similar structures. Also, WikiProjects have been ideal for such an exploration owing to their well-established and shared structures, shared membership and publicly available historical data about each volunteer. Through our analysis, we found that (i) generalized prior work-productivity experience (measured by overall prior article and article talk page edits) is positively associated with retention, but negatively associated with productivity within the focal group, (ii) prior leadership experience (measured by overall prior project and project talk page edits) is negatively associated with both retention and productivity within the focal group, and (iii) localized prior work-productivity experience (measured by pre-joining article edits on a focal group) is positively associated with both retention and productivity within the focal group.
"
Strategies for Engaging Communities in Creating Physical Civic Technologies,https://dl.acm.org/authorize?N657218,"
Despite widespread interest in civic technologies, empowering neighbourhoods to take advantage of these technologies in their local area remains challenging. This paper presents findings from the Ardler Inventors project, which aimed to understand how neighbourhoods can be supported in performing roles normally carried out by researchers and designers. We describe the end-to-end process of bringing people together around technology, designing and prototyping ideas, and ultimately testing several devices in their local area. Through this work, we explore different strategies for infrastructuring local residents' participation with technology, including the use of hackathon-like intensive design events and pre-designed kits for assembly. We contribute findings relating to the ability of these strategies to support building communities around civic technology and the challenges that must be addressed. Providing neighbourhoods with the means to best take advantage of new technologies in their local area is a difficult challenge. Through this research, we have prototyped a series of strategies for engaging neighbourhood residents in different stages of a design process: from gathering insights and prototyping ideas, through to fabricating refined prototypes and conducting deployments to gain feedback. Each of these approaches has shown promise in building the relationships, skills and enthusiasm needed to support future creativity in the neighbourhood. However, we also saw that some of our assumptions about the role of technology and creativity in neighbourhoods were misconceived. Further work must continue to explore how neighbourhoods can better take ownership not just of the technology, but of the processes that develop them.
"
Neuromechanics of a Button Press,https://dl.acm.org/authorize?N657219,"
To press a button, a finger must push down and pull up with the right force and timing. How the motor system succeeds in button-pressing, in spite of neural noise and lacking direct access to the mechanism of the button, is poorly understood. This paper investigates a unifying account based on neuromechanics. Mechanics is used to model muscles controlling the finger that contacts the button. Neurocognitive principles are used to model how the motor system learns appropriate muscle activations over repeated strokes though relying on degraded sensory feedback. Neuromechanical simulations yield a rich set of predictions for kinematics, dynamics, and user performance and may aid in understanding and improving input devices. We present a computational implementation and evaluate predictions for common button types. Neuromechanics views button-pressing as optimal control limited by physical, neural, and sensory bounds. In our study, we defined the motor control problem, proposed theoretical ideas on how the motor system solves them, implemented them computationally, and inspected predictions against empirical data. We sum up the findings thus: The model can – even with minimal fitting of parameters to data – predict some core characteristics of FD and DV patterns, as well as the use of force and the temporal precision for four distinct button types. The model fared less well in two respects: first, in failing to predict the ranges of kinematics variables for some buttons and, second, in failing to predict varied button-pressing strategies. Although much work remains to be done, the results are promising and support the optimality assumption. We conclude that neuromechanics deserves more attention as a rigorous and biologically motivated theory for the study of input methods.
"
Is it Happy?: Behavioural and Narrative Frame Complexity Impact Perceptions of a Simple Furry Robot's Emotions,https://dl.acm.org/authorize?N657210,"
Critical to social human-robot interaction is a robot's emotional richness, expressed within the parameters of its physical display. While emotion arousal is straightforward to convey, human valence (positivity) evaluations are famously ambiguous, whether we are assessing other humans or a robot. Imagine someone breathing raggedly: are they nervous, or excited? To assess the premise that irregular breathing connotes low valence (emotion negativity), we implemented different levels of breathing variability and complexity in simple furry robots. We asked 10 participants to watch and feel the behaviors, rate their valence, and explain their impressions. While a quantitative exploration of new and previous data showed correlation between multi-scale entropy and valence, the rich narratives revealed by thematic analysis of participant explanations call into question whether a single motion can, alone, be unambiguously valenced. Based on this evidence that people perceive robots as having inner lives, we recommend ways to build up narrative contexts over multiple interactions. We present further evidence that even a very simple robot can be capable of evocative emotion display and interaction. We presented evidence that the valence of a breathing-like behaviour can be determined through complexity measures such as MSE for valence. However, we have also presented evidence that simple dimensional measures for determining an emotion state do not capture the diversity and depth of people’s subjective experience in this kind of interaction. Valence seems especially vulnerable to differences in narrative frame. As such, we advocate emotion behaviour authoring considers narrative framing to help affective grounding [24].

"
Designing a Reclamation of Body and Health: Cancer Survivor Tattoos as Coping Ritual,https://dl.acm.org/authorize?N657211,"
Historically, tattoos have been perceived as a mark of deviant behavior from the perspective of Western medicine. However, cancer survivor tattoos are one of many strategies used to recover from the trauma of cancer diagnosis and treatment. In this study, we seek to understand the significance of these tattoos in the context of survivorship. We interviewed 19 cancer survivors about their survivor tattoos, exploring the benefits of designing, discussing, and displaying these tattoos as elements of emotional recovery post-cancer. We found that the act of designing a survivor tattoo facilitated all three elements of post-traumatic growth processes, including: (1) changed self-perception; (2) changed sense of relationships with others; and (3) changed philosophy of life. Through participants' lived experiences, we discuss information about emotions, health, and recovery encoded in tattoos, and provide implications for tools to help future cancer survivors recover from the trauma of diagnosis and treatment. There are as many ways to cope with cancer as there are cancer survivors. The range of behaviors related to cancer survivor tattoos is similarly varied. Survivor tattoos can be very private—a secret totem for the survivor to keep up their spirits or for a sense of protection. But survivor tattoos can also be a public signal, shared both offline and online in social situations, giving the survivor a chance to tell their story or advocate for cancer awareness. By interviewing survivors with tattoos, we were able to identify all three elements of post-traumatic growth, which were fostered and facilitated by the acts of designing, discussing, and displaying cancer survivor tattoos. In this study, we discuss three opportunities for HCI and health designers and researchers to enhance the benefits of post-trauma growth mechanisms supported by the “work” involved with survivor tattoos. First, by incorporating tattoos into the clinical record, designers can help legitimate the information encoded in survivor tattoos that convey aspects of changed self-perception after trauma. Second, digital tools to encourage trauma survivors to collaboratively design visual expressions of feelings and memories can facilitate the necessary changed sense of relationships with others, which is essential to post-traumatic growth. Finally, researchers should embrace creative means of engaging with artifacts designed by research participants, which can be used to elicit changed philosophy of life after trauma. These three design implications give HCI and health designers and researchers greater sensitivity in working with survivors of trauma, while enhancing trauma survivors’ engagement with essential elements on post-traumatic growth.
"
Mismatch of Expectations: How Modern Learning Resources Fail Conversational Programmers,https://dl.acm.org/authorize?N657212,"
Conversational programmers represent a class of learners who are not required to write any code, yet try to learn programming to improve their participation in technical conversations. We carried out interviews with 23 conversational programmers to better understand the challenges they face in technical conversations, what resources they choose to learn programming, how they perceive the learning process, and to what extent learning programming actually helps them. Among our key findings, we found that conversational programmers often did not know where to even begin the learning process and ended up using formal and informal learning resources that focus largely on programming syntax and logic. However, since the end goal of conversational programmers was not to build artifacts, modern learning resources usually failed these learners in their pursuits of improving their technical conversations. Our findings point to design opportunities in HCI to invent learner-centered approaches that address the needs of conversational programmers and help them establish common ground in technical conversations. In conclusion, we have contributed insights from conversational programmers across a wide range of job roles who experience challenges and try to learn programming to improve their conversations. In particular, we have described their learning approaches and struggles and highlighted six reasons why modern resources designed for traditional learners, such as CS students and professional programmers, are not appropriate for this learner population. We have also highlighted ways in which HCI can play a pivotal role in designing learning resources and interactions that are suitable not only for conversational programmers but also other members of society who are increasingly wanting to develop programming and technical literacy.
"
An Experience Sampling Study of User Reactions to Browser Warnings in the Field,https://dl.acm.org/authorize?N657213,"
Web browser warnings should help protect people from malware, phishing, and network attacks. Adhering to warnings keeps people safer online. Recent improvements in warning design have raised adherence rates, but they could still be higher. And prior work suggests many people still do not understand them. Thus, two challenges remain: increasing both comprehension and adherence rates. To dig deeper into user decision making and comprehension of warnings, we performed an experience sampling study of web browser security warnings, which involved surveying over 6,000 Chrome and Firefox users in situ to gather reasons for adhering or not to real warnings. We find these reasons are many and vary with context. Contrary to older prior work, we do not find a single dominant failure in modern warning design---like habituation---that prevents effective decisions. We conclude that further improvements to warnings will require solving a range of smaller contextual misunderstandings. We performed a large-scale study of web browser warning behavior using the Experience Sampling Method (ESM). In so doing, we observed that when encountering warning messages in situ, participants have a wide variety of reasons for choosing to adhere to or proceed past a given warning. Based on our qualitative data, we conclude that warnings have improved to the point that additional gains in adherence rates are likely only to be made by examining contextual factors and a wider variety of users’ concerns, rather than through one-size-fits-all improvements. Similarly, given that users make inconsistent decisions regarding whether to proceed or adhere to a warning, our results suggest that habituation plays a smaller role in user decision making than previously thought.
"
Extracting Design Guidelines for Wearables and Movement in Tabletop Role-Playing Games via a Research Through Design Process,https://dl.acm.org/authorize?N657214,"
We believe that wearables and movement are perfect fit for enhancing tabletop role-playing (TTRPG) experience, since they can provide embodied interaction, are perceived as character-costumes, enhance ludic properties and increase the connectedness to the imaginary game worlds. By providing these improvements, they can increase the immersiveness and player/character relationship which are critical for an ideal TTRPG experience. To investigate this underexplored area, we conducted an extensive research through design process which includes a (1) participatory design workshop with 25 participants, (2) preliminary user tests with Wizard-of-Oz and experience prototypes with 15 participants, (3) production of a new game system, wearable and tangible artifacts and (4) summative user tests for understanding the effects on experience with 16 participants. As a result of our study, we extracted design guidelines about how to integrate wearables and movement in narrative-based tabletop games and communicate how the results of each phase affected our artifacts. With this project, we propose the first example of the articulation of a research through design process for designing wearables and movement-based gameplay for TTRPG. For doing so, we undertook an extensive research design process involving 53 participants in the process and communicate the methods and the outcomes of our study in different phases. We also explained how these outcomes affected our design decisions while designing the WEARPG game system and our artifacts which are the Elemental Gauntlets and the Luck Stone. As a result of our study, players indicated that the wearables and movementbased gameplay increased their player/character relationship and immersion experiences. Therefore, we believe that researchers can replicate our design process to introduce new game systems or peripheral devices to computer augmented games and to other narrative-based games which require long term engagement. Other than that, as a result of our research process, we produced generalizable design guidelines that can be incorporated in the game design process. These guidelines can help designers in integrating new modalities to games, designing peripheral devices which will be used as supportive controllers and designing game mechanics for computer-augmented analog games.

"
Exploring the Design of Tailored Virtual Reality Experiences for People with Dementia,https://dl.acm.org/authorize?N657225,"
Despite indications that recreational virtual reality (VR) experiences could be beneficial for people with dementia, this area remains unexplored in contrast to the body of work on neurological rehabilitation through VR in dementia. With recreational VR applications coming to the market for dementia, we must consider how VR experiences for people with dementia can be sensitively designed to provide comfortable and enriching experiences. Working with seven participants from a local dementia care charity, we outline some of the opportunities and challenges inherent to the design and use of VR experiences with people with dementia and their carers through an inductive thematic analysis. We also provide a series of future directions for work in VR and dementia: 1) careful physical design, 2) making room for sharing, 3) utilizing all senses, 4) personalization, and 5) ensuring the active inclusion of the person with dementia. This paper has presented a study with 7 participants focusing on the design of novel virtual reality environments for people with dementia and their carers. Through iterative design workshops, including an extended engagement with a couple living with dementia, we discuss the design of three different VR environments. Our thematic analysis of textual data from these workshops has produced three themes: feeling foolish and feeling free, seeking to share new worlds, and blending the old with the new. We close with a discussion of five indications for future design in the area: 1) physical design of systems, 2) making room for sharing, 3) utilizing all senses, 4) personalization, and 5) positioning the person with dementia as an active participant. Finally, we call for design to occupy a new space in technologies for dementia – one which is not occupied by the past or focused on perceived cognitive deficits, but oriented towards the present and ready to make use of a full spectrum of interactivity.
"
Project Zanzibar: A Portable and Flexible Tangible Interaction Platform,https://dl.acm.org/authorize?N657226,"
We present Project Zanzibar: a flexible mat that can locate, uniquely identify and communicate with tangible objects placed on its surface, as well as sense a user's touch and hover hand gestures. We describe the underlying technical contributions: efficient and localised Near Field Communication (NFC) over a large surface area; object tracking combining NFC signal strength and capacitive footprint detection, and manufacturing techniques for a rollable device form-factor that enables portability, while providing a sizable interaction area when unrolled. In addition, we detail design patterns for tangibles of varying complexity and interactive capabilities, including the ability to sense orientation on the mat, harvest power, provide additional input and output, stack, or extend sensing outside the bounds of the mat. Capabilities and interaction modalities are illustrated with self-generated applications. Finally, we report on the experience of professional game developers building novel physical/digital experiences using the platform. We presented Zanzibar, a platform for tangible interaction. The Zanzibar Mat combines capacitive sensing and NFC communication in a novel way, which allows multitouch and hover gesture input to coexist with physical object manipulation and control. The hardware design of the Mat addresses the challenge of providing NFC coverage over a large area, in a scalable and efficient manner. Our tracking system fuses capacitive sensing data and NFC to provide robust tracking of objects. We documented our fabrication techniques to make the Mat rollable: a form-factor that makes the device portable, while providing a surface area large enough to support interaction with multiple tangible objects. Our implementation builds on the existing NFC ecosystem and extends it with custom NFC tag designs that can be used to build tangible objects of varying complexity and interactive capabilities. One characteristic of a useful platform is its ability to support varied and compelling applications. To this end, we have developed a set of scenarios that showcase different capabilities of the platform, illustrating the various interaction modalities it can support and highlighting how Zanzibar can be used together with existing computing devices (including augmented reality displays) to create a range of distinct physical/digital experiences. Our application examples have focused on play and education, two areas that we are passionate about and where the benefits of tangible interaction are well accepted. We have further validated our platform by deploying it with professional game developers, and learnt from their experience creating physical/digital game hybrids. We hope that our work motivates and enables others to continue to explore new ways to play, learn, and interact.
"
The RAD: Making Racing Games Equivalently Accessible to People Who Are Blind,https://dl.acm.org/authorize?N657227,"
We introduce the racing auditory display (RAD), an audio-based user interface that allows players who are blind to play the same types of racing games that sighted players can play with an efficiency and sense of control that are similar to what sighted players have. The RAD works with a standard pair of headphones and comprises two novel sonification techniques: the sound slider for understanding a car's speed and trajectory on a racetrack and the turn indicator system for alerting players of the direction, sharpness, length, and timing of upcoming turns. In a user study with 15 participants (3 blind; the rest blindfolded and analyzed separately), we found that players preferred the RAD's interface over that of Mach 1, a popular blind-accessible racing game. We also found that the RAD allows an avid gamer who is blind to race as well on a complex racetrack as casual sighted players can, without a significant difference between lap times or driving paths. This paper offers a vision of how video games can go beyond just being blind-accessible to being equivalently accessible to people who are blind, allowing them to play with a similar sense of control (intention) and efficiency as sighted players can. To this end, we introduce the racing auditory display (RAD) to help racing games become equivalently accessible to people who are blind. It comprises two novel sonification techniques: the sound slider for understanding a car’s speed and trajectory on a racetrack and the turn indicator system for alerting players of the direction, sharpness, length, and timing of upcoming turns. Through a pair of empirical studies, we found that players preferred the RAD’s interface over that of Mach 1, a popular blind-accessible racing game, and at times “felt like [they] had as much information as if [they] could see the track” (P1). We demonstrated that the RAD makes it possible for a gamer who is blind to race comparably to casual players using sight. Still, there are several limitations to our studies and to the RAD. First, our study included just four self-described gamers and three people who are blind, so our results cannot be assumed to apply to everyone from these groups. A more thorough followup study targeting gamers who are blind would be needed for this. Second, the RAD relies on 3D sound spatialization. Not everyone can hear spatialized sounds correctly with off-theshelf head-related transfer functions (HRTFs). Future games could allow players to load an HRTF from a profile so they can hear spatialized sound clearly in many different games. Last, the RAD is not as effective with non-gamers and does not teach them “video game literacy” such as how video game vehicle handling works, nor is it effective at helping players recover from crashes or from driving off the track. A future version of the RAD could include a Mach 1-style probing feature for helping players learn the game mechanics and recover from crashes. We also think it would be feasible to extend the RAD to incorporate other racing game elements such as opponent vehicles, boosts, item pickups, and shortcuts. We hope that just as user interface toolkits provide tools such as scrollbars, sliders, menus, and radio buttons that “just work” when software is published, game engines will one day include building blocks such as walls and track pieces that will “just work” with user interfaces such as the RAD or AudioGPS [17] when games are published to make all games blind-friendly.

"
Inclusive Computing in Special Needs Classrooms: Designing for All,https://dl.acm.org/authorize?N657228,"
With a growing call for an increased emphasis on computing in school curricula, there is a need to make computing accessible to a diversity of learners. One potential approach is to extend the use of physical toolkits, which have been found to encourage collaboration, sustained engagement and effective learning in classrooms in general. However, little is known as to whether and how these benefits can be leveraged in special needs schools, where learners have a spectrum of distinct cognitive and social needs. Here, we investigate how introducing a physical toolkit can support learning about computing concepts for special education needs (SEN) students in their classroom. By tracing how the students' interactions-both with the physical toolkit and with each other-unfolded over time, we demonstrate how the design of both the form factor and the learning tasks embedded in a physical toolkit contribute to collaboration, comprehension and engagement when learning in mixed SEN classrooms. There can be many challenges for supporting learning in SEN classrooms, especially for abstract topics like computing. Students often have a wider mix of abilities than their peers in mainstream school settings, and it can be difficult to structure learning tasks that simultaneously provide engaging and effective learning experiences for all. However, as our study has shown, the affordances of physical interfaces have much promise in SEN classrooms, especially when the design of the task type and supporting materials enable self-regulated, embodied learning with appropriate support from the instructors. If tasks are designed in this way, physical interfaces can enable students with a range of difficulties to leverage their abilities to collaborate and engage with curricular content, while fostering comprehension, enjoyment and a sense of self-accomplishment. There is much scope for designing new technologies to support more inclusive computing.
"
Caption Crawler: Enabling Reusable Alternative Text Descriptions using Reverse Image Search,https://dl.acm.org/authorize?N657229,"
Accessing images online is often difficult for users with vision impairments. This population relies on text descriptions of images that vary based on website authors' accessibility practices. Where one author might provide a descriptive caption for an image, another might provide no caption for the same image, leading to inconsistent experiences. In this work, we present the Caption Crawler system, which uses reverse image search to find existing captions on the web and make them accessible to a user's screen reader. We report our system's performance on a set of 481 websites from alexa.com's list of most popular sites to estimate caption coverage and latency, and also report blind and sighted users' ratings of our system's output quality. Finally, we conducted a user study with fourteen screen reader users to examine how the system might be used for personal browsing. In this work, we presented the Caption Crawler system, a real-time system for image captioning that relies on reverse image search to reuse pre-existing captions for the target image from other websites. We performed an automated crawl using the system over 481 web pages selected from alexa.com’s list of the most popular websites in order to obtain an idea of the caption coverage on common sites and characterize the performance of our system; we were able to provide alt text for about 12% of images that lacked it, with only 18 seconds’ latency on average. We also presented a study of caption preferences that found both sighted and blind users preferred hearing the longest available caption by default, and a lab study with fourteen blind users to further evaluate our system’s performance. Our findings indicate that users are receptive to the idea of quickly and automatically replacing missing and low-quality alt texts with pre-existing captions from elsewhere on the web, that pre-existing human-authored captions are preferred to current automated vision-to-language approaches, and that providing a queue of alternative possibilities was valued by end users as a way for them to learn more about an image and have increased confidence in caption accuracy. These performance and user-study findings suggest that our approach can play an important role in helping to address the issue of missing and poor-quality alt text online.

"
How Far Is Up?: Bringing the Counterpointed Triad Technique to Digital Storybook Apps,https://dl.acm.org/authorize?N657220,"
Interactive storybooks, such as those available on the iPad, offer multiple ways to convey a story, mostly through visual, textual and audio content. How to effectively deliver this combination of content so that it supports positive social and educational development in pre-literate children is relatively underexplored. In order to address this issue we introduce the ""Counterpointed Triad Technique"". Drawing from traditional literary theory we design visual, textual and audio content that each conveys different aspects of a story. We explore the use of this technique through a storybook we designed ourselves called ""How Far Is Up?"". A study involving 26 kindergarten children shows that ""How Far Is Up?"" can engage pre-literature children while they are reading alone and also when they are reading with an adult. Based on our craft knowledge and study findings, we present a set of design strategies that aim to provide designers with practical guidance on how to create engaging interactive digital storybooks. Interactive storybooks, such as those designed for the iPad, are continually popular with young children. These storybooks offer multiple ways to convey a story, however, there is the need for further research into how to design for both independent and shared reading of story apps. We introduced the Counterpointed Triad Technique. Derived from traditional literary theory we extended it to digital storybook design. In doing this we emphasize that visual, textual and audio content can each convey different aspects of a story. We present this as one possible approach that may be adopted by designers in order to support individual and shared reading of story apps. We explore the use of this technique through the design of How Far Is Up?. A study involving 26 kindergarten children suggests that How Far Is Up? can engage them alone and when reading with an adult. Based on our craft knowledge and study findings, we propose five design strategies that aim to provide designers with practical guidance on how to create engaging interactive storybooks. Our research provides designers with a technique that results in counterpointed app content. It operates as an alternative to current symmetrical design content that features redundancy within its content. Although our approach focuses on the design of apps that feature a story, we believe it may also be applicable to other interactive experiences that use other hardware, such as augmented reality where virtual and real-world story content could be counterpointed and other content forms, such as games, where interactive features direct players to counterpointed content. Our work represents a beginning towards a more comprehensive understanding of counterpointed design in order to support both independent and shared sessions, so that ultimately, more and more children as well as adults can profit from the benefits of engaging in both experiences.
"
Understanding the Accessibility of Smartphone Photography for People with Motor Impairments,https://dl.acm.org/authorize?N657221,"
We present the results of an exploration to understand the accessibility of smartphone photography for people with motor impairments. We surveyed forty-six people and interviewed twelve people about capturing, editing, and sharing photographs on smartphones. We found that people with motor impairments encounter many challenges with smartphone photography, resulting in users capturing fewer photographs than they would like. Participants described various strategies they used to overcome challenges in order to capture a quality photograph. We also found that photograph quality plays a large role in deciding which photographs users share and how often they share, with most participants rating their photographs as average or poor quality compared to photos shared on their social networks. Additionally, we created design probes of two novel photography interfaces and received feedback from our interview participants about their usefulness and functionality. Based on our findings, we propose design recommendations for how to improve the accessibility of mobile photoware for people with motor impairments. We have presented results of our exploration into understanding the accessibility of smartphone photography for people with motor impairments. We found that people with motor impairments experience many challenges during the photo capture process. As a result, they capture photos less often than they would like due to the time and physical effort required to capture a good photo. We also found that they share fewer photos than they would like because of perceived deficiencies in their photos. We introduced two accessible photography concepts, Pair Photography and Infrastructure Camera Control, and received feedback from participants about their functionality and usefulness. Finally, we presented design recommendations for how to improve the accessibility of smartphone photography for people with motor impairments. Our research shows that people with motor impairments do engage in smartphone photography, but the inaccessibility of mobile photoware deters users from enjoying the personal and social benefits smartphone photography offers. We end with a quote from P9 about the importance of smartphone photography in his life: “The photographs are for social media. Social media is for expanding your social network, which is something that people with a disability typically struggle with, the size or sphere of their social network, either because they don’t have mobility, they can’t get places or because the activities that they are doing just aren’t as widely varied as everybody else. I like to think about it more than just the accessibility of cameras.”

"
Ohmic-Touch: Extending Touch Interaction by Indirect Touch through Resistive Objects,https://dl.acm.org/authorize?N657222,"
When an object is interposed between a touch surface and a finger/touch pen, the change in impedance caused by the object can be measured by the driver software. This phenomenon has been used to develop new interaction techniques. Unlike previous works that focused on the capacitance component in impedance, Ohmic-Touch enhances touch input modality by sensing resistance. Using 3D printers or inkjet printers with conductive materials and off-the-shelf electronic components/sensors, resistance is easily and precisely controllable. We implement mechanisms on touch surfaces based on the electrical resistance of the object: for example, to sense the touching position on an interposed object, to identify each object, and to sense light, force, or temperature by using resistors and sensors. Additionally, we conduct experimental studies that demonstrate that our technology has a recognition accuracy of the resistance value of 97%. In this paper, we proposed a method called Ohmic-Touch that extends the modality of touch inputs using the electric resistance on commercial capacitive touch surfaces. OhmicTouch provides a series of interactions without requiring the touch object to be powered. As such, the proposed method allows for the use of inexpensive materials. Therefore, the proposed can be a practical technique for providing greater modality to inputs on touch surfaces. We believe this paper will contribute to advances in the HCI community as a basis for expanding input modalities for capacitive touch surfaces. In a future study, we will implement effective applications that utilize the features of this technique for mobile devices and its efficiency will be evaluated.

"
TeleHuman2: A Cylindrical Light Field Teleconferencing System for Life-size 3D Human Telepresence,https://dl.acm.org/authorize?N657223,"
For telepresence to support the richness of multiparty conversations, it is important to convey motion parallax and stereoscopy without head-worn apparatus. TeleHuman2 is a ""hologrammatic"" telepresence system that conveys full-body 3D video of interlocutors using a human-sized cylindrical light field display. For rendering, the system uses an array of projectors mounted above the heads of participants in a ring around a retroreflective cylinder. Unique angular renditions are calculated from streaming depth video captured at the remote location. Projected images are retro-reflected into the eyes of local participants, at 1.3º intervals providing angular renditions simultaneously for left and right eyes of all onlookers, which conveys motion parallax and stereoscopy without head-worn apparatus or head tracking. Our technical evaluation of the angular accuracy of the system demonstrates that the error in judging the angle of a remote arrow object represented in TeleHuman2 is within 1 degree, and not significantly different from similar judgments of a collocated arrow object. We presented TeleHuman2, a remote telepresence system that conveys full-sized 3D video of remote participants via a cylindrical light field display. For rendering, the system uses a large array of projectors mounted above the heads of participants in a ring around a human-sized horizontally retroreflective cylinder. Each projector has its own renderer calculating a viewport from a 3D relief map provided by the video capture system at the remote location. This capture system consists of an array of visible light stereoscopic ZED cameras connected to a PC. Locally, each projector image is retro-reflected back for each angle into the eyes of local participants, conveying correct continuous motion parallax and stereoscopy to multiple participants without any need for head-worn apparatus or head tracking. Our technical evaluation of angular accuracy of the system demonstrates that the error in judging the angle of an arrow object represented in TeleHuman2 is within 1 degree, and not significantly different from that of the real arrow object.
"
Co-designing Mobile Online Safety Applications with Children,https://dl.acm.org/authorize?N657224,"
Parents use mobile monitoring software to observe and restrict their children's activities in order to minimize the risks associated with Internet-enabled mobile devices. As children are stakeholders in such technologies, recent research has called for their inclusion in its design process. To investigate children's perceptions of parental mobile monitoring technologies and explore their interaction preferences, we held two co-design sessions with 12 children ages 7-12. Children first reviewed and redesigned an existing mobile monitoring application. Next, they designed ways children could use monitoring software when they encounter mobile risks (e.g., cyberbullying, inappropriate content). Results showed that children acknowledged safety needs and accepted certain parental controls. They preferred and designed controls that emphasized restriction over monitoring, taught risk coping, promoted parent-child communication, and automated interactions. Our results benefit designers looking to develop parental mobile monitoring technologies in ways that children will both accept and can actively benefit from. Our work envisioned new futures for and presents new perspectives on mobile monitoring technologies by working with children as co-designers. Results showed that, while children acknowledged mobile safety needs and accepted certain parental controls, they preferred technologies that emphasized restriction over monitoring, taught risk coping, promoted parent-child communication, and automated interactions. The expanded understanding of children’s desires for mobile monitoring technologies advances the goal of developing flexible tools that fit into family value systems. Our results benefit designers looking to develop parental mobile monitoring technologies in ways that children will both accept and can actively benefit from.
"
Intellingo: An Intelligible Translation Environment,https://dl.acm.org/authorize?N657235,"
Translation environments offer various translation aids to support professional translators. However, translation aids typically provide only limited justification for the translation suggestions they propose. In this paper we present Intellingo, a translation environment that explores intelligibility for translation aids, to enable more sensible usage of translation suggestions. We performed a comparative study between an intelligible version and a non-intelligible version of Intellingo. The results show that although adding intelligibility does not necessarily result in significant changes to the user experience, translators can better assess translation suggestions without a negative impact on their performance. Intelligibility is preferred by translators when the additional information it conveys benefits the translation process and when this information is not part of the translator's readily available knowledge. In this paper we presented Intellingo, an intelligible translation environment for professional translation work that includes various translation aids, such as term bases, translation memory and machine translation. In contrast with traditional translation environments, the user interface of Intellingo presents translation aids in a more intelligible manner in order to allow translators to use them in a more efficient way. It provides additional context about the quality and source of the suggestions, as well as subtle explanations why suggestions were made. Furthermore, Intellingo highlights how translation suggestions are related to the context of the ongoing translation. To investigate the impact of intelligible translation aids on the translation process, a user study was performed with professional translators. Participants were positive to very positive in their comments about both the simple version and the intelligible version of Intellingo. The results show that intelligibility does help professional translators to assess the quality of the generated suggestions and to understand how these suggestions can be used in translation, without distracting them or negatively impacting their efficiency. Surprisingly, the study showed that adding intelligibility in the user interface design did not result in significant changes to the user experience. Intelligible design of translation aids does not affect the quality of the translation suggestions themselves. However, the intelligible features inform translators quickly about the quality and context of the suggestions to support better decision making. Translators only prefer intelligible translation aids when the additional information they convey benefits the translation process, and when this information is not part of the translators readily available knowledge. Usage of intelligibility in design needs to be carefully balanced: providing more information and context might lead to a decrease in efficiency and a potential information overload. We showed various possibilities to add intelligibility to professional translation tools, and explored their impact on the user experience.

"
Thor's Hammer: An Ungrounded Force Feedback Device Utilizing Propeller-Induced Propulsive Force,https://dl.acm.org/authorize?N657236,"
We present a new handheld haptic device, Thor's Hammer, which uses propeller propulsion to generate ungrounded, 3-DOF force feedback. Thor's Hammer has six motors and propellers that generates strong thrusts of air without the need for physical grounding or heavy air compressors. With its location and orientation tracked by an optimal tracking system, the system can exert forces in arbitrary directions regardless of the device's orientation. Our technical evaluation shows that Thor's Hammer can apply up to 4 N of force in arbitrary directions with less than 0.11 N and 3.9° of average magnitude and orientation errors. We also present virtual reality applications that can benefit from the force feedback provided by Thor's Hammer. Using these applications, we conducted a preliminary user study and participants felt the experience more realistic and immersive with the force feedback. This paper introduced Thor’s Hammer, an ungrounded force feedback device that can create 3-DOF force feedback. The technical evaluation demonstrated that the device can generate up to 4 N of continuous and precise force feedback in arbitrary directions. The informal user study showed promise in the device, provided insights regarding how users experienced the force feedback, and identified areas for which the device could be improved. Together with the results from the technical evaluation and the user study, we believe that the new device may open up many possibilities in creating more realistic experiences in virtual environments. Due to the 3D printed nature of the device, and its use of off-the-shelf components, the device can be easily replicated and operated without any grounding structures. Since the use of propeller propulsion for 3-DOF force feedback had not been investigated prior to this work, our focus was to build a proof-of-concept device. The device currently has several limitations and future work should seek to address them. For instance, while there was no mention of fatigue in the user study, the device is larger and heavier than most VR controllers and a prolonged use could cause fatigue. High latency of the feedback also limits the use of the device to scenarios with gradually changing force feedback. Future work could also include an expansion to a 6-DOF force feedback device by using rotor configurations that support omnidirectional actuation [11] and a more controlled psychophysical study investigating how users perceive the force created by propeller propulsion compared to a grounded force feedback device and the effect of unintended haptic sensations such as vibration and torque on perception of the force.

"
Transforming Last-mile Logistics: Opportunities for more Sustainable Deliveries,https://dl.acm.org/authorize?N657237,"
Road congestion, air pollution and sustainability are increasingly important in major cities. We look to understand how last-mile deliveries in the parcel sector are impacting our roads. Using formative field work and quantitative analysis of consignment manifests and location data, we identify how the effectiveness of life-style couriers is contributing to both environmental and non-environmental externalities. This paper presents an analysis of delivery performances and practices in last-mile logistics in central London, quantifying the impacts differing levels of experience have on overall round efficiency. We identify eleven key opportunities for technological support for last-mile parcel deliveries that could contribute to both driver effectiveness and sustainability. We finish by examining how HCI can lead to improved environmental and social justice by re-considering and realizing future collaborative visions in last-mile logistics. In this paper we have introduced last-mile parcel deliveries and life-style couriers to HCI. Through our empirical study of this domain in central London, we have shown how studying variation in delivery effectiveness of drivers and understanding their practices and routines can help identify opportunities for designing technology to support more effective delivery of parcels. From our qualitative field study, we have illustrated the importance of driver knowledge, personal relationships, and decision making about when to walk and how to navigate multistory and multipurpose buildings all contribute to driver effectiveness. Exploiting this insight as a resource for design we have explored how HCI can lead to new technology to help utilize the curbside better for deliveries, encourage better walking strategies that promote consolidation and collaboration, and help challenge policy frameworks and unsustainability of logistics and life-style couriers. We have identified eleven opportunities for HCI from our empirical work to make novice drivers instantly better and help deliveries get to people (not postcodes) when delivering parcels in cities. We finish by, we hope, stimulating a new discussion in HCI about how we can help bring about new and future business models in last-mile logistics that can more profoundly intervene in the last-mile to promote greater environmental and social justice.
"
CraftML: 3D Modeling is Web Programming,https://dl.acm.org/authorize?N657238,"
We explore web programming as a new paradigm for programmatic 3D modeling. Most existing approaches subscribe to the imperative programming paradigm. While useful, there exists a gulf of evaluation between procedural steps and the intended structure. We present CraftML, a language providing a declarative syntax where the code is the structure. CraftML offers a rich set of programming features familiar to web developers of all skill levels, such as tags, hyperlinks, document object model, cascade style sheet, JQuery, string interpolation, template engine, data injection, and scalable vector graphics. We develop an online IDE to support CraftML development, with features such as live preview, search, module import, and parameterization. Using examples and case studies, we demonstrate that CraftML offers a low floor for beginners to make simple designs, a high ceiling for experts to build complex computational models, and wide walls to support many application domains such as education, data physicalization, tactile graphics, assistive devices, and mechanical components. In this work, we set out to explore the web programming paradigm as a viable alternative to the imperative programming paradigm for programmatic 3D modeling. We developed a new language, CraftML, that offers a rich set of programming constructs familiar to web developers, such as declarative syntax, semantic tags, CSS, Javascript, JQuery, string interpolation, and templates. We showed examples and case studies as supporting evidence for concluding that the web programming paradigm is indeed a viable and beneficial alternative.

"
ForceBoard: Subtle Text Entry Leveraging Pressure,https://dl.acm.org/authorize?N657239,"
We present ForceBoard, a pressure-based input technique that enables text entry by subtle finger motion. To enter text, users apply pressure to control a multi-letter-wide sliding cursor on a one-dimensional keyboard with alphabetical ordering, and confirm the selection with a quick release. We examined the error model of pressure control for successive and error-tolerant input, which was incorporated into a Bayesian algorithm to infer user input. A user study showed that, after a 10-minute training, the average text entry rate reached 4.2 wpm (Words Per Minute) for character-level input, and 11.0 wpm for word-level input. Users reported that ForceBoard was easy to learn and interesting to use. These results demonstrated the feasibility of applying pressure as the main channel for text entry. We conclude by discussing the limitation, as well as the potential of ForceBoard to support interaction with constraints from form factor, social concern and physical environments. In this paper, we present ForceBoard, which allows users to type text with pressure with subtle motion movement of the finger. This pushes the limits of motion amplitude required for text entry. We conducted a series of user studies to determine the design strategies, including keyboard layout, width of cursor, feedback design, interaction logic and so on. In particular, we examined users’ ability to control pressure in a fast and inaccurate fashion. The empirical results and established error model of pressure control complemented prior research on modeling a person’s ability of accurate pressure control. Based on the results, we adapted a widely used statistical decoding algorithm to interpret pressurebased text input. The results showed that after ten minutes of training, users could input 11 words per minute with ForceBoard. Meanwhile, subjective user feedback revealed that ForceBoard was easy to learn and interesting to use. We conclude by discussing potential applications of ForceBoard as well as its limitations.

"
PageFlip: Leveraging Page-Flipping Gestures for Efficient Command and Value Selection on Smartwatches,https://dl.acm.org/authorize?N657230,"
Selecting an item of interest on smartwatches can be tedious and time-consuming as it involves a series of swipe and tap actions. We present PageFlip, a novel method that combines into a single action multiple touch operations such as command invocation and value selection for efficient interaction on smartwatches. PageFlip operates with a page flip gesture that starts by dragging the UI from a corner of the device. We first design PageFlip by examining its key design factors such as corners, drag directions and drag distances. We next compare PageFlip to a functionally equivalent radial menu and a standard swipe and tap method. Results reveal that PageFlip improves efficiency for both discrete and continuous selection tasks. Finally, we demonstrate novel smartwatch interaction opportunities and a set of applications that can benefit from PageFlip. In this paper, we explored the design and performance of PageFlip, a technique that leverages corner-command mappings and supports command invocation and value selection in a single corner-drag action on smartwatches. We first examined the design parameters such as corners, angular segments and distance segments, and then compared the performance of PageFlip with a standard swipe-tap method and a functionally equivalent radial menu. The results indicated that PageFlip significantly increased the efficiency for both discrete and continuous tasks by combining multiple operations into a single action. Finally, we used three applications to demonstrate suitable uses of PageFlip for novel smartwatch interaction.

"
Inpher: Inferring Physical Properties of Virtual Objects from Mid-Air Interaction,https://dl.acm.org/authorize?N657231,"
We present Inpher, a virtual reality system for setting physical properties of virtual objects using mid-air interaction. Users simply grasp virtual objects and mimic their desired physical movement. The physical properties required to fulfill that movement will then be inferred directly from that motion. We provide a 3D user interface that does not require users to have an abstract model of physical properties. Our approach leverages users' real world experiences with physics. We conducted a bodystorming to investigate users' mental model of physics. Based on our iterative design process, we implemented techniques for inferring mass, bounciness and friction. We conducted a case study with 15 participants with varying levels of physics education. The results indicate that users are capable of demonstrating the required interactions and achieve satisfying results. We presented Inpher, a system for setting physical properties using mid-air interaction. The design is based on the ability of humans to express physical motion based on experience and intuition. Our case study showed that users with little physics background were able to train bounciness, relative mass and friction of virtual objects. The results indicate that it is in fact possible to infer physical properties from interactions with virtual objects. Our approach can be used as an integral part of content creation systems for novices. We envision many different applications, which can potentially incorporate our approach. Examples include level editors, puzzle games, interactive systems for education and more.

"
Making Core Memory: Design Inquiry into Gendered Legacies of Engineering and Craftwork,https://dl.acm.org/authorize?N657232,"
This paper describes the Making Core Memory project, a design inquiry into the invisible work that went into assem-bling core memory, an early form of computer information storage initially woven by hand. Drawing on feminist tradi-tions of situated knowing, we designed an electronic quilt and a series of participatory workshops that materialize the work of the core memory weavers. With this case we not only broaden dominant stories of design, but we also reflect on the entanglement of predominantly male, high status labor with the ostensibly low-status work of women's hands. By integrating design and archival research as a means of cultural analysis, we further expand conversations on design research methods within human-computer inter-action (HCI), using design to reveal legacies of practice elided by contemporary technology cultures. In doing so, this paper highlights for HCI scholars that worlds of hand-work and computing, or weaving and space travel, are not as separate as we might imagine them to be. By interrogating connections between textiles and engineering, and enlivening a forgotten legacy of woven software, our project brings important histories to HCI today. Here we suggest that HCI’s gendered visions of innovation in the past create absences about what we can know in the present. Doing this project meant facing the fact that perhaps every woman who could tell this story is now gone. And while the accounts of the engineers and astronauts are canonized through our achievements — our giant leap for all mankind — we may never know the experiences of the Little Old Ladies. Because we neglected to collect their stories in the past, we fail to know them in the present. Reviving their accounts informs our contemporary understanding of what innovation looks like and, in turn, shapes possibilities for building technology otherwise.
"
Augmenting Code with In Situ Visualizations to Aid Program Understanding,https://dl.acm.org/authorize?N657233,"
Programmers must draw explicit connections between their code and runtime state to properly assess the correctness of their programs. However, debugging tools often decouple the program state from the source code and require explicitly invoked views to bridge the rift between program editing and program understanding. To unobtrusively reveal runtime behavior during both normal execution and debugging, we contribute techniques for visualizing program variables directly within the source code. We describe a design space and placement criteria for embedded visualizations. We evaluate our in situ visualizations in an editor for the Vega visualization grammar. Compared to a baseline development environment, novice Vega users improve their overall task grade by about 2 points when using the in situ visualizations and exhibit significant positive effects on their self-reported speed and accuracy.

"
Pointing All Around You: Selection Performance of Mouse and Ray-Cast Pointing in Full-Coverage Displays,https://dl.acm.org/authorize?N657234,"
As display environments become larger and more diverse - now often encompassing multiple walls and room surfaces - it is becoming more common that users must find and manipulate digital artifacts not directly in front of them. There is little understanding, however, about what techniques and devices are best for carrying out basic operations above, behind, or to the side of the user. We conducted an empirical study comparing two main techniques that are suitable for full-coverage display environments: mouse-based pointing, and ray-cast 'laser' pointing. Participants completed search and pointing tasks on the walls and ceiling, and we measured completion time, path lengths and perceived effort. Our study showed a strong interaction between performance and target location: when the target position was not known a priori the mouse was fastest for targets on the front wall, but ray-casting was faster for targets behind the user. Our findings provide new empirical evidence that can help designers choose pointing techniques for full-coverage spaces. With the increasing viability and appeal of wide angle and fullcoverage display environments it has become more important to evaluate the available interface-design choices, to ensure that these new types of systems are usable and practical. One of the key user actions to support in these new environments is digital object selection and targeting. We carried out a study that explored the effects of two relevant pointing techniques in targeting tasks around the room and the ceiling. We found that a mouse-based technique provides the fastest targeting interaction when the targets do not require the participant to move their body, but a Ray-casting technique was superior for targets at larger angles. Additionally, we discovered that the Mouse technique has the advantage of enabling the cursor to be ""parked"" while the user looks elsewhere, and that the Ray-casting technique enables better overlap of searching and targeting tasks when the user needs to find an object of interest in the room. Our findings can help inform designers as they choose interaction techniques that best suit the intended environment, and subsequently support the success of interfaces that take advantage of the full physical environment for digital and augmented information in our future work and home spaces.

"
The Dark (Patterns) Side of UX Design,https://dl.acm.org/authorize?N657245,"
Interest in critical scholarship that engages with the complexity of user experience (UX) practice is rapidly expanding, yet the vocabulary for describing and assessing criticality in practice is currently lacking. In this paper, we outline and explore the limits of a specific ethical phenomenon known as ""dark patterns,"" where user value is supplanted in favor of shareholder value. We assembled a corpus of examples of practitioner-identified dark patterns and performed a content analysis to determine the ethical concerns contained in these examples. This analysis revealed a wide range of ethical issues raised by practitioners that were frequently conflated under the umbrella term of dark patterns, while also underscoring a shared concern that UX designers could easily become complicit in manipulative or unreasonably persuasive practices. We conclude with implications for the education and practice of UX designers, and a proposal for broadening research on the ethics of user experience. In this paper, we have provided an overview of the landscape of dark patterns from the perspective of UX practitioners, describing the breadth of these patterns as they currently exist and the potential uptakes for HCI research and UX practice in further defining this ethical and value-laden phenomenon. We have recharacterized existing practitioner-led notions of dark patterns to reflect the strategies that designers activate when manipulating the balance of user and shareholder value, supporting both ethically-focused UX practice and future HCI scholarship regarding ethics and values.
"
Charrette: Supporting In-Person Discussions around Iterations in User Interface Design,https://dl.acm.org/authorize?N657246,"
As a rule, user interface designers work iteratively. Over the course of a project, they repeatedly gather feedback, typically through in-person meetings, and update their designs accordingly. Through formative work, we find that design software tools do not support designers in managing meeting notes and previous design iterations as a cohesive whole. This causes designers to rely on ad-hoc practices for organizing work, which makes it hard for them to keep track of relevant feedback and explain their design decisions. To address this problem, we present Charrette, a system that allows designers to curate design iterations, attach meeting notes to the relevant content, and navigate sequences of design iterations with the associated notes to facilitate in-person discussions. In an exploratory user study, we evaluate how Charrette affects designers' self-reported ease in handling feedback during face-to-face discussions, compared with using their own tools. We find that using Charrette correlates with increased confidence and recall in discussing previous design decisions. In this paper, we examined the difficulties designers face in presenting their design process during in-person discussions. Effectively presenting process requires both handling diverse feedback as well as we alternatives and history. Through formative work, we identified three archetypical practices in design work We then implemented Charrette, which provides software support for each of these practices. We evaluated Charrette in a short-term exploratory comparative study with 12 professional designers to gauge how software tools to support these practices affected designers’ ease in discussing their process. From this evaluation, we show that, although they faced usability issues, designers readily leveraged explicit support for design history to ground discussion with previous work and feedback.

"
Design Within a Patriarchal Society: Opportunities and Challenges in Designing for Rural Women in Bangladesh,https://dl.acm.org/authorize?N657247,"
This paper examines the opportunities and issues that arise in designing technologies to support low-income rural women in Bangladesh. Through a qualitative, empirical study with 90 participants, we reveal systemic everyday challenges that women face that form the backdrop against which technology design could potentially happen. We discuss how technology is already impacting women's lives, sometimes by reinforcing their subservient role in society and sometimes used tactically by women to gain a measure of agency. The issues raised by our participants concerning technology's place in their lives provide HCI researchers with valuable guidance about what might (or might not) be appropriate to design for them. We also show how prevalent HCI research and design strategies may fit more poorly than expected into rural women's lives, and we discuss possible alternative design directions, and the ethical and pragmatic trade-offs that they entail. Our contribution is not to ""solve"" the problem of designing for low-income rural women, but to expand the HCI community's understanding of technology design within deeply patriarchal societies. This paper unpacked opportunities for and challenges to designing technologies that empower rural, low-income women in Bangladesh. Our field study revealed systemic challenges that women face and identified barriers these challenges pose to the efficacy of prevalent HCI research and design strategies. To move forward, we proposed ways to design within the patriarchal system, even if we wish to subvert it, and discussed the ethical and pragmatic trade-offs of such an approach. Taken together, our findings expand the HCI community’s understanding of technology design within deeply patriarchal societies.
"
This App Would Like to Use Your Current Location to Better Serve You: Importance of User Assent and System Transparency in Personalized Mobile Services,https://dl.acm.org/authorize?N657248,"
Modern mobile apps aim to provide personalized services without appearing intrusive. A common strategy is to let the user initiate the service request (e.g., ""click here to receive coupons for your favorite products""), a practice known as ?overt personalization."" Another strategy is to assuage users' privacy concerns by being transparent about how their data would be collected, utilized and stored. To test these two strategies, we conducted a 2 (Personalization: Overt vs. Covert) x 2 (Transparency: High vs. Low) factorial experiment, with a fifth control condition. Participants (N=302) interacted with GreenByMe, a prototype of an eco-friendly mobile application. Data show that overt personalization affects perceived control. Significant three-way interactions between power usage, perceived overt personalization and perceived information transparency was seen on perceived ease of use, trust in the app, user engagement and behavioral intention to use the app in the future. In addition, results reveal that perceived information transparency also promotes trust, which is negatively linked with privacy concerns and positively correlated with user engagement and product involvement. The current study demonstrates an initial exploration of the role of personalization and information transparency in enhancing user engagement and product involvement. We found that overt personalization is positively associated with users’ perceived control. Results indicate that instead of the overt personalization and information transparency cues, users’ perceptions of overt personalization were significantly associated with privacy concern, user engagement and product involvement. Moreover, power usage tended to explain significant variance in perceived control, perceived ease of use, trust, user engagement, product involvement, attitude toward application, behavioral intention, and purchase intention. Based on the results, the current study provides suggestions to designers and developers. Given the significance of power usage, user-centered design is essential to take into account the differential value of personalization and transparency for engendering usability in mobile and other applications among low- and high-power users. Finally, to increase trust, reduce privacy concern and enhance user experience, features and cues suggesting overt personalization mechanism and high information transparency respectively should not only be present on the user interface, but made more apparent to them.

"
Pictures Worth a Thousand Words: Reflections on Visualizing Personal Blood Glucose Forecasts for Individuals with Type 2 Diabetes,https://dl.acm.org/authorize?N657249,"
Type 2 Diabetes Mellitus (T2DM) is a common chronic condition that requires management of one's lifestyle, including nutrition. Critically, patients often lack a clear understanding of how everyday meals impact their blood glucose. New predictive analytics approaches can provide personalized mealtime blood glucose forecasts. While communicating forecasts can be challenging, effective strategies for doing so remain little explored. In this study, we conducted focus groups with 13 participants to identify approaches to visualizing personalized blood glucose forecasts that can promote diabetes self-management and understand key styles and visual features that resonate with individuals with diabetes. Focus groups demonstrated that individuals rely on simple heuristics and tend to take a reactive approach to their health and nutrition management. Further, the study highlighted the need for simple and explicit, yet information-rich design. Effective visualizations were found to utilize common metaphors alongside words, numbers, and colors to convey a sense of authority and encourage action and learning. In this study we examined the perceptions and attitudes of individuals with type 2 diabetes recruited from low literacy populations towards visual representations of personalized blood glucose forecasts. The findings emphasize the importance of actionable, direct, simple, and information rich visualizations. These findings highlight the need for future work investigating the most effective ways of communicating health information and uncertainty in lowrisk contexts where individuals have the agency to make decisions.
"
Pac-Many: Movement Behavior when Playing Collaborative and Competitive Games on Large Displays,https://dl.acm.org/authorize?N657240,"
Previous work has shown that large high resolution displays (LHRDs) can enhance collaboration between users. As LHRDs allow free movement in front of the screen, an understanding of movement behavior is required to build successful interfaces for these devices. This paper presents Pac-Many; a multiplayer version of the classical computer game Pac-Man to study group dynamics when using LHRDs. We utilized smartphones as game controllers to enable free movement while playing the game. In a lab study, using a 4m × 1m LHRD, 24 participants (12 pairs) played Pac-Many in collaborative and competitive conditions. The results show that players in the collaborative condition divided screen space evenly. In contrast, competing players stood closer together to avoid benefits for the other player. We discuss how the nature of the task is important when designing and analyzing collaborative interfaces for LHRDs. Our work shows how to account for the spatial aspects of interaction with LHRDs to build immersive experiences. In this paper, we presented Pac-Many, a multiplayer game for LHRDs inspired by the classical computer game Pac-Man. Furthermore, we presented a lab study comparing the players’ behavior in a collaborative and a competitive playing mode. The results show that the players were socially engaged in the collaborative condition and shared tasks in the game. Thereby the players minimized the physical effort and moved less in front of the display. In contrast, the competitive condition triggered physical action of the players. The implementation of Pac-Many allows an arbitrary number of players to join a game. Furthermore, the game maze can be displayed on multiple distributed displays simultaneously. This allows us, in future work, to analyze the behavior of more than two players in front of one display. Furthermore, we will compare playing Pac-Many remotely to collocated play.

"
BebeCODE: Collaborative Child Development Tracking System,https://dl.acm.org/authorize?N657241,"
Continuous tracking young children's development is important for parents because early detection of developmental delay can lead to better treatment through early intervention. Screening tests, often based on questions answered by a parent, are used to assess children's development, but responses from only one parent can be subjective and even inaccurate due to limited memory and observations. In this work, we propose a collaborative child development tracking system, where screening test responses are collected through collaboration between parents or caregivers. We implement BebeCODE, a mobile system that encourages parents to independently answer all developmental questions for a given age and resolve disagreements through chatting, image/video sharing, or asking a third person. A 4-week deployment study of BebeCODE with 12 families found that parents had approximately 22% disagreements about questions regarding their children's developmental and BebeCODE helped them reach a consensus. Parents also reported that their awareness of their child's development, increased with BebeCODE. In this paper, we presented the design and implementation of BebeCODE, a collaborative mobile system to assess child development. BebeCODE encourages parents to answer all developmental questions independently and resolve disagreements to reach a consensus via chatting, image/video sharing, and asking a third person. We conducted a 4-week deployment study and found that BebeCODE successfully helped parents participate in their children’s developmental assessment and resolve their disagreement to reach a consensus.

"
Beyond the Libet Clock: Modality Variants for Agency Measurements,https://dl.acm.org/authorize?N657242,"
The Sense of Agency (SoA) refers to our capability to control our own actions and influence the world around us. Recent research in HCI has been investigating SoA to provide users an instinctive sense of ""I did that"" as opposed to ""the system did that"". However, current agency measurements are limited. The Intentional Binding (IB) paradigm provides an implicit measure of the SoA, however, it is constrained by requiring high visual attention to a ""Libet clock"" on-screen. In this paper, we extended the timing stimuli through auditory and tactile cues. Our results demonstrate that audio timing through voice commands and haptic timing through tactile cues on the hand, are an effective alternative measure of the SoA using the IB paradigm. They both address current limitations of the traditional method such as visual attention overload and lack of engagement. We discuss how our results can be applied to measure SoA in tasks involving different interactive scenarios such as in Mixed/Virtual Reality. Current research on agency in the field of HCI has been limited by agency measures based on subjective judgement. While the IB paradigm provides an implicit and quantitative measurement of the SoA, it has limitations regarding high visual attention. Here we provide two alternative techniques that employ audio timing through voice commands and haptic timing through tactile stimulation on the hand. Our techniques allow measuring perception of time in an IB task, revealing non-significant differences with the traditional visual method (Libet clock), but addressing high visual demand and lack of engagement. We believe this work will enable agency implication in HCI applications. Measuring users’ SoA in broader modalities will allow exploring interaction techniques that give users an instinctive sense of control on the environment.

"
PokeRing: Notifications by Poking Around the Finger,https://dl.acm.org/authorize?N657243,"
Smart-rings are ideal for subtle and always-available haptic notifications due to their direct contact with the skin. Previous researchers have highlighted the feasibility of haptic technology in smart-rings and their promise in delivering noticeable stimulations by poking a limited set of planar locations on the finger. However, the full potential of poking as a mechanism to deliver richer and more expressive information on the finger is overlooked. With three studies and a total of 76 participants, we informed the design of PokeRing, a smart-ring capable of delivering information via stimulating eight different locations around the index finger's proximal phalanx. We report our evaluation of the performance of PokeRing in semi-realistic wearable conditions, (standing and walking), and its effective usage for information transfer with twenty-one spatio-temporal patterns designed by six interaction designers in a workshop. Finally, we present three applications that exploit PokeRing's notification usages.

"
Forgotten But Not Gone: Identifying the Need for Longitudinal Data Management in Cloud Storage,https://dl.acm.org/authorize?N657244,"
Users have accumulated years of personal data in cloud storage, creating potential privacy and security risks. This agglomeration includes files retained or shared with others simply out of momentum, rather than intention. We presented 100 online-survey participants with a stratified sample of 10 files currently stored in their own Dropbox or Google Drive accounts. We asked about the origin of each file, whether the participant remembered that file was stored there, and, when applicable, about that file's sharing status. We also recorded participants' preferences moving forward for keeping, deleting, or encrypting those files, as well as adjusting sharing settings. Participants had forgotten that half of the files they saw were in the cloud. Overall, 83% of participants wanted to delete at least one file they saw, while 13% wanted to unshare at least one file. Our combined results suggest directions for retrospective cloud data management. By investigating our participants’ perspectives on a stratified sample of files stored in their own Google Drive or Dropbox account, we built a better understanding of the contents of cloud-storage accounts, identifying latent needs for retrospective file management tools. We used a stratified sample to measure a broad cross-section of files users retain in their cloud storage accounts, rather than focusing on the files most likely to arouse security and privacy concerns (e.g., files named “taxreturn2017.pdf” or that contain saved passwords). Even so, we found that 83% of participants wanted to permanently delete at least one file from this sample of ten. This result highlights the disconnect between our participants’ desired file-management decisions and the high overhead of retrospectively managing thousands of files in a cloud storage account. Thus, our results highlight the need for retrospective privacy mechanisms that empower users to manage the risks latent in their file archives without expending unreasonable effort.
"
"A Functional Optimization Based Approach for Continuous 3D Retargeted Touch of Arbitrary, Complex Boundaries in Haptic Virtual Reality",https://dl.acm.org/authorize?N657355,"
Passive or actuated physical props can provide haptic feedback, leading to a satisfying sense of presence and realism in virtual reality. However, the mismatch between the physical and virtual surfaces (boundaries) can diminish user experience. Haptic retargeting can overcome this limitation by utilizing visio-haptic effects. Previous investigations in haptic retargeting have focused on methods for point based position retargeting and techniques for remapping 2D shapes or simple 3D shape changes. Our approach extends haptic retargeting to complex, arbitrary shapes that provide a continuous mapping across all points on a boundary. This new approach also allows for multi-finger interaction. We describe a functional optimization to find the ideal spatial warping function with different goals: a maximum mapping smoothness, a minimum mismatch between the real and virtual world, or the combination of the two. We report on a preliminary user study of different optimization goals and elaborate potential applications through a set of demonstrations. In this paper we introduced a new technique for continuous haptic retargeting of complex arbitrary 3D shapes based on a functional optimization. This optimization can have three goals, maximizing the smoothness (gradient of the mapping), minimizing position mismatch, or the combination of the two. We also described techniques for defining the boundary conditions for this optimization. From our preliminary user study we have shown that this technique can be useful when the mismatch between the two boundary conditions (surfaces) are under the Just Noticeable Threshold. We believe these techniques can be readily applied to haptic retargeting of physical props in virtual reality, and we hope that this will enable further study and development of haptic retargeting techniques.

"
"Sense of Presence, Attitude Change, Perspective-Taking and Usability in First-Person Split-Sphere 360° Video",https://dl.acm.org/authorize?N657356,"
This paper examines the sense of presence, attitude change, perspective-taking, and usability of a split-sphere, first-person perspective 360 degree video about gender inequality, in which people can choose to watch the narrative from the male or female character's perspective. Sixty-seven participants were randomly assigned to watch (1) the video in 360 degree split-view in a head-mounted display, (2) the same film as 180 degree in a HMD, or (3) a flat control version of the video on a laptop. The 360 degree split-sphere increased the viewers' feeling of personal responsibility for resolving gender inequality, desire to rewatch the video, fear of missing out, and feeling of missing the full story. The 180 degree video created the strongest sense of presence, embodiment, and understanding of the character. However, people with greater egocentric projection onto the male character felt less responsible for resolving gender inequality, particularly in the 360 degree split-view. This paper examined the sense of presence, attitude change, perspective-taking, and usability of a first-person, 360° split sphere video about gender inequality by comparing it with a 180° video and a flat video in a between-subjects experiment. The findings suggest that, while creating a lower sense of presence than the 180° video, the first-person, 360° split-sphere video developed a stronger personal responsibility about advancing gender equality. However, the 360° splitsphere video poses new challenges for the user experience: The users experienced fear of missing out, and had difficulty following the narrative, receiving only a fragmented storyline instead of being able to focus on the story. Moreover, we should also be aware that freely choosing whose perspective to take in a split-view 360° video may reinforce rather than change preexisting stereotypes.

"
“I Hear You”: Understanding Awareness Information Exchange in an Audio-only Workspace,https://dl.acm.org/authorize?N657357,"
Graphical displays are a typical means for conveying awareness information in groupware systems to help users track joint activities, but are not ideal when vision is constrained. Understanding how people maintain awareness through non-visual means is crucial for designing effective alternatives for supporting awareness in such situations. We present a lab study simulating an extreme scenario where 32 pairs of participants use an audio-only tool to edit shared audio menus. Our aim is to characterise collaboration in this audio-only space in order to identify whether and how, by itself, audio can mediate collaboration. Our findings show that the means for audio delivery and choice of working styles in this space influence types and patterns of awareness information exchange. We thus highlight the need to accommodate different working styles when designing audio support for awareness, and extend previous research by identifying types of awareness information to convey in response to group work dynamics. We presented a study that examined workspace awareness information exchange during an audio-only collaborative task. The results showed that concealing or exposing audio output in such a workspace had an impact on which workspace awareness information was exchanged between partners, and that this observed impact was also dependent on the working style partners chose to employ. These results highlighted that the relevance of workspace awareness information is not static but dynamic, changing according to how collaborators choose to work with sounds. Our findings provide empirical evidence that audio can be used as a sole means for supporting nonvisual collaboration with shared menus and for maintaining workspace awareness and common ground during such collaborations. Further, we identified which type of workspace awareness information was exchanged under different conditions, and thus further extended previous research by identifying what information is relevant and when, and should therefore be captured about collaborators’ interactions and conveyed to partners when designing workspace awareness support for non-visual collaboration.

"
User-Driven Design Principles for Gesture Representations,https://dl.acm.org/authorize?N657358,"
Many recent studies have explored user-defined interactions for touch and gesture-based systems through end-user elicitation. While these studies have facilitated the user-end of the human-computer dialogue, the subsequent design of gesture representations to communicate gestures to the user vary in style and consistency. Our study explores how users interpret, enact, and refine gesture representations adapting techniques from recent elicitation studies. To inform our study design, we analyzed gesture representations from 30 elicitation papers and developed a taxonomy of design elements. We then conducted a partnered elicitation study with 30 participants producing 657 gesture representations accompanied by think-aloud data. We discuss design patterns and themes that emerged from our analysis, and supplement these findings with an in-depth look at users' mental models when perceiving and enacting gesture representations. Finally, based on the results, we provide recommendations for practitioners in need of ""visual language"" guidelines to communicate possible user actions. Our analysis of user-defined gesture representations has defined guidelines and principles for designing symbolic communication of interaction in both academic papers and gesture-based systems. This study has furthered conversation on human-computer communication and the intentional use of graphical elements to improve clarity and consistency in symbolic communication. These findings can be used by researchers and designers alike to guide future creations of representations that will take end-user behavior and preference into consideration, refining the less explored system-side of the human-computer dialogue.
"
To Put That in Perspective: Generating Analogies that Make Numbers Easier to Understand,https://dl.acm.org/authorize?N657359,"
Laypeople are frequently exposed to unfamiliar numbers published by journalists, social media users, and algorithms. These figures can be difficult for readers to comprehend, especially when they are extreme in magnitude or contain unfamiliar units. Prior work has shown that adding ""perspective sentences"" that employ ratios, ranks, and unit changes to such measurements can improve people's ability to understand unfamiliar numbers (e.g., ""695,000 square kilometers is about the size of Texas""). However, there are many ways to provide context for a measurement. In this paper we systematically test what factors influence the quality of perspective sentences through randomized experiments involving over 1,000 participants. We develop a statistical model for generating perspectives and test it against several alternatives, finding beneficial effects of perspectives on comprehension that persist for six weeks. We conclude by discussing future work in deploying and testing perspectives at scale. In this work we have shown that perspective statements help people estimate unknown quantities and that the beneficial effects of perspectives can remain significant for at least six weeks after the time of exposure. We tested several automated policies for generating perspectives and found that each provides substantial benefits over a control condition without perspectives, demonstrating that it is both possible and relatively easy to improve reader comprehension at scale. Interestingly, we found that a simple, global model for generating perspectives is competitive with a personalized policy. This is not to say that global approaches are always superior to personalized ones, but rather that one can construct effective domain-specific perspectives for a wide audience without elaborate optimization. Encouraged by these results, we have since deployed perspectives in “instant answer” numbers returned by the Bing search engine. The model developed in this paper was used to create a library of perspectives for country areas in terms of U.S. states, which was first evaluated by a third-party panel of human judges in a side-by-side comparison before being incorporated into the search engine. Now, when the search engine receives a query from a user in the U.S. about the geographic area of a country, it displays a small piece of text comparing the country to a U.S. state. For example, as shown in Figure 8, when responding to a query for the “area of Pakistan”, the search engine puts the answer of “307,373 square miles” into perspective as “about twice the size of California”. With this first scenario successfully launched, we are actively working to add perspectives to other instant answers provided by the search engine. Figure 8. An example perspectives generated by our model, rendered live on the Bing search engine, which phrases the area of Pakistan as twice the size of California. Though the work presented here focuses on the prominent but relatively narrow domain of country-level statistics, the insights it reveals apply much more broadly. Having established the importance of simple multipliers and familiar reference objects in generating effective analogies, the main challenge going forward is generalizing these ideas to arbitrary domains so that perspectives can be deployed and tested at scale. One approach to solving this problem is to replace domain-specific human feedback on the effectiveness of different perspectives with machine-learned models that can be automated across a variety of domains. For instance, in ongoing work similar to that of Hullman et al. [10], we are creating a database of reference objects that covers a wide range of measurements and contains proxy features for gauging the familiarity and analogical suitability of these reference objects. These features include how often reference objects are mentioned in different text corpora, queried in search engines, and visited on Wikipedia, all of which can be gathered automatically and easily localized to different subpopulations. We see these as important steps in utilizing online platforms to improve numerical comprehension among both authors and their audiences. We hope that the perspectives framework will not only aid producers and consumers of information, but also stimulate research in education, journalism, and cognitive psychology.
"
Addressing Network Anxieties with Alternative Design Metaphors,https://dl.acm.org/authorize?N657350,"
Optimism and positivity permeate discourses of smart interactive network technologies. Yet we do not have to look too far or too deep to find anxieties knotting up on the horizon and festering below the network's glistening surface. This paper contributes a set of concepts, tactics, and novel design forms for addressing network anxieties generated through a design-led inquiry, or research through design approach. We present three technically grounded metaphors illustrated with examples selected from our exploratory design process. Weaving together concepts from surveillance studies, cultural studies, and other areas of the humanities with our visual and physical design work, we help draw attention to under-addressed concerns within HCI while proposing alternative ways of framing and engaging design issues arising with network technologies. This paper has contributed a set of concepts, tactics, and design forms for addressing network anxieties. We began by framing a territory of negative network affects within which to inventively find, frame, and create problems associated with network technologies. Framing our inquiry around negative network affects directs us toward affective forces rather then easily quantifiable or statistically significant network effects. Our use of this term “network affects” engages in intentional semantic slippage between psychological affect, or emotion, and philosophical theory of affect as visceral and vital forces extending beyond emotion that “drive us toward movement, toward thought and extension, that can likewise suspend us (as if in neutral) across a barely registering accretion of force-relations, or that can leave us overwhelmed by the world’s apparent intractability.” [44, p. 1] For our task of addressing network anxieties, we are drawn to affective registers for their capacity to spark imagination and propel us through overwhelming feelings of intractability toward the inventive framing and making of problems. The designs presented here, an RtD contribution in their own right, illustrated design tactics that help operationalize the design metaphors of edge cases, pervasive fields, and unique personal identifiers. These tactics are tools that others may use to address network anxieties by inventively framing problems or by affirmatively crafting responses. These tactics also function to reveal the thinking behind our own research through design process, responding to calls to demystify and explain design practice in HCI [5,43,47,57], Design metaphors have a rich history within HCI. The alternative metaphors we’ve presented—grounded in technical networking discourse but redirected toward the negatively affective—help us see constructs such as clouds, smart homes, and personal digital assistants as metaphors by critically imagining alternatives (fog, cages, and spies, perhaps.) If we indeed want to address network anxieties along with other unwelcome aspects of interactive technology, we may well need new metaphors to do so. Finally, we hope that we have also contributed to the corpus of work that employs theories and methods from the humanities and arts to HCI. One way we sought to do so in this paper is to offer an approach to thinking and writing that is open and lively, that does not attempt to come to quick resolution, but rather endeavors to be generous and generative. We approached our making activities similarly, drawing inspiration from arts practices such as tactical media, social practice, and art intervention to provide a space for creative inquiry that is both playful and sincere.
"
Supporting Communication between Grandparents and Grandchildren through Tangible Storytelling Systems,https://dl.acm.org/authorize?N657351,"
Grandparents and grandchildren that live apart often rely on communication technologies, such as messengers, video conferencing, and phone calls for maintaining relationships. While some of these systems are challenging for grandparents, others are less engaging for children. To facilitate communication, we developed StoryBox, a tangible device that allows sharing photos, tangible artifacts, and audio recordings of everyday life. We conducted a preliminary study with two families to identify design issues, and further refine the prototype. Subsequently, we conducted a field study with four families for up to four weeks to better understand real-world use and examine inter-generational connectedness. We found that StoryBox was accessible, simple, and helped bridge the technological gap between grandparents and grandchildren. Children communicated asynchronously in a playful and idiosyncratic manner, and grandparents shared past family memories. We provide insights on how to ease communication between different generations, engage them in sharing activities, and strengthen family relationships. In this paper, we present the design and implementation of a tangible storytelling system – StoryBox – for sharing photos, tangible artifacts, and audio recordings of everyday life. To better understand grandparents and grandchildren’s realworld use and examine connectedness, we evaluated StoryBox in a preliminary study with two families and a subsequent study with four families for up to four weeks. We found that StoryBox enabled children to express themselves freely in a playful manner, was simple to use, and helped bridge the intergenerational technological gap. We further provided insights on how to ease communication between different generations, engage them in sharing activities, and strengthen family relationships.

"
Tangible Tens: Evaluating a Training of Basic Numerical Competencies with an Interactive Tabletop,https://dl.acm.org/authorize?N657352,"
Basic numerical competencies developed in kindergarten form the foundations of math achievement. This indicates the importance of early interventions in the case of numerical difficulties. Building on research on math manipulatives and tangible interfaces, we developed a training of basic numerical competencies using an interactive tabletop in combination with physical LEGO-like blocks. In an experiment, we evaluated the effectiveness of the training on children's learning of the partner number concept, basic numerical competencies and number line estimation, compared to a content-wise similar training with physical manipulatives and a human tutor. We observed significant increases in children's understanding of the partner number concept and basic numerical competencies in both training conditions, but no differential training effects. As children can play on the interactive surface with reasonable autonomy, it seems to provide a low threshold possibility to enrich kindergarten education on numerical concepts. The present study showed that trainings using physical manipulatives to teach the partner number concept can increase preschool children’s basic numerical competencies. Building on findings from embodied numerical trainings, the interactive surface training was one of the first to harness haptic experiences in combination with an interactive tabletop to enrich math learning. The significant learning effects of the training on the interactive surface based on system feedback seem particularly interesting. As children can play the training game on their own, it seems to provide a low threshold possibility to enrich kindergarten education on numerical concepts.
"
"'You Can Always Do Better!"": The Impact of Social Proof on Participant Response Bias",https://dl.acm.org/authorize?N657353,"
Evaluations of technological artifacts in HCI4D contexts are known to suffer from high levels of participant response bias---where participants only provide positive feedback that they think will please the researcher. This paper describes a practical, low-cost intervention that uses the concept of social proof to influence participant response bias and successfully elicit critical feedback from study participants. We subtly exposed participants to feedback that they perceived to be provided by people 'like them', and experimentally controlled the tone and content of the feedback to provide either positive, negative, or no social proof. We then measured how participants' quantitative and qualitative evaluations of an HCI artifact changed based on the feedback to which they were exposed. We conducted two controlled experiments: an online experiment with 245 MTurk workers and a field experiment with 63 women in rural India. Our findings reveal significant differences between participants in the positive, negative, and no social proof conditions, both online and in the field. Participants in the negative condition provided lower ratings and a greater amount of critical feedback, while participants in the positive condition provided higher ratings and a greater amount of positive feedback. Taken together, our findings demonstrate that social proof is a practical and generalizable technique that could be used by HCI researchers to influence participant response bias in a wide range of contexts and domains. Our social proof intervention has a number of key benefits that make it practical for researchers and practitioners to implement. One of our aims was to create an intervention that is generalizable and reproducible. We demonstrated that our intervention can be applied in two distinct contexts—an online experiment and a field study with low-literate participants in resource-constrained settings. In both experiments, we used the same experimental procedure with minor variations and received similar results that prove the efficacy of our social proof intervention. Compared to other techniques that aim to reduce response bias (e.g., randomized response [60] and unmatched count [50]), our intervention is low-cost, practical, easy to understand for organizations and participants, reproducible in different contexts (as we demonstrated), effective for both quantitative and qualitative feedback, and elicits critical feedback even when participants are evaluating a single artifact that is known to be associated with the researcher [15]. Taken together, these benefits suggest that, with a small amount of adaptation (described below) the intervention could be used by HCI researchers in a wide range of contexts and domains.
"
Teaching Language to Deaf Infants with a Robot and a Virtual Human,https://dl.acm.org/authorize?N657354,"
Children with insufficient exposure to language during critical developmental periods in infancy are at risk for cognitive, language, and social deficits [55]. This is especially difficult for deaf infants, as more than 90% are born to hearing parents with little sign language experience [48]. We created an integrated multi-agent system involving a robot and virtual human designed to augment language exposure for 6-12 month old infants. Human-machine design for infants is challenging, as most screen-based media are unlikely to support learning in [33]. While presently, robots are incapable of the dexterity and expressiveness required for signing, even if it existed, developmental questions remain about the capacity for language from artificial agents to engage infants. Here we engineered the robot and avatar to provide visual language to effect socially contingent human conversational exchange. We demonstrate the successful engagement of our technology through case studies of deaf and hearing infants. This paper describes the design of a unique dual-agent system that uses a physical robot and a virtual human to engage 6-12 month old deaf infants in linguistic interactions. Our system was bolstered by a perception system capable of estimating infant attention and engagement through thermal imaging and eye tracking. We documented our experiences in designing for a unique population (deaf infants), and summarized the lessons and guidelines that we established over an iterative design process. Our design was informed by experimental sessions spread over three years, highlighted here by three case studies. This system has been successful at soliciting infant attention, directing attention to the linguistic content, and keeping the infant engaged for developmentally appropriate lengths of time. We also observed instances of infants copying robot behavior, of infants producing signs displayed by the avatar, and of infants producing signs to the non-signing robot agent that they had observed the virtual human perform. These initial experiences give us hope that longer-term exposure to a system based on this work may be able to impact long-term learning in this unique population.
"
ExtraSensory App: Data Collection In-the-Wild with Rich User Interface to Self-Report Behavior,https://dl.acm.org/authorize?N657365,"
We introduce a mobile app for collecting in-the-wild data, including sensor measurements and self-reported labels describing people's behavioral context (e.g., driving, eating, in class, shower). Labeled data is necessary for developing context-recognition systems that serve health monitoring, aging care, and more. Acquiring labels without observers is challenging and previous solutions compromised ecological validity, range of behaviors, or amount of data. Our user interface combines past and near-future self-reporting of combinations of relevant context-labels. We deployed the app on the personal smartphones of 60 users and analyzed quantitative data collected in-the-wild and qualitative user-experience reports. The interface's flexibility was important to gain frequent, detailed labels, support diverse behavioral situations, and engage different users: most preferred reporting their past behavior through a daily journal, but some preferred reporting what they're about to do. We integrated insights from this work back into the app, which we make available to researchers for conducting in-the-wild studies. In this paper, we introduce the ExtraSensory App, a mobile application for collecting data in-the-wild, including sensor measurements and self-reported detailed labels of behavioral context. We validated this app in an in-the-wild study with 60 users. The app’s rich label-reporting interface was important to engage users with different behavior styles and phone-interaction preferences and to acquire detailed labels for over 300,000 minutes of diverse behavioral contexts. ExtraSensory’s history page showed to be very useful and the features it offered helped users recall their past context. The additional watch component turned out to be very helpful to keep the user-interaction from interfering with natural behavior. To maximize the utility of the watch, its prompts should be cleverly timed and require minimal reaction (single button press). Ongoing data collection and re-training of real-time classifiers will improve the server guesses and notifications and make user interaction easier and less time consuming. We believe that the insights that we describe in this paper will inspire future designs of in-the-wild data-collection studies. The public version of the ExtraSensory App that we provide will allow for further collections of data and studies that use real-time context-recognition in-the-wild for various applications in health-monitoring, aging-care, and other domains.
"
Collaborative Live Media Curation: Shared Context for Participation in Online Learning,https://dl.acm.org/authorize?N657366,"
In recent years, online education's reach and scale have increased through new platforms for large and small online courses. However, these platforms often rely on impoverished modalities, which provide limited support for participation in social learning experiences. We present Collaborative Live Media Curation (CLMC), a new medium for sharing context and participation in online learning. CLMC involves collaborative, synchronous collection, creation, and assemblage of web media, including images, text, video, and sketch. CLMC integrates live media including streaming video, screenshares, audio, and text chat. We deploy and study LiveMâché, a CLMC technology probe, in four situated online learning contexts. We discovered student and instructor strategies for sharing context and participating including creating curations in advance, sketching to illustrate and gesture, real-time transformations, sharing perspective, and assembling live streams. We develop implications through live experience patterns, which describe how spatial and computing structures support social activities. Collaborative live media curation is a new medium for live CSCW. Prior live streaming forms do not afford collaborative, free-form assemblage of live modalities. By extending free-from web curation with synchronous collaboration and live media, CLMC enables a new contextualization of live experience. By deploying the LiveMâché CLMC probe in four online learning situations, we provoked new participatory online learning experiences. We observed how participants invoke new collaborative live media curation strategies for sharing context, grounding collaboration, and constructing meaning through the assemblage of media and performance of deictic gestures. Like others, we found that shared context is based on common understanding of framing [14] and social construction of mutual understanding [4]. The strategies—creating shared curation space in advance, sketching to illustrate and gesture, real-time element transformations, sharing perspective, assembling web cam and screenshare streams—contribute new, concrete means for promoting collaborative meaning making through shared visual and social context. Collaborative free-form web curation afforded new modalities for participation. Prior forms limit most participants to limited modalities, such as text chat. CLMC’s open-ended integration of media and modalities affords new opportunities for any participant to engage in the collection, broadcasting, and assemblage of media. Finally, we articulated patterns of online live experience. Prior live media platforms typically support a single activity pattern. For example, Google Hangouts supports the small team pattern, while Twitch supports broadcast. CLMC is more flexible, enabling participants to assign roles and assemble media to form small-team, broadcast, and touring patterns of live experience. Participants can shift between patterns, using roles, to support dynamic social contexts. Future work has the potential to provide value by exploring how new strategies for assembling and structuring media will support new forms of participation and shared context in situated live experiences.
"
"Measuring, Understanding, and Classifying News Media Sympathy on Twitter after Crisis Events",https://dl.acm.org/authorize?N657367,"
This paper investigates bias in coverage between Western and Arab media on Twitter after the November 2015 Beirut and Paris terror attacks. Using two Twitter datasets covering each attack, we investigate how Western and Arab media differed in coverage bias, sympathy bias, and resulting information propagation. We crowdsourced sympathy and sentiment labels for 2,390 tweets across four languages (English, Arabic, French, German), built a regression model to characterize sympathy, and thereafter trained a deep convolutional neural network to predict sympathy. Key findings show: (a) both events were disproportionately covered (b) Western media exhibited less sympathy, where each media coverage was more sympathetic towards the country affected in their respective region (c) Sympathy predictions supported ground truth analysis that Western media was less sympathetic than Arab media (d) Sympathetic tweets do not spread any further. We discuss our results in light of global news flow, Twitter affordances, and public perception impact. We presented a data-driven approach to tease out differences between Western and Arab Twitter news reporting of the 2015 Paris and Beirut attacks, where we found evidence for differential coverage across the attacks. For sympathy bias, we found that Western media tweets were less sympathetic when covering the Beirut attacks, however Western media was overall less sympathetic than Arab media, even for Paris. Finally, based on our labeled data, we trained a deep CNN to predict sympathy from unlabeled data, and results further supported our ground truth analysis that Western media had less sympathetic tweets than the Arab media, across both attacks. As a more general framework, our work contributes to an understanding of media bias on Twitter, and factors that may influence it, which are not necessarily limited to the studied attacks. We believe the methods we adopted are more widely applicable to other areas of computational journalism, and can serve as useful tools to better understand, expose, and design around media bias.

"
Defining Through Expansion: Conducting Asynchronous Remote Communities (ARC) Research with Stigmatized Groups,https://dl.acm.org/authorize?N657368,"
Researchers in HCI have typically relied on face to face (FtF) methods for recruitment and data collection in their research with people living with HIV, whereas social scientists have adopted computer-mediated approaches to address concerns about data validity and access to this stigmatized population. In this paper, we use the asynchronous remote community (ARC) research method to leverage HCI instruments in an online format. ARC successfully engaged people living with HIV in terms of participation and retention by providing a safe space to discuss their experiences. By expanding on past ARC studies, we contribute to an ongoing conversation about defining ARC and working towards increased data validity -- especially in stigmatized communities. We started this paper by exploring how computer-mediated interventions have helped researchers study PLH in the social sciences. To contribute to the conversation on how HCI researchers could conduct studies with stigmatized populations, we analyzed the suitability of the ARC method for research with PLH in an online platform. We also offered a glimpse of the analytic power that ARC offers via data triangulation in order to validate findings. To summarize, the main contributions of our paper are: (1) the provision of five new lessons on how to apply the ARC method in HCI research via the application of the method to a highly stigmatized and isolated population, (2) the confirmation of lessons learned from previous ARC research, (3) the introduction of a formal definition of the ARC method, a list of minimum requirements for its application, and ways to determine its success, and (4) a contribution to the ongoing CHI4Good or Good4CHI discussion. To conclude, we argue that the ARC method is indeed suitable for studying stigmatized individuals under an HCI lens. We hope that this method not only helps the CHI community conduct studies with hard-to-reach – or atypical– populations, but that it also helps balance out the benefits derived from such endeavors between researchers and participants.
"
ActiveErgo: Automatic and Personalized Ergonomics using Self-actuating Furniture,https://dl.acm.org/authorize?N657369,"
Proper ergonomics improves productivity and reduces risks for injuries such as tendinosis, tension neck syndrome, and back injuries. Despite having ergonomics standards and guidelines for computer usage since the 1980s, injuries due to poor ergonomics remain widespread. We present ActiveErgo, the first active approach to improving ergonomics by combining sensing and actuation of motorized furniture. It provides automatic and personalized ergonomics of computer workspaces in accordance to the recommended ergonomics guidelines. Our prototype system uses a Microsoft Kinect sensor for skeletal sensing and monitoring to determine the ideal furniture positions for each user, then uses a combination of automatic adjustment and real-time feedback to adjust the computer monitor, desk, and chair positions. Results from our 12-person user study demonstrated that ActiveErgo significantly improves ergonomics compared to manual configuration in both speed and accuracy, and helps significantly more users to fully meet ergonomics guidelines. We present ActiveErgo, the first active approach to improve ergonomics by providing automatic and personalized ergonomics through a combination of sensing and selfactuating workspaces. Our user study results demonstrated that our prototype significantly improved users’ postures and assisted more users in satisfying ergonomics guidelines. ActiveErgo was also significantly faster (2.2x) than the manual approach, and all participants preferred our system.
"
Presenting The Accessory Approach: A Start-up's Journey Towards Designing An Engaging Fall Detection Device,https://dl.acm.org/authorize?N657360,"
This paper explores a design experiment concerning the development of a personalised and engaging wearable fall detection device customised for care home residents. The design experiment focuses on a start-up company's design process, which utilises a new design approach, which I name the accessory approach, to accommodate given cultural fit purposes of a wearer. Influenced by accessory design, that belong neither to fashion nor jewellery, the accessory approach is a way of designing wearables that involve both functional and expressive qualities including the wearer's physical, psychological and social needs. The accessory approach is proven to enable first hand insight of the wearer's preferences, leading to in-depth knowledge and enhanced iterative processes, which support the design of a customised device. This type of knowledge is important for the HCI community as it brings accessory design disciplines into play when wanting to understand and design for individual needs, creating engaging wearables design. This paper introduces a design experiment to facilitate the implementation of an accessory approach in a start-up company with the aim of developing an personal wearable fall detection device to be worn at all times by care home residents. Through the accessory approach, the company, with the help of two jewellery design interns, was introduced to ethnographic methods to generate cultural fit purposes of the wearers. The benefit of this act established engaging relationships between the interns and the care home residents, for the interns to gain deeper insight of the wearer’s physical, psychological and social needs. Such an accessory approach enabled the interns to acquire an empathic understanding of the persons they were designing for, and their personal preferences in an effective and fast manner. The insights inspired the interns to design the final prototype, Pearl, functioning as a magnetic brooch covering the electronics of a fall detection device – in 16 different choices of customisation. These findings are of importance as the accessory approach functions as a skillset for the designers to connect with and get first hand insight of a wearer’s preferences, leading to in-depth knowledge of how to understand adorning factors and meet cultural fit purposes. This knowledge is of importance for the HCI community as the accessory approach activates embedded emotional values of worn objects and determines them to be of relevance when designing wearable health technology for an engaging experience.
"
What Moves Players?: Visual Data Exploration of Twitter and Gameplay Data,https://dl.acm.org/authorize?N657361,"
In recent years, microblogging platforms have not only become an important communication channel for the game industry to generate and uphold audience interest but also a rich resource for gauging player opinion. In this paper we use data gathered from Twitter to examine which topics matter to players and to identify influential members of a game's community. By triangulating in-game data with Twitter activity we explore how tweets can provide contextual information for understanding fluctuations in in-game activity. To facilitate analysis of the data we introduce a visual data exploration tool and use it to analyze tweets related to the game Destiny. In total, we collected over one million tweets from about 250,000 users over a 14-month period and gameplay data from roughly 3,500 players over a six-month period. In this paper, by adopting a qualitative approach and by using the online-only multiplayer shooter Destiny as a case study, we explored for which communication purposes players use Twitter, which users constitute important members, and if Twitter can help in explaining in-game activity. Our study shows that while the volume of tweets and changes in sentiment are useful indicators for identifying potential events of interest, gameplay data can provide an additional means to assist in this regard. At the same time, our results suggest that Twitter can act as a valuable source for contextual clues to help understand variations in in-game activity. Our results also provide evidence that players use Twitter for a variety of communication purposes, including the sharing of fan art and game guides, searching for other people to play with, for discussing game-related issues and, in certain instances, even private matters of community members, as well as – in case of streamers – for announcing on-going live sessions. In terms of influential community members, streamers maintain some of the most popular accounts besides the official game and developer affiliated accounts.

"
Veritaps: Truth Estimation from Mobile Interaction,https://dl.acm.org/authorize?N657362,"
We introduce the concept of Veritaps: a communication layer to help users identify truths and lies in mobile input. Existing lie detection research typically uses features not suitable for the breadth of mobile interaction. We explore the feasibility of detecting lies across all mobile touch interaction using sensor data from commodity smartphones. We report on three studies in which we collect discrete, truth-labelled mobile input using swipes and taps. The studies demonstrate the potential of using mobile interaction as a truth estimator by employing features such as touch pressure and the inter-tap details of number entry, for example. In our final study, we report an F1-score of .98 for classifying truths and .57 for lies. Finally we sketch three potential future scenarios of using lie detection in mobile applications; as a security measure during online log-in, a trust layer during online sale negotiations, and a tool for exploring self-deception. We are frequently subject to lying, and to date lack means of classifying lies on mobile devices beyond written text and speech. This leaves a large space of interaction open to deception. We explore the feasibility of a content-agnostic, sensorled approach to lie detection on smartphones that considers only taps and swipes. Through three studies we presented empirical evidence for the feasibility of commodity lie detection using mobile interaction. First, we found significant differences in response times between lies and truths for simple mobile interactions. Next, we reported on the individual interaction differences observed between lying and truth telling in a mobile version of the Ultimatum game that encouraged lying. The study showed that some features of mobile interaction varies with the honesty of an action. Specifically, properties of number entry were good indicators of deceit. Last, we reported on a study where participants took part in a mobile dice game that incentivized lying. We trained a classifier on mobile sensor data that ignores the input data itself. We achieved 96% precision and 95% recall in truth detection, and 65% precision and 59% recall for lie detection. While promising, these results do not support reliable binary lie classification. Instead, we suggest their use a means of improving peoples’ own near-chance level lie classification. Based on the findings, we introduced Veritaps: an optional layer in mobile interaction, allowing users to share truth assessments of their input. We presented three potential use cases of Veritaps, across online form-filling, inter-personal communication, and personal reflection.

"
How the Experts Do It: Assessing and Explaining Agent Behaviors in Real-Time Strategy Games,https://dl.acm.org/authorize?N657363,"
How should an AI-based explanation system explain an agent's complex behavior to ordinary end users who have no background in AI? Answering this question is an active research area, for if an AI-based explanation system could effectively explain intelligent agents' behavior, it could enable the end users to understand, assess, and appropriately trust (or distrust) the agents attempting to help them. To provide insights into this question, we turned to human expert explainers in the real-time strategy domain --""shoutcasters""-- to understand (1) how they foraged in an evolving strategy game in real time, (2) how they assessed the players' behaviors, and (3) how they constructed pertinent and timely explanations out of their insights and delivered them to their audience. The results provided insights into shoutcasters' foraging strategies for gleaning information necessary to assess and explain the players; a characterization of the types of implicit questions shoutcasters answered; and implications for creating explanations by using the patterns and abstraction levels these human experts revealed. The results of our study suggest that explaining intelligent agents to humans has much to gain from looking to the human experts. The expert explainers in our case — RTS shoutcasters — revealed implications into what, when, and how human audiences of such systems need explanations, and how realtime constraints can come together with explanation-building strategies. Among the results we learned were: RQ1 Investigating the what’s and where’s of casters’ realtime information foraging to assess and understand the players showed that the most commonly used patches of the information environment were the Actuators (“A” in the PEAS model). This suggests that explanation systems that currently support only Performance measures should consider also presenting information from the Actuators and Sensors. RQ2 The how’s of casters’ foraging revealed a common pattern, which we termed the A-E-P+S loop, and the most common cues and triggers that led shoutcasters to move through this loop. Future explanation systems may be wellserved to prioritize and recommend explanations according to this loop and its triggers. RQ3 As model explainer, the casters revealed strategies for “satisficing” with explanations that may not have precisely answered all the questions the audience had in mind, but were feasible given the time and resource constraints in effect when comprehending, assessing, and explaining, all in real time as play progresses. These strategies may be likewise applicable to interactive explanation systems. RQ4 The detailed contents of the casters’ explanations revealed patterns of how they paired properties (“adjectives and adverbs”) with different objects (“nouns”) and actions (“verbs”). Interactive explanation systems may be able to leverage these patterns to communicate succinctly about an agent’s tactics and strategies. Ultimately, both shoutcasters’ and explanation systems’ jobs are to improve the audience members’ mental model of the agents’ behavior. As Cheung, et al. [4] put it, “...commentators are valued for their ability to expose the depth of the game.” Hopefully, future explanation systems will be valued for the same reason.
"
A Study of Urban Heat: Understanding the Challenges and Opportunities for Addressing Wicked Problems in HCI,https://dl.acm.org/authorize?N657364,"
The Urban Heat Island Effect (UHI) is a phenomenon whereby cities tend to be hotter than suburbs. We frame the UHI as a ""wicked problem"" that poses a range of economic, healthcare, and social challenges. Our paper examines how different stakeholders negotiate complex value systems, collect data, and rely on collaborative platforms to address the problem of urban heat. Using documentary filmmaking as a research method, we conducted ethnographically-oriented interviews with participants including vulnerable communities, urban architects, microclimate researchers, and grassroots activists. Our findings reveal that unlike problems that can be solved using traditional HCI paradigms of distributed work, the UHI presents an entanglement of challenges that do not necessarily converge on a single solution. We conclude by discussing two opportunities for addressing wicked problems through social computing: knowledge systems for sharing hybrid data across domains and interactive forums for discourse among diverse actors. In this paper, we used site-specific interviews that followed a documentary filmmaking approach to investigate the UHI effect in Sydney, Australia. In a forthcoming manuscript, in-depth consideration will be given to filmmaking as a cinematic genre to understand how cinematic techniques can be used to capture nuanced aspects of participants’ lives. While methodology is not the primary contribution of this paper, we nevertheless reflected on how: filmmaking requires preparation and care to effectively elicit in-depth responses; delivers deep insights into people’s first-person perspectives; supports generative dialectic between participants; and entails new forms of accountability between stakeholders. The focus of this paper has been primarily to examine the practices, challenges, and approaches of different stakeholders working in the domain of urban heat. Our study reveals heat as a complex problem that is interlinked with issues beyond individual thermal comfort, and involves economic, climate change, social justice, and public healthcare concerns. The technologically-mediated entanglements between stakeholders, information flow, and actions do not converge on a singular solution to UHI. However, our findings suggest that CHI research can engage with wicked problems by developing: 1) knowledge systems for sharing hybrid data from personal sensors, scientific datasets, and first-hand observations; and 2) tools for supporting dialectic through larger-scale video collection, annotation, and sharing between stakeholders. Above all, we hope our work has contributed to HCI paradigms for holistically addressing the big challenges of our time.
"
It's a Wrap: Mapping On-Skin Input to Off-Skin Displays,https://dl.acm.org/authorize?N657375,"
Advances in sensing technologies allow for using the forearm as a touch surface to give input to off-skin displays. However, it is unclear how users perceive the mapping between an on-skin input area and an off-skin display area. We empirically describe such mappings to improve on-skin interaction. We collected discrete and continuous touch data in a study where participants mapped display content from an AR headset, a smartwatch, and a desktop display to their forearm. We model those mappings and estimate input accuracy from the spreads of touch data. Subsequently, we show how to use the models for designing touch surfaces to the forearm for a given display area, input type, and touch resolution. Mapping skin input to off-skin displays is a challenge for onskin interaction. Interaction can be improved by employing mappings that more closely match a user’s perception. By applying mappings that are consistent with a user’s perception, the content on the display can be interacted with touches on the forearm in an unambiguous manner. Our study revealed such mappings between on-skin input and off-skin displays. We collected touch data from participants mapping content from common display types to their arm, using both discrete and continuous touch input. Skin-based interface designs can benefit from the derived models.
"
Designing in the Dark: Eliciting Self-tracking Dimensions for Understanding Enigmatic Disease,https://dl.acm.org/authorize?N657376,"
The design of personal health informatics tools has traditionally been explored in self-monitoring and behavior change. There is an unmet opportunity to leverage self- tracking of individuals and study diseases and health conditions to learn patterns across groups. An open research question, however, is how to design engaging self-tracking tools that also facilitate learning at scale. Furthermore, for conditions that are not well understood, a critical question is how to design such tools when it is unclear which data types are relevant to the disease. We outline the process of identifying design requirements for self-tracking endometriosis, a highly enigmatic and prevalent disease, through interviews (N=3), focus groups (N=27), surveys (N=741), and content analysis of an online endometriosis community (1500 posts, N=153 posters) and show value in triangulating across these methods. Finally, we discuss tensions inherent in designing self-tracking tools for individual use and population analysis, making suggestions for overcoming these tensions. We contribute an understanding of the design of personal health informatics tools that leverage the self-tracking of individuals to better understand group-level characteristics of diseases. For diseases or health conditions where the relevant dimensions of the disease are not well understood, we outline a process for designing self-tracking tools in this context. We find there is value in examining different data sources to elicit the relevant dimensions of a disease. Triangulation across these sources informs what constitutes the experience of disease, indicating it is feasible to capture the disease experience directly from people with a particular health condition.

"
Examining Wikipedia With a Broader Lens: Quantifying the Value of Wikipedia's Relationships with Other Large-Scale Online Communities,https://dl.acm.org/authorize?N657377,"
The extensive Wikipedia literature has largely considered Wikipedia in isolation, outside of the context of its broader Internet ecosystem. Very recent research has demonstrated the significance of this limitation, identifying critical relationships between Google and Wikipedia that are highly relevant to many areas of Wikipedia-based research and practice. This paper extends this recent research beyond search engines to examine Wikipedia's relationships with large-scale online communities, Stack Overflow and Reddit in particular. We find evidence of consequential, albeit unidirectional relationships. Wikipedia provides substantial value to both communities, with Wikipedia content increasing visitation, engagement, and revenue, but we find little evidence that these websites contribute to Wikipedia in return. Overall, these findings highlight important connections between Wikipedia and its broader ecosystem that should be considered by researchers studying Wikipedia. Critically, our results also emphasize the key role that volunteer-created Wikipedia content plays in improving other websites, even contributing to revenue generation. In this paper, we presented results that identify and quantify relationships between Wikipedia and the large-scale communities Reddit and Stack Overflow. In general, we observe a one-way relationship in which Wikipediainfluenced content adds value to the external communities, but no evidence of substantial contributions in the reverse direction is observed. This research highlights the value of examining online communities using a broad lens, as crosscommunity relationships can have large effects.
"
Identifying Speech Input Errors Through Audio-Only Interaction,https://dl.acm.org/authorize?N657378,"
Speech has become an increasingly common means of text input, from smartphones and smartwatches to voice-based intelligent personal assistants. However, reviewing the recognized text to identify and correct errors is a challenge when no visual feedback is available. In this paper, we first quantify and describe the speech recognition errors that users are prone to miss, and investigate how to better support this error identification task by manipulating pauses between words, speech rate, and speech repetition. To achieve these goals, we conducted a series of four studies. Study 1, an in-lab study, showed that participants missed identifying over 50% of speech recognition errors when listening to audio output of the recognized text. Building on this result, Studies 2 to 4 were conducted using an online crowdsourcing platform and showed that adding a pause between words improves error identification compared to no pause, the ability to identify errors degrades with higher speech rates (300 WPM), and repeating the speech output does not improve error identification. We derive implications for the design of audio-only speech dictation. We reported on four studies to characterize and address the difficulty of identifying speech recognition errors when using audio-only speech input. Study 1 revealed that by listening to audio clips alone, users could identify less than half of the speech recognition errors. We then addressed the most common type of error that participants had missed in Study 1—errors where multiple words blended together— by inserting pauses between each word and varying speech rate in the audio output. The simple solution of inserting even a 1ms pause between words improved the ability to identify errors, while a fast speech rate made the task more difficult, and repeating the audio output had no effect. These findings have implications for speech-based text input for a variety of non-visual contexts, and an important avenue of future work will be to extend the investigation to accessibility for blind and visually impaired users.
"
Designing the Audience Journey through Repeated Experiences,https://dl.acm.org/authorize?N657379,"
We report on the design, premiere and public evaluation of a multifaceted audience interface for a complex non-linear musical performance called Climb! which is particularly suited to being experienced more than once. This interface is designed to enable audiences to understand and appreciate the work, and integrates a physical instrument and staging, projected visuals, personal devices and an online archive. A public premiere concert comprising two performances of Climb! revealed how the audience reoriented to the second performance through growing understanding and comparison to the first. Using trajectories as an analytical framework for the audience 'journey' made apparent: how the trajectories of a single performance are embedded within the larger trajectories of a concert and the creative work as a whole; the distinctive demands of understanding and interpretation; and the potential of the archive in enabling appreciation across repeated performances. Technically complex and non-linear interactive performances place particular demands on audiences in terms of appreciating what is going on and ultimately in arriving at some kind of personal interpretation. Our experience from Climb! reveals how supporting audience understanding is a multifaceted challenge that touches upon several aspects of interaction design. The key insight to emerge from our work is that experience designers (composers, performers and technicians) need to carefully design audience journeys – or trajectories – to shape an unfolding understanding of a work. This is true both within a given performance (considering what the audience experiences before, during and after the show) but also across many performances over the lifetime of the work (so that audiences can compare different performances, for example). Further complexities arise from the personal nature of interpretation which leads audience members to require various and different cues to scaffold their individual understandings, with these cues being spread across multiple interfaces, both projected and personal. We also note the potential of archives for capturing and making available supporting materials over the lifetime of a work, with archives being designed for audiences and with works perhaps becoming self-archiving. Finally, we highlight three areas for future work. First, we have identified several areas in which the current interfaces can be improved, including incorporating live video into the projection, adding more cues for challenges and more interpretative resources in the app. We aim to refine the current interfaces and to evaluate these refinements in a further round of performances. This will also provide the opportunity to begin to explore the impact of longer sequences of performances. Second, although beyond the current scope of Climb!, personal screens clearly have the potential to support audience interaction with works of this kind (and may even lead people to expect it). Previous HCI research has explored approaches such as cheering and applauding [1], voting [21], participating in real-time generation of the score [16], or suggesting moods that prompt the performers to improvise [13]. It would be valuable to address such interaction more explicitly within the audience journey. Third, the current archive doesn’t yet support the kind of personalised storytelling that was reported in [12], e.g., the combination of official documentation with people’s own media using templates. This might also extend to allowing people to weave their own photos, videos and comments into the archive, similar to examples of crowd sourcing rich media associated with music concerts [33] and marathon races [15]. And specialist versions of the archive might integrate and augment the digital score as a key facet of the experience.

"
Printed Paper Actuator: A Low-cost Reversible Actuation and Sensing Method for Shape Changing Interfaces,https://dl.acm.org/authorize?N657370,"
We present a printed paper actuator as a low cost, reversible and electrical actuation and sensing method. This is a novel but easily accessible enabling technology that expands upon the library of actuation-sensing materials in HCI. By integrating three physical phenomena, including the bilayer bending actuation, the shape memory effect of the thermoplastic and the current-driven joule heating via conductive printing filament, we developed the actuator by simply printing a single layer conductive Polylactide (PLA) on a piece of copy paper via a desktop fused deposition modeling (FDM) 3D printer. This paper describes the fabrication process, the material mechanism, and the transformation primitives, followed by the electronic sensing and control methods. A software tool that assists the design, simulation and printing toolpath generation is introduced. Finally, we explored applications under four contexts: robotics, interactive art, entertainment and home environment. In this paper, we present a paper actuator, which is a composite material printed by desktop FDM 3D printers. While many approaches to actuating papers have been introduced before, the main contribution of our paper is the design of the composite, which seems simple but indeed combines three physical phenomena: electrical resistive heating of conductive thermoplastic, shape memory effect, and bi-layer actuation. We hope to introduce our paper actuator as a low cost and easy-to-fabricate enabling material to the community.

"
Leveraging Semantic Transformation to Investigate Password Habits and Their Causes,https://dl.acm.org/authorize?N657371,"
It is no secret that users have difficulty choosing and remembering strong passwords, especially when asked to choose different passwords across different accounts. While research has shed light on password weaknesses and reuse, less is known about user motivations for following bad password practices. Understanding these motivations can help us design better interventions that work with the habits of users and not against them. We present a comprehensive user study in which we both collect and analyze users' real passwords and the reasoning behind their password habits. This enables us to contrast the users' actual behaviors with their intentions. We find that user intent often mismatches practice, and that this, coupled with some misconceptions and convenience, fosters bad password habits. Our work is the first to show the discrepancy between user intent and practice when creating passwords, and to investigate how users trade off security for memorability. In theory, good password hygiene and risk management are straightforward: strong, unique passwords for all accounts, but especially for more important ones. However, the proliferation of accounts, weak password policies, and difficulty remembering all of these passwords make good password behaviors hard to implement in practice. Throughout this research, we have observed that users’ security perceptions and intent rarely match their security realities. Some reasons for this lie in misconceptions about risk and a desire for convenience, identified by other researchers. But another large reason, uncovered by our research, lies in the sheer complexity of managing many accounts over a large time span – a task that is cognitively hard for humans. We have recommended development of tools that reduce this cognitive load and identify cases where password sharing increases user risk. Our future research will investigate whether this kind of intervention can measurably improve user password strategies.

"
Impact Activation Improves Rapid Button Pressing,https://dl.acm.org/authorize?N657372,"
The activation point of a button is defined as the depth at which it invokes a make signal. Regular buttons are activated during the downward stroke, which occurs within the first 20 ms of a press. The remaining portion, which can be as long as 80 ms, has not been examined for button activation for reason of mechanical limitations. The paper presents a technique and empirical evidence for an activation technique called Impact Activation, where the button is activated at its maximal impact point. We argue that this technique is advantageous particularly in rapid, repetitive button pressing, which is common in gaming and music applications. We report on a study of rapid button pressing, wherein users' timing accuracy improved significantly with use of Impact Activation. The technique can be implemented for modern push-buttons and capacitive sensors that generate a continuous signal. For this paper, we defined and empirically investigated an activation technique called Impact Activation. A user study showed IA to be superior to the traditional button activation methods for all performance metrics in a fast tapping task: IA had higher success rates, less asynchrony, and less variance (see figures 6 and 7). The performance gains were greater in the physical button condition than in the touch button condition. The benefit of IA is likely to be attributable to the stronger stimulation of the fingertip allowing the motor system to calibrate its outputs better in the fast, episodic tapping task. We hypothesize that IA is closer to users’ perceived activation point than a traditional button activation point is. The activation point design choice had a much greater effect on the mean of asynchrony than on its standard deviation. To the best of our knowledge, the design of buttons has not been approached before by exploiting findings from theories of temporal motor control. Our work may reveal a new opportunity to understand why user performance has been much worse with touchscreen devices than physical input devices. In HCI research, work on touch input has focused, for example, on the “fat finger problem” [28] and system latency [6, 35]. Techniques such as IA that are motivated by motor control literature may complement the gains achieved in these areas. Furthermore, it could address well-known performance impairments reported for touchscreen gaming [3, 36, 37]. We see several opportunities to build on this work. First, our results are limited to a particular modality combination: audio cue with visual feedback. We believe the benefit of IA will persist with other modalities, but it may be smaller. This remains a topic for future work. Also, our experimental task was relatively short, so there may be some quickly obtained advantages to using IA, particularly for novice users. To assess the robustness of the effect, a longitudinal study should be carried out. Longitudinal research would expose possible recalibration effects [30]. Finally, our experiment tested two specific button designs. Although these represent commonly used types, we see it as worthwhile to replicate the work with other button designs.

"
MindNavigator: Exploring the Stress and Self-Interventions for Mental Wellness,https://dl.acm.org/authorize?N657373,"
Mental wellness is a desirable health outcome for students. However, current personal informatics systems do not adequately support students in creating concrete mental health-related goals and turning them into actionable plans. In this paper, we introduce MindNavigator - a workshop in which groups of college students were invited to generate behavioral change goals to manage daily life stress and practice personalized interventions for two weeks. We describe the manner in which participants identified both stressors and pleasures to create actionable, engaging, and open-ended behavioral plans that aided in stress relief. We found that the social nature of the workshop helped participants understand themselves and execute self-intervention in new ways. Through this practice, we build on prior studies to propose an analytical framework of personal informatics for mental wellness. We propose the MindNavigator—an approach to exploring the landscape of students’ stress and implementing personalized interventions for stress management through an engagement workshop. Our work provides insights into the landscape of students’ stress, including their stressors, stress-relievers, goalsetting strategies, and goal-revision strategies. By illuminating how students manage stress with personalized interventions and describing the results using an analytical framework, we can better inform the design of PI systems for mental wellness. In our future work, we will implement our approach on a computing platform and will examine the long-term impact of MindNavigator on stress.

"
"""Only if you use English you will get to more things"": Using Smartphones to Navigate Multilingualism",https://dl.acm.org/authorize?N657374,"
We contribute to the intersection of multilingualism and human-computer interaction (HCI) with our investigation of language preferences in the context of the interface design of interactive systems. Through interview data collected from avid smartphone users located across distinct user groups in India, none of whom were native English speakers, we examine the factors that shape language choice and use on their mobile devices. Our findings indicate that these users frequently engage in English communication proactively and enthusiastically, despite their lack of English fluency, and we detail their motivations for doing so. We then discuss how language in technology use can be a way of putting forth mobility as an aspect of one's identity, making the case for an intersectional approach to studying language in HCI. We studied multilingualism in smartphone use by Indian users for whom English was not a native language. Through data collected from five sites across India, we offered an enriched understanding of the factors that shape language preference on smartphones. We found that participants used English, even when they were more fluent in another language. We found that this preference was tied to mobility depending on participants’ backgrounds, and described how English-based smartphones helped or hindered participants’ use of language. We concluded with the merits of examining language through an intersectional lens for the design of multilingual interactive systems. We surmise that conducting future work in contexts where other languages take the place of English, such as Mandarin Chinese in China or Afrikaans in South Africa, could lead to further, important insights, given the paucity of research in the domain.
"
The Problem of Community Engagement: Disentangling the Practices of Municipal Government,https://dl.acm.org/authorize?N657385,"
In this paper, we work to inform the growing space of Digital Civics with a qualitative study of community engagement practices across the breadth of municipal departments and agencies in a large US city. We conducted 34 inter-views across 15 different departments, including elected and professional city employees to understand how different domains within local government define and practice the work of engaging residents. Our interviews focused on how respondents conceptualized community engagement, how it fit into the other forms of work, and what kinds of outcomes they sought when they did 'engagement.' By reporting on this broad qualitative account of the many forms the work of community engagement takes in local government, we are contributing to an expansive view of digital civics that looks beyond the transactions of service delivery or the privileged moments of democratic ritual, to consider the wider terrain of mundane, daily challenges when trying to bridge between municipal government and city residents. The empirical fieldwork we have reported here provides a characterization of the variety of practices that compose community engagement in a large municipality. Rather than a narrower focus on the practices of engagement in a single domain or a specific department, we took an expansive view in order to better understand and compare the breadth of work occurring across the range of functional elements of municipal government. Doing so contributes to a growing literature that takes an expansive view of civic interactions, looking beyond moments of rational deliberation, or service transaction. As the characteristics of engagement practices change between departments and projects so too do the needs and affordances of technology. For instance, digital civic interventions to support the goal of increasing institutional accessibility of public works require different affordances that a similar goal being pursued in public schools—the access points are different as is the meaning of community engagement. As others have pointed out, these competing logics and practices create unique challenges when working with public civic entities. By better understanding the practices of community engagement, digital civic interventions can be situated to the responsibilities of civic authorities as well as afford more productive participation from residents. However, this begs the questions of whether digital civics should provide tools and support for the work of community engagement as is—responding to user need, such as it were? Or, do we design tools that reflect what we expect of civic authorities and residents alike in the work of governance? We pose these questions as a way to reflect what we, as an intellectual community, aspire to as we experiment with systems that have real consequences both for the people who use them and the people for whom they work.
"
Using Co-Design to Examine How Children Conceptualize Intelligent Interfaces,https://dl.acm.org/authorize?N657386,"
Prior work has shown that intelligent user interfaces (IUIs) that use modalities such as speech, gesture, and writing pose challenges for children due to their developing cognitive and motor skills. Research has focused on improving recognition and accuracy by accommodating children's specific interaction behaviors. Understanding children's expectations of IUIs is also important to decrease the impact of recognition errors that occur. To understand children's conceptual model of IUIs, we completed four consecutive participatory design sessions on designing IUIs with an emphasis on error detection and correction. We found that, while children think of interactive systems in terms of both user input and behavior and system output and behavior, they also propose ideas that require advanced system intelligence, e.g., context and conversation. Our work contributes new understanding of how children conceptualize IUIs and new methods for error detection and correction, and will inform the design of future IUIs for children to improve their experience. We conducted four consecutive Cooperative Inquiry design sessions with a group of seven children that focused on designing IUIs with an emphasis on error correction. We constructed a conceptual model based on children’s understanding of IUIs and found that the children conceptualize IUIs as whole systems and expect advanced system intelligence. DS 4 (Big Props) elicited the richest themes, while interviewing the children and observing them interact with technology failed to elicit the same depth as the PD activities. We present new design recommendations for error correction for children that are aligned with their conceptual model. By integrating our work on understanding how children conceptualize IUIs with prior work on techniques for improving accuracy, we can design better IUI technology that is tailored towards children.
"
Rethinking Engagement with Online News through Social and Visual Co-Annotation,https://dl.acm.org/authorize?N657387,"
The emergence of fake news, as well as filter bubbles and echo chambers, has precipitated renewed attention upon the ways in which news is consumed, shared and reflected and commented upon. While online news comments sections offer space for pluralist and critical discussion, studies suggest that this rarely occurs. Motivated by common practices of annotating, defacing and scribbling on physical newspapers, we built a mobile app -- Newsr -- that supports co-annotation, in the form of graffiti, on online news articles, which we evaluated in-the-wild for one month. We report on how the app encouraged participants to reflect upon the act of choosing news stories, whilst promoting exploration, the critique of content, and the exposure of bias within the writing. Our findings highlight how the re-design of interactive online news experiences can facilitate more directed, ""in-the-moment"" critique of online news stories as well as encourage readers to expand the range of news content they read. We report on the design of a mobile app, Newsr, which encourages readers to interact critically with news stories drawn from both familiar and new sources of news. We conducted an evaluation of the app in-the-wild over a period of 4 weeks drawing upon current news stories. Our findings are based upon qualitative analysis of data gathered form interviews and focus groups with 15 users. Our work contributes to knowledge on critical engagement with news looking primarily at how users can annotate news stories in new ways. This also contributes to work on the social annotation of news, work that explores and challenges filter bubbles. Our findings provide a set of implications for the design of news reading applications, that can be applied to designs and research that addresses how readers think about news sources, and in work that explores new ways to interact with the news. These design implications are presented in terms of challenges in such interfaces, challenges we argue, encourage critical reflection. We have already discussed how the curation of stories will affect user responses, and acknowledge our process selected stories around ‘othering’. As has been demonstrated, good content selection for presentation to users is key to engagement and reflection, and considering existing journalistic practice this is somewhat unsurprising as all news sources are edited and curated to evoke reactions from readers. Therefore, this is a limitation of our study, and future work in this area may wish to explore content selection from a more diverse range of stories. In this paper we have looked at two modalities: the swipeable interface for choosing stories, and annotation through graffiti. We believe these are complementary. Together, they push readers to explore new sources as the graffiti itself becomes a draw to the article. The swiping interaction is quick, and together with the piecemeal annotation of news (both in the type of content and how content is annotated), allow annotation over time, making Newsr a better fit with reading with your pen and reading on the move.
"
Season Traveller: Multisensory Narration for Enhancing the Virtual Reality Experience,https://dl.acm.org/authorize?N657388,"
In the same way that we experience the real-world through a range of senses, experiencing a virtual environment through multiple sensory modalities may augment both our presence within a scenario and our reaction to it. In this paper, we present Season Traveller, a multisensory virtual reality (VR) narration of a journey through four seasons within a mystical realm. By adding olfactory and haptic (thermal and wind) stimuli, we extend traditional audio-visual VR technologies to achieve enhanced sensory engagement within interactive experiences. Using both subjective measures of presence and elicited physiological responses, we evaluated the impact of different modalities on the virtual experience. Our results indicate that 1) the addition of any singular modality improves sense of presence with respect to traditional audio-visual experiences and 2) providing a combination of these modalities produces a further significant enhancement over the aforementioned improvements. Furthermore, insights into participants' psychophysiology were extrapolated from electrodermal activity (EDA) and heart rate (HR) measurements during each of the VR experiences. In summary, the research presented in this paper is one of the first systematic investigations into studying the effects of multisensory stimuli when integrated within an HMD as a way of enhancing sense of presence in VR experiences. Multiple preliminary studies were conducted to finalise the smell stimuli regarding the delivery method, intensity, comfort, relaxation, and naturalness. The main study was performed to evaluate the effectiveness of multisensory stimuli in VR and highlighted significant improvements in scores for sensory factors, engagement, richness, and involvement when compared to traditional audio-visual VR experiences. Additionally, our analysis based on psychophysiology measurements indicates opportunities for future work to effectively compare different factors contributing to participants’ sense of presence in multisensory VR.

"
Playing with Streakiness in Online Games: How Players Perceive and React to Winning and Losing Streaks in League of Legends,https://dl.acm.org/authorize?N657389,"
Streakiness refers to observed tendency towards consecutive appearances of particular patterns. In video games, streakiness is oftentimes inevitable, where a player keeps winning or losing for a short period. However, the phenomenon remains understudied in present online game research. How do players perceive streakiness? How does it impact player experience (PX)? How should streakiness be taken into consideration for the design of PX? In this paper, we address these questions through a qualitative study of player discussions about streakiness in League of Legends. We found that players developed various ways to describe a streak. Both winning and losing streaks negatively impacted PX. Players devised numerous strategies to manage streakiness, among which disengagement was a primary means. We analyze streakiness as a social construct through which players coped with complex game systems. We discuss design implications for managing streakiness in online games. In this paper, we studied PX with streakiness in League of Legends. Players did not perceive streakiness as a manifestation of system randomness. Our study of the two online forums revealed how they collectively developed various explanations and coping strategies which they claimed to be effective. By analyzing player online discussions as sensemaking, we highlight the socially constructed aspect of PX where players’ own language and values are at play in interpreting, explaining, and enhancing their lived experience with gameplay. We call for more interpretive approaches in PX research.
"
RoMA: Interactive Fabrication with Augmented Reality and a Robotic 3D Printer,https://dl.acm.org/authorize?N657380,"
We present the Robotic Modeling Assistant (RoMA), an interactive fabrication system providing a fast, precise, hands-on and in-situ modeling experience. As a designer creates a new model using RoMA AR CAD editor, features are constructed concurrently by a 3D printing robotic arm sharing the same design volume. The partially printed physical model then serves as a tangible reference for the designer as she adds new elements to her design. RoMA's proxemics-inspired handshake mechanism between the designer and the 3D printing robotic arm allows the designer to quickly interrupt printing to access a printed area or to indicate that the robot can take full control of the model to finish printing. RoMA lets users integrate real-world constraints into a design rapidly, allowing them to create well-proportioned tangible artifacts or to extend existing objects. We conclude by presenting the strengths and limitations of our current design. We presented RoMA, an interactive fabrication system with an on-site and hands-on modeling experience. As a designer creates a new design using the AR CAD editor, features are constructed by a robotic arm on the shared printing platform. The designer can rotate the platform, and use the partially printed physical model as a tangible reference for further design. RoMA enables the designer to integrate real-world constraints into a design rapidly and intuitively and allows the designer to directly design and print on and around a physical object.

"
Visualizing API Usage Examples at Scale,https://dl.acm.org/authorize?N657381,"
Using existing APIs properly is a key challenge in programming, given that libraries and APIs are increasing in number and complexity. Programmers often search for online code examples in Q&A forums and read tutorials and blog posts to learn how to use a given API. However, there are often a massive number of related code examples and it is difficult for a user to understand the commonalities and variances among them, while being able to drill down to concrete details. We introduce an interactive visualization for exploring a large collection of code examples mined from open-source repositories at scale. This visualization summarizes hundreds of code examples in one synthetic code skeleton with statistical distributions for canonicalized statements and structures enclosing an API call. We implemented this interactive visualization for a set of Java APIs and found that, in a lab study, it helped users (1) answer significantly more API usage questions correctly and comprehensively and (2) explore how other programmers have used an unfamiliar API. Code examples are a key learning resource when learning unfamiliar APIs. Current tools for searching and browsing code examples often produce large collections of code examples that developers only have limited time and attention to review. In this paper, we introduce the concept of a synthetic code skeleton, which summarizes a variety of API usage features from a collection of code examples simultaneously in a single view. EXAMPLORE instantiates the synthetic code skeleton with statistical distributions and allows a user to drill down to individual concrete code examples mined from 380K GitHub repositories. We conducted a within-subjects study with sixteen Java programmers and found that participants could answer more API usage questions correctly, with more detail and confidence, when using EXAMPLORE compared to searching for usage examples online. Many of these developers could envision EXAMPLORE fitting into their development workflows, helping them explore unfamiliar APIs. In future work, we hope to extend the code skeleton to support different programming languages and allow multiple related API methods to be the focal point of our visualization.

"
A Multi-site Investigation of Community Awareness Through Passive Location Sharing,https://dl.acm.org/authorize?N657382,"
Local community ties are an important social resource, but research shows that these ties have been declining. The social significance of location information offers an opportunity address this decline and support local community building. Through this research, we aim to understand if and how passive location sharing might be socially beneficial for communities. We conducted a deployment of MoveMeant, a location awareness app, across three different communities. Following a research through design approach, we conducted 45 interviews with users of the system and community leaders. The findings suggest that communities face issues related to lack of awareness, cohesion, and identity. We show that the app can help increase awareness of important community resources. At the same time, the findings also show a negative effect of surfacing divisions in a community, which we discuss as a intermediate, perceptual step that may contribute to the amplification effect of technology. This work described the deployment of MoveMeant, a community app that uses anonymized and aggregated location information for network-to-person communication. Across three field sites and interviews with 45 community members and leaders, we show how the information in the app engaged with the issues of awareness, cohesion, and community identity. We synthesized our findings to propose surfacing, the effect of technology to make differences within a community more salient, as an intermediate step towards amplification. We discussed how the information could potentially be used by community leaders as a tool for political action. Other organizations are beginning to use such personal data for social benefit. For example, Decode4 is a consortium of different organizations across the European Union that is exploring how people might use their own data traces for the good of the wider community. Our work suggests the promise behind such efforts in increasing awareness, but also the potential danger of unintentionally surfacing distinctions within the community at the same time. Like architecture, data is given meaning by the way that people use it. More and more data is being collected and efforts taken to make that data available to the public. Taking into account the potential unintended effects of sharing data is a concern for the future that we as designers and researchers should acknowledge and better understand.

"
"Trends and Trajectories for Explainable, Accountable and Intelligible Systems: An HCI Research Agenda",https://dl.acm.org/authorize?N657383,"
Advances in artificial intelligence, sensors and big data management have far-reaching societal impacts. As these systems augment our everyday lives, it becomes increasing-ly important for people to understand them and remain in control. We investigate how HCI researchers can help to develop accountable systems by performing a literature analysis of 289 core papers on explanations and explaina-ble systems, as well as 12,412 citing papers. Using topic modeling, co-occurrence and network analysis, we mapped the research space from diverse domains, such as algorith-mic accountability, interpretable machine learning, context-awareness, cognitive psychology, and software learnability. We reveal fading and burgeoning trends in explainable systems, and identify domains that are closely connected or mostly isolated. The time is ripe for the HCI community to ensure that the powerful new autonomous systems have intelligible interfaces built-in. From our results, we propose several implications and directions for future research to-wards this goal. Recent advances in machine learning and artificial intelligence have far-reaching impacts on society at large. While researchers in the ML and AI communities are working on making their algorithms explainable, their focus is not on usable, practical and effective transparency that works for and benefits people. Given HCI’s core interest in technology that empowers people, this is a gap that we as a community can help to address, to ensure that these new and powerful technologies are designed with intelligibility from the ground up. From a literature analysis of 12,412 papers citing 289 core papers on explainable systems, we mapped the research space from diverse domains related to explainable systems. We revealed fading vs. burgeoning trends and connected vs. isolated domains, and from this, extracted several implications, future directions and opportunities for HCI researchers. While this is only a first step, we argue that true progress towards explainable systems can only be made through interdisciplinary collaborations, where expertise from different fields (e.g., machine learning, cognitive psychology, humancomputer interaction) is combined and concepts and techniques are further developed from multiple perspectives to move research forward.

"
TopicOnTiles: Tile-Based Spatio-Temporal Event Analytics via Exclusive Topic Modeling on Social Media,https://dl.acm.org/authorize?N657384,"
Detecting anomalous events of a particular area in a timely manner is an important task. Geo-tagged social media data are useful resource for this task; however, the abundance of everyday language in them makes this task still challenging. To address such challenges, we present TopicOnTiles, a visual analytics system that can reveal information relevant to anomalous events in a multi-level tile-based map interface by using social media data. To this end, we adopt and improve a recently proposed topic modeling method that can extract spatio-temporally exclusive topics corresponding to a particular region and a time point. Furthermore, we utilize a tile-based map interface to efficiently handle large-scale data in parallel. Our user interface effectively highlights anomalous tiles using our novel glyph visualization that encodes the degree of anomaly computed by our exclusive topic modeling processes. To show the effectiveness of our system, we present several usage scenarios using real-world datasets as well as comprehensive user study results. This paper presented a tile-based visual analytics system TopicOnTiles for anomalous event detection using geo-tagged social media data. TopicOnTiles is mainly built upon a tilebased map interface using the novel topic modeling technique that extracts spatio-temporally exclusive topics with respect to the neighboring tiles. TopicOnTiles also provides various visual encodings such as the glyphs, vertical grids, and heatmaps to facilitate anomalous event detection tasks. We showed usage scenarios using Twitter data from New York City, where our system effectively reveals the event of the 2013 ING NYC Marathon and the Trayvon Martin protest. Furthermore, we conducted user studies showing the efficiency and the usability of TopicOnTiles compared to other baseline settings. As our future work, we plan to extend our visual system to a real-time monitoring system that can solve various problems arising in urban areas such as natural disasters, crimes, and so on, by associating the geo-tagged textual social media data with other types of geo-tagged data, such as mobile user data.

"
"In a New Land: Mobile Phones, Amplified Pressures and Reduced Capabilities",https://dl.acm.org/authorize?N657395,"
Framed within the theoretical lens of positive and negative security, this paper presents a study of newcomers to Sweden and the roles of mobile phones in the establishment of a new life. Using creative engagement methods through a series of workshops, two researchers engaged 70 adult participants enrolled into further education colleges in Sweden. Group narratives about mobile phone use were captured in creative outputs, researcher observations and notes and were analysed using thematic analysis. Key findings show that the mobile phone offers security for individuals and a safe space for newcomers to establish a new life in a new land as well as capitalising on other spaces of safety, such as maintaining old ties. This usage produces a series of threats and vulnerabilities beyond traditional technological security thinking related to mobile phone use. The paper concludes with recommendations for policies and support strategies for those working with newcomers. In reducing some of the challenges to social isolation and claiming some control over how and where integration into society takes place, mobile phone use contributes to and shapes a newcomer’s sense of freedom. These freedoms are, however, fragile as the participant narratives in this study express. The everyday pressures for newcomers amplify some of the disadvantages of constant and intense mobile phone use and potentially renders expected methods of managing mobile phone use weaker. Mobile phone usage techniques that serve to strengthen these freedoms should be encouraged through guidance and also be complemented by specific techniques that, whilst attentive to the need for developing new freedoms, minimise potential harms of a close, constant and intimate relationship with the device.

"
Lessons from the Woodshop: Cultivating Design with Living Materials,https://dl.acm.org/authorize?N657396,"
This paper describes an eighteen-month ethnography of timber framing at a tiny house construction program in Port Townsend, Washington. This case exposes the intricate, ongoing processes that define a project where people learn to imagine, create, and ultimately maintain living materials. This case sheds light on the nature and scope of interaction design with living materials, an area of growing significance to HCI scholarship on new materials, sustainable design, and digital fabrication. Drawing from this project, we distill five lessons for design with living, finite materials. We end by discussing three emerging areas for HCI: designing for material recuperation, collaborating with more-than-human actors, and approaching material properties as prototyping sites. HCI’s interest in living, changing materials both in theory and in application has brought new challenges for designers. Using timber framing as a lens, we have shown how centering the living qualities of materials — grappling with materials as non-human design collaborators — decenters designers and situates them in longer material histories that extend from traces of past encounters into future forms. In the woodshop we learned how design activity takes place alongside and through forces of decay and resurgence where new fabrication and resource scarcity meet, highlighting five characteristics of living materials that interrogate contemporary design practice: legible textures, defensive traces, reparative expressions, vital decay, and performative scarcity. We also learned that cultivating a design practice with these living material characteristics requires multiple methodological reorientations. The first is a move away from treating materials as passively awaiting designer intervention in order to see agency in action; looking more closely we find materials have their own lives that extend before and after the design encounter. In this approach, materials and non-human forces become active collaborators in design, and the human designer’s touch is just one of many important meeting points from the material’s historically grounded perspective. Second is a move away from treating Nature as somehow separate from HCI practices or as a boundless source of raw materials and fabrication models; instead it suggests turning toward mutually constituted technology development pursuits that inhabit the messy intersections of ecological and industrial processes. When viewed as alive alongside humans, materials enact resource scarcity and limitations as central design engagements requiring recognition of the rhythms of ecological growth and decay for which HCI practices are partially responsible. Lastly, our work expands a program of work on computational composites and digital material within HCI by exploring properties as temporary alignments between material proclivities, tools, and meanings rather than fixed attributes of the material to be manipulated. The designer’s task, we argued, is to work alongside living materials, exploring their possibilities and crafting more sustainable relationships in partnership with non-human collaborators.
"
Squadbox: A Tool to Combat Email Harassment Using Friendsourced Moderation,https://dl.acm.org/authorize?N657397,"
Communication platforms have struggled to provide effective tools for people facing harassment online. We conducted interviews with 18 recipients of online harassment to understand their strategies for coping, finding that they often resorted to asking friends for help. Inspired by these findings, we explore the feasibility of friendsourced moderation  as a technique for combating online harassment. We present Squadbox, a tool to help recipients of email harassment coordinate a ""squad"" of friend moderators to shield and support them during attacks. Friend moderators intercept email from strangers and can reject, organize, and redirect emails, as well as collaborate on filters. Squadbox is designed to let its users implement highly customized workflows, as we found in interviews that harassment and preferences for mitigating it vary widely. We evaluated Squadbox on five pairs of friends in a field study, finding that participants could comfortably navigate around privacy and personalization concerns. In this work, we study the emergent practices of recipients of online harassment, finding from 18 interviews that many harassment recipients rely on friends and family to shield themselves from harassing messages. Building on this strategy, we propose friendsourced moderation as a promising technique for anti-harassment tools. We developed Squadbox, a tool to help harassment recipients coordinate a squad of friends to moderate aspects of their email. From a field study, we found that the use of friends as moderators simplifies issues surrounding privacy and personalization but also presents challenges for relationship maintenance.
"
Hoarding and Minimalism: Tendencies in Digital Data Preservation,https://dl.acm.org/authorize?N657398,"
Digital data, from texts to files and mobile applications, has become a pervasive component of our society. With seemingly unlimited storage in the cloud at their disposal, how do people approach data preservation, deciding what to keep and discard? We interviewed 23 participants with diverse backgrounds, asking them about their perceived digital data: what ""stuff"" they kept through the years, why, how they used it, and what they considered important. In an iterative analysis process, we uncovered a spectrum of tendencies that drive preservation strategies, with two extremes: hoarding (where participants accumulated large amounts of data, even if considered of little value) and minimalism (where they kept as little as possible, regularly cleaning their data). We contrast and compare the two extremes of the spectrum, characterize their nuanced nature, and discuss how our categorization compares to previously reported behaviors such as filing and piling, email cleaners and keepers. We conclude with broad implications for shaping technology. We have shown how participants approached digital data preservation driven by a spectrum of underlying tendencies with two extremes: hoarding (where they accumulated large amounts of data, sometimes considered useless, experiencing in some cases challenges with managing it) and minimalism (where they tried to keep as little as possible, preventing or reacting to data as a way to be in control of it). There was nuance and variation within individuals, but tendencies close to both extremes of the spectrum appeared to be a way for participants to build their own identity in relation to data (I have data therefore I am vs. I am more than my data). The contribution and value of our work lies in: 1) bringing to light a spectrum of tendencies with hoarding and minimalism on two ends, characterizing them in depth, 2) comparing and contrasting different user behaviours, showing their common role for identity construction, 3) putting them in context compared to previously reported behaviors in the literature. These findings move forward our understanding of how people preserve digital data, a generally under-unexplored topic. Furthermore, they have broad implications for shaping technology, opening rich possibilities for future work. Now that we are in the foothills of a new world where seductive cloud storage is pervasive, it is critical to understand what drives people’s behaviors so that we can shape this world in a way that promotes informed decisions and well-being.

"
Mercury: A Messaging Framework for Modular UI Components,https://dl.acm.org/authorize?N657399,"
In recent years, the entity--component--system pattern has become a fundamental feature of the software architectures of game-development environments such as Unity and Unreal, which are used extensively in developing 3D user interfaces. In these systems, UI components typically respond to events, requiring programmers to write application-specific callback functions. In some cases, components are organized in a hierarchy that is used to propagate events among vertically connected components. When components need to communicate horizontally, programmers must connect those components manually and register/unregister events as needed. Moreover, events and callback signatures may be incompatible, making modular UIs cumbersome to build and share within or across applications. To address these problems, we introduce a messaging framework, Mercury, to facilitate communication among components. We provide an overview of Mercury, outline its underlying protocol and how it propagates messages to responders using relay nodes, describe a reference implementation in Unity, and present example systems built using Mercury to explain its advantages. We have presented Mercury, a hierarchical communication framework for software components, and described its specification, several use cases, motivations for its evolution, advantages and limitations of its design, and a reference implementation in Unity. Mercury allows programmers to achieve complex routing behavior easily, while focusing almost entirely on how software components (both local and remote) respond to messages.

"
Characterizing Finger Pitch and Roll Orientation During Atomic Touch Actions,https://dl.acm.org/authorize?N657390,"
Atomic interactions in touch interfaces, like tap, drag, and flick, are well understood in terms of interaction design, but less is known about their physical performance characteristics. We carried out a study to gather baseline data about finger pitch and roll orientation during atomic touch input actions. Our results show differences in orientation and range for different fingers, hands, and actions, and we analyse the effect of tablet angle. Our data provides designers and researchers with a new resource to better understand what interactions are possible in different settings ( e.g.  when using the left or right hand), to design novel interaction techniques that use orientation as input (e.g. using finger tilt as an implicit mode), and to determine whether new sensing techniques are feasible (e.g. using fingerprints for identifying specific finger touches). The performance characteristics of atomic touch interactions, such as tap, drag, scale, rotation and flick, have been extensively studied. However, less is known regarding how they are carried out by users. In this paper we studied the natural pitch and roll orientation of all ten fingers while performing such actions. We used an IMU in a low cost and easily reproducible setup to accurately measure finger orientation. Our results provide a set of baselines about pitch and roll orientation for all the fingers of both hands for one setting (a flat tablet in front of the user). We found that for a given hand, the little, ring and middle fingers are used in a similar manner, whereas the thumb uses different range of orientations. Additional analyses about how changing the angle of the tablet affects people’s finger orientations suggest that ranges of orientation tighten as the tablet pitch increases. Our data provides designers and researchers with a new resource to better understand the use of pitch and roll as new degrees of freedom (e.g. using finger pitch as a secondary mode) and to determine whether new sensing techniques are feasible (e.g. using fingerprints for identifying specific finger touches).
"
Extending Manual Drawing Practices with Artist-Centric Programming Tools,https://dl.acm.org/authorize?N657391,"
Procedural art, or art made with programming, suggests opportunities to extend traditional arts like painting and drawing; however, this potential is limited by tools that conflict with manual practices. Programming languages present learning barriers and manual drawing input is not a first class primitive in common programming models. We hypothesize that by developing programming languages and environments that align with how manual artists work, we can build procedural systems that enhance, rather than displace, manual art. To explore this, we developed Dynamic Brushes, a programming and drawing environment motivated by interviews with artists. Dynamic Brushes enables the creation of ad-hoc drawing tools that transform stylus inputs to procedural patterns. Applications range from transforming individual strokes to behaviors that draw multiple strokes simultaneously, respond to temporal events, and leverage external data. Results from an extended evaluation with artists provide guidelines for learnable, expressive systems that blend manual and procedural creation. Motivated by conversations with artists, we created Dynamic Brushes, a visual programming and drawing environment for blending manual and procedural production through personal tool creation. Our evaluation demonstrated that tool development can be engaging for manual artists while providing opportunities to extend their established practice and style. We see future research opportunities in developing mechanisms to support artists in visualizing, recording and inspecting drawing behaviors, and experimenting with different brush primitives to support procedural control over brush textures and physics. Overall, we are excited about the potential of domain-specific programming languages to foster broader participation in creative system development.
"
Sensing Interruptibility in the Office: A Field Study on the Use of Biometric and Computer Interaction Sensors,https://dl.acm.org/authorize?N657392,"
Knowledge workers experience many interruptions during their work day. Especially when they happen at inopportune moments, interruptions can incur high costs, cause time loss and frustration. Knowing a person's interruptibility allows optimizing the timing of interruptions and minimize disruption. Recent advances in technology provide the opportunity to collect a wide variety of data on knowledge workers to predict interruptibility. While prior work predominantly examined interruptibility based on a single data type and in short lab studies, we conducted a two-week field study with 13 professional software developers to investigate a variety of computer interaction, heart-, sleep-, and physical activity-related data. Our analysis shows that computer interaction data is more accurate in predicting interruptibility at the computer than biometric data (74.8% vs. 68.3% accuracy), and that combining both yields the best results (75.7% accuracy). We discuss our findings and their practical applicability also in light of collected qualitative data. In this paper, we presented the results of a two-week field study with 13 professional software developers in which we examined the use of a wide variety of biometric and computer interaction features to predict interruptibility. Our analysis shows that we are able to predict interruptibility at the computer with 75.3% accuracy (a 26.6% improvement over the baseline) and that computer interaction features are more accurate than the biometric ones (74.8% vs. 68.3%). We further show that the best time windows to extract features vary across feature categories and that certain features can affect interruptibility over long periods of time. Finally, we show that even a generally trained model can accurately predict interruptibility for new subjects to overcome the cold start problem, and that even small sets of samples can be used to rapidly improve the classifier. As a next step, we plan to generalize our model to a broader range of knowledge workers and explore its potential to actively reduce interruption cost by indicating the interruptibility status to co-workers and fostering undisrupted work.
"
Customizing Developmentally Situated Design (DSD) Cards: Informing Designers about Preschoolers' Spatial Learning,https://dl.acm.org/authorize?N657393,"
To date, developmental needs and abilities of children under 4 years old have been insufficiently taken into account at the early stages of technology design. Bekker and Antle [6] created developmentally situated design (DSD) cards as a design tool to inform children's technology designers about children's development starting from 5 years of age. In this paper, we describe how we customized DSD cards for a specific developmental skill (i.e., spatial learning) of children between 2- and 4-year-olds for tangible interaction design. The cards were evaluated after a user study in which 19 participants from different backgrounds used the cards in three design workshops. Our analysis of observational notes and online survey identify and discuss how specific card features support or limit use by our participants. We draw on our findings to set forth design considerations and possible refinements that make age specific knowledge about very young children's spatial learning to inform technologies based on tangible interaction. We customized and used DSD cards which were originally created by [6] as a knowledge transfer vehicle in a domain specific design space targeting children younger than 4 years old. Based on our user study findings and experience, we present general considerations for the customization of the DSD cards for informing children’s technology designers and developers. The customization of the DSD cards is not only necessary when targeting users at different age groups within different design spaces, but also while using at different stages of a larger technology design process. Thus, this study not only contributes to a design practice in a specific domain knowledge, but inspires any type of complex domain space with a wicked design problem or extreme target groups. As future work, we will revise the cards relying on the the findings and feedback from our user study, and reevaluate the customized cards in participatory design workshops which enable the participants to contribute to the customization in design-in-use studies, and later in Game-Jam sessions which have real-life constraints. Moreover, a further study that compares the customized DSD cards with another age specific cards would be helpful to validate not only the effectiveness of our customized cards, but also contribute to better design considerations for age specific card-based design tools in general.
"
Framed Guessability: Improving the Discoverability of Gestures and Body Movements for Full-Body Interaction,https://dl.acm.org/authorize?N657394,"
The wide availability of body-sensing technologies (such as Nintendo Wii and Microsoft Kinect) has the potential to bring full-body interaction to the masses, but the design of hand gestures and body movements that can be easily discovered by the users of such systems is still a challenge. In this paper, we revise and evaluate Framed Guessability, a design methodology for crafting discoverable hand gestures and body movements that focuses participants' suggestions within a ""frame,"" i.e. a scenario. We elicited gestures and body movements via the Guessability and the Framed Guessability methods, consulting 89 participants in-lab. We then conducted an in-situ quasi-experimental study with 138 museum visitors to compare the discoverability of gestures and body movements elicited with these two methods. We found that the Framed Guessability movements were more discoverable than those generated via traditional Guessability, even though in the museum there was no reference to the frame. In this paper, we revised Framed Guessability and provided an evaluation of this novel methodology for designing hand gestures and body movements for full-body interaction. The revised version of Framed Guessability that we introduced in this paper incorporates “frames” into the elicitation process, in order to provide a unifying context for applications –such as Human-Data Interaction –in which there are not well-established design patterns or metaphors. During an elicitation study (in-lab), frames clue people in to which actions are more (or less) appropriate. The results of our experimental evaluation (in-situ –when all references to the original priming frame are removed) show that Framed Guessability significantly increases the discoverability of control actions, when compared to the gestures and body movements elicited with traditional Guessability. The value of this design methodology rests on the unusual result that higher discoverability was attained with no in-situ priming. This work contributes to interaction design in two ways. (1) We illustrate that one facet that matters to the users of fullbody systems (in terms of “discoverability”) is establishing a connecting thread across control actions. This is in deep contrast with the idea that only standardized actions (e.g., pinch and zoom) can be easily discovered, and opens new opportunities for creatively crafting rich interaction designs. We will clarify this in the Conclusion. (2) Although we evaluated Framed Guessability with an interactive data visualization exhibit, it can be applied to a plethora of fullbody interaction scenarios -e.g., interactive installations for cultural heritage, interactive public art, and commercial devices such as smart TVs, in which users do not typically consult manuals before interacting. Future work should explore what happens when users are also primed in-situ, i.e. the degree to which the number and nature of cues incorporated into final design space (e.g., visual cues, or environmental design cues) can improve the discoverability of control actions. We also believe that Framed Guessability can be used in scenarios that go beyond human-computer interaction: for example, to catalog the schemata that are triggered by specific frames, and to understand interconnected reasoning patterns that may be triggered by gestures and body movements.
"
Beagle: Automated Extraction and Interpretation of Visualizations from the Web,https://dl.acm.org/authorize?N657305,"
""How common is interactive visualization on the web?"" ""What is the most popular visualization design?"" ""How prevalent are pie charts really?"" These questions intimate the role of interactive visualization in the real (online) world. In this paper, we present our approach (and findings) to answering these questions. First, we introduce Beagle, which mines the web for SVG-based visualizations and automatically classifies them by type (i.e., bar, pie, etc.). With Beagle, we extract over 41,000 visualizations across five different tools and repositories, and classify them with 85% accuracy, across 24 visualization types. Given this visualization collection, we study usage across tools. We find that most visualizations fall under four types: bar charts, line charts, scatter charts, and geographic maps. Though controversial, pie charts are relatively rare for the visualization tools that were studied. Our findings also suggest that the total visualization types supported by a given tool could factor into its ease of use. However this effect appears to be mitigated by providing a variety of diverse expert visualization examples to users. In this paper, we presented Beagle, an automated system for collecting, labeling and analyzing visualizations created on the web. Beagle supports a flexible design with two standalone components. The Web Crawler extracts SVG-based visualizations from webpages, and was able to extract over 41,000 visualizations from the web. The Annotator uses SVGfocused classification techniques to label visualizations, and achieves 86% classification accuracy, in a multi-class classification test with 24 different visualization types. We then use the resulting visualization collection to study usage of the different visualization types across tools. We found that only a small fraction of webpages (0.05%) contain SVG visualizations created using browser-based tools. Furthermore, the vast majority of visualizations in the collection are covered by just four visualization types: bar charts, line charts, scatter charts, and geographic maps. And though they are hotly debated in the visualization community, pie charts are somewhat rare for the particular visualization tools that we studied. Our findings indicate that in addition to tools that provide flexibility (e.g., D3), users may also benefit from visualization tools that facilitate fast ease of use by focusing on supporting a small set of visualization types.

"
Mediating Conflicts in Minecraft: Empowering Learning in Online Multiplayer Games,https://dl.acm.org/authorize?N657306,"
Multiplayer online games, such as Minecraft, have the potential to be powerful sites for youth learning, but can be plagued by inter-personal conflicts. This brings the need for online moderation. However, only very little is known about the practices through which such moderation happens, or how socio-technical systems could be designed to enable 'safe' learning spaces online. To start addressing this gap, our research examines the existing mediation practices within a moderated Minecraft server for children aged 8-13. As part of our 14 months long engagement, we triangulate data from participant observation, interviews, and analysis of server logs. We demonstrate how---in trying to 'keep peace'---the online moderators monopolised the conflict resolution process, essentially preventing the children from actively working with and learning from the experiences of conflict. In response to these findings, we present an alternative framework for online conflict mediation, suggesting ways in which existing conflict resolution techniques originating in Prevention Science could be re-interpreted for online multiplayer settings. This paper presents findings from a long term participatory engagement with a moderated Minecraft server for young gamers (aged 8-13). By combining such ethnographic case study data with scholarship from learning sciences, we argue that the existing authoritarian moderating practices can not only prevent young gamers from actively engaging with and learning from conflicts they experience online; but can also reinforce a disempowering set of social behaviours and values more broadly. As a first step towards the design of alternative moderating models, we draw on Prevention Science interventions to articulate a set of principles to guide future designs.
"
GestAKey: Touch Interaction on Individual Keycaps,https://dl.acm.org/authorize?N657307,"
Conventionally, keys on a physical keyboard have only two states: ""released'' and ""pressed''. As such, various techniques, such as hotkeys, are designed to enhance the keyboard expressiveness. Realizing that user inevitably perform touch actions during keystrokes, we propose GestAKey, leveraging location and motion of the touch on individual keycaps to augment the functionalities of existing keystrokes. With a log study, we collected touch data for both normal usage (typing and hotkeys) and while performing touch gestures (location and motion), which are analyzed to assess the viability of augmenting keystrokes with simultaneous gestures. A controlled experiment was conducted to compare GestAKey with existing keyboard interaction techniques, in terms of efficiency and learnability. The results show that GestAKey has comparable performance with hotkey. We further discuss the insights of integrating such touch modality into existing keyboard interaction, and demonstrate several usage scenarios. In this work, we introduced GestAKey, a technique that enabled multifunctional keystrokes on a touch-sensitive keycap, opening up new design opportunities for interactions on a keyboard. We developed a custom-made keycap and built the software and hardware systems to detect touch on individual keycaps. We conducted a log study and verified the feasibility of gestural interaction on keycaps. A controlled experiment showed that although subject to system imperfections, our technique still had a comparable overall performance. Applications using the GestAKey technology should leverage the flexibility of gestural input and tightly couple it with existing key functions. With this approach, we believe GestAKey technology will enable the creation of intuitive, learnable, and efficient interaction experiences on keyboards.

"
How Social Dynamics and the Context of Digital Content Impact Workplace Remix,https://dl.acm.org/authorize?N657308,"
As highlighted in recent work on remix in online content creation communities, people commonly take and appropriate digital content for new activities. Less is known, however, about how people repurpose digital content as part of work. We report findings from an interview study with 19 individuals in which we explored how digital content in the workplace becomes a material for remix. Our analysis emphasizes (i) how digital content is obtained from colleagues for remix, (ii) how content is made available for remix by others, and (iii) how digital content is transformed for remix. In attending to these broader processes of remix, we consider the roles of workplace technologies, such as those for file sharing, as well as social norms that mediate access, remix, and acknowledgement. We draw implications for design of technology that emphasize support for individuals in making digital content available for remix, and raising awareness of the context of that content. Remix is a complex set of processes where individuals navigate technical systems and social dynamics to use coworkers’ digital content toward new purposes. Co-workers may be implicated in these broader processes of remix through their involvement in raising awareness about content, making content available, and conceptually transforming content for reuse. The amount of co-worker involvement, in addition to the perceived contribution of digital content, impacts how individuals perceive acknowledgement and credit in workplace remix. Further, the context of digital content impacts how it is understood to be available for remix, as well as the work that goes into making it available. Designing technical systems to convey the nuance of context, as well as to acknowledge the tensions between social dynamics and technical systems, may support workflow and the remix of materials in the workplace.
"
Somewhere Over the Rainbow: An Empirical Assessment of Quantitative Colormaps,https://dl.acm.org/authorize?N657309,"
An essential goal of quantitative color encoding is the accurate mapping of perceptual dimensions of color to the logical structure of data. Prior research identifies weaknesses of 'rainbow' colormaps and advocates for ramping in luminance, while recent work contributes multi-hue colormaps generated using perceptually-uniform color models. We contribute a comparative analysis of different colormap types, with a focus on comparing single- and multi-hue schemes. We present a suite of experiments in which subjects perform relative distance judgments among color triplets drawn systematically from each of four single-hue and five multi-hue colormaps. We characterize speed and accuracy across each colormap, and identify conditions that degrade performance. We also find that a combination of perceptual color space and color naming measures more accurately predict user performance than either alone, though the overall accuracy is poor. Based on these results, we distill recommendations on how to design more effective color encodings for scalar data. Combining perceptual color models and color naming models leads to higher predictive accuracy for both time and error than either alone. This suggests that lower-level perception and language-level processes may both play a role in the interpretation of quantitative color encodings. We also observe that increasing perceptual and name differences correlate with higher judgment accuracy, but that this trend is non-linear, tapering off among the highest quartile of differences for both measures. That said, we believe the primary take-away is a need for caution, as neither the error model nor time model lead to accurate prediction of the observed experimental results (let alone for new, unseen conditions). Improved models or measures could lead to more accurate predictions of user performance. Some issues may arise from the triplet comparison task: perceptual color models are fit to pairwise discrimination judgments, and so may be less well-suited for the comparison tasks studied here. Moreover, our measures of difference do not take into account either the relative color space locations or the magnitude of the underlying color distances, only their difference. In addition, the inclusion of color legends in each trial may affect the predictive utility of color models. If our experiments were re-run without a visible color legend – such that subjects must make similarity judgments based on perception alone – it is possible that the results might align more closely with color model predictions. We leave exploration of these possibilities to future research.

"
Two Kinds of Novel Multi-user Immersive Display Systems,https://dl.acm.org/authorize?N657300,"
Stereoscopic display is a standard display mode for virtual reality environments. Typical 3D projection provides only a single stereoscopic video stream; thus co-located users cannot correctly perceive the virtual scene based on their own position and view. Several works devoted to developing multi-user stereoscopic display, but the number of users is very limited or the technical implementation is complicated. In this paper we put forward two flexible and simple projection-based multi-user stereoscopic display systems. The first one, named TPA, is based on a triple-projector array and provides a 120Hz active stereo for three users. Two TPAs can be combined to form a six-user system. The second one, named DPA, is a dual-projector and easy-implemented system providing individual stereoscopic video stream for two to six users. Finally, a co-located multi-user virtual fireman simulation training system and a virtual tennis simulation system were created to verify the effectiveness of our systems. A multi-user stereoscopic projection display system extends the single view to a multi-view display, which is an effective display means to realize multi-user co-located collaboration. Each user can watch the virtual scene through his/her single correct perspective view based on his/her own position or viewpoint while increasing their sense of reality experience and immersion. In this paper, two projection-based multi-user stereoscopic display systems are introduced. One is called TPA, which is based on a triple-projector array and can provide a 120Hz active stereo for three or six users. The other is called DPA, which is a dual-projector and easy-realized system providing individual stereoscopic video stream for 2 to 6 users. Both systems are flexible and easy to be implemented by researchers or engineers. They can produce multi-view systems supporting collocated collaboration such as the multi-user virtual fireman training system and the virtual tennis simulation system. The systems can also be used in many other applications such as synchronously playing PowerPoints with different languages and playing movies with multi-language subtitles for multi-users. Two problems remain to be settled to further improve the multi-user stereoscopic projection display technology. One is to enhance the refresh rate of the projector image. The inner image refresh rate of certain types of projectors is more than 120Hz. For example, the Panasonic PT-HZ900 type LCD projector and the triple-chip DLP projector have a refresh rate of 480Hz; and the double-speed DLP projector with RGBRGB6 phase color wheel has a rate of 240Hz. However, these projectors remain outputting with the refresh rate of 120Hz. For instance, the Panasonic PT-HZ900 projects each image four times to perfect crosstalk and residue shadow, while the 240Hz DLP projector projects each image twice to improve the rainbow effect. Theoretically, higher refresh rate of the above types of projectors may be used to extend user capacity. The second problem lies in the open/close switch rate of the shutter glasses. If the number of users increases, the open/close switch rate of the shutter should be improved in response, and the experiment has revealed overdriving zero crossing driving might be used for the open/close switch rate of LC shutter to support 360Hz or higher. There is much room for improvement regarding multi-user projection display.

"
The Effects of Badges and Avatar Identification on Play and Making in Educational Games,https://dl.acm.org/authorize?N657301,"
In our study (N=2189), we divided participants into 6 badge conditions: 1) Role model badges (e.g., Einstein), 2) Personal interest badges (e.g., Movies), 3) Achievement badges (e.g., ""Code King""), 4) Choice, 5) Choice with badges always visible, and 6) No badges. Participants played a CS programming game, then used an editor to create their own level. Badges promoted avatar identification (personal interest, role model), player experience (achievement, role model), intrinsic motivation (achievement, role model), and self-efficacy (role model) during both the game and the editor. Independent of badges, avatar identification promoted player experience, intrinsic motivation, and self-efficacy. Additionally, avatar identification promoted greater overall time spent in both the game and the editor, and led to significantly higher overall quality of the completed game levels (as rated by 3 independent externally trained QA testers). Our study has implications for the design of badge systems and sheds new light on the effects of avatar identification on play and making. In this study, we have looked at how badges and avatar identification impact both play and making in an educational game. We found that certain badges could promote avatar identification (personal interest, role model), player experience (achievement, role model), intrinsic motivation (achievement, role model), and programming self-efficacy (role model) during both the game and the editor. Avatar identification promoted player experience, intrinsic motivation, programming self-efficacy, and the total time spent playing and making. Avatar identification also promoted other meaningful in-editor activity, such as playtesting time, etc. and led to significantly higher overall quality of the completed game levels (as rated by 3 independent externally trained QA testers). Here, we’ve conducted a first study (N=2189) on alternative badge types, and a first study of badges and avatar identification in a making context. These findings contribute to both the literature on badges and avatars.

"
Full-Body Ownership Illusion Can Change Our Emotion,https://dl.acm.org/authorize?N657302,"
Recent advances in technology have allowed users to experience an illusory feeling of full body ownership of a virtual avatar. Such virtual embodiment has the power to elicit perceptual, behavioral or cognitive changes related to oneself, however, its emotional effects have not yet been rigorously examined. To address this issue, we investigated emotional changes as a function of the level of the illusion (Study 1) and whether changes in the facial expression of a virtual avatar can modulate the effects of the illusion (Study 2). The results revealed that stronger illusory feelings of full body ownership were induced in the synchronous condition, and participants reported higher valence in the synchronous condition in both Studies 1 and 2. The results from Study 2 suggested that the facial expression of a virtual avatar can modulate participants' emotions. We discuss the prospects of the development of therapeutic techniques using such illusions to help people with emotion-related symptoms such as depression and social anxiety.

"
c.light: A Tool for Exploring Light Properties in Early Design Stage,https://dl.acm.org/authorize?N657303,"
Although a light becomes an important design element, there are little techniques available to explore shapes and light effects in early design stages. We present c.light, a design tool that consists of a set of modules and a mobile application for visualizing the light in a physical world. It allows designers to easily fabricate both tangible and intangible properties of a light without a technical barrier. We analyzed how c.light contributes to the ideation process of light design through a workshop. The results showed that c.light largely expands designers' capability to manipulate intangible properties of light and, by doing so, it facilitates collaborative and inverted ideation process in early design stages. It is expected that the results of this study could enhance our understanding of how designers manipulate light in a physical world in early design stages and could be a good stepping stone for future tool development. We presented the light design tool for demonstrating the light properties in the early design stages as a design element of the product. The tool comprised of physical modules and a mobile application, which for exploring both tangible and intangible properties of light. A design workshop was conducted and the eight findings could be discovered from the analysis. And we could discover the three values of the tool by grouping the findings. Through the analysis the workshop and design outcomes, we were able to reveal that the tool supported utilization of the light properties and reduced the ideation gap through explorative and inverted design procedures. We also found the tool to naturally create a collaborative ideation environment. We expect this tool and the insights from the workshop will provide meaningful benefits to the designers who are in the early design stage of product design and to the development of the future light design tools.
"
Steering through Successive Objects,https://dl.acm.org/authorize?N657304,"
We investigate stroking motions through successive objects with styli. There are several promising models for stroking motions, such as crossing tasks, which require endpoint accuracy of a stroke, or steering tasks, which require continuous accuracy throughout the trajectory. However, a task requiring users to repeatedly steer through constrained path segments has never been studied, although such operations are needed in GUIs, e.g., for selecting icons or objects on illustration software through lassoing. We empirically confirmed that the interval, trajectory width, and obstacle size significantly affect the movement speed. Existing models can not accurately predict user performance in such tasks. We found several unexpected results such as that steering through denser objects sometimes required less times than expected. Speed profile analysis showed the reasons behind such behaviors, such as participants' anticipation strategies. We also discuss the applicability of exiting performance models and revisions. In this study, we investigated straight stroke movements constrained by obstacles, which simulates a part of lassoing operations through icons. Participants’ behaviors were significantly affected by the task parameters of interval (I), tolerance width (W), and obstacle size (S). Yet, the observations did not match all expectations. Although participants could accelerate more in longer intervals, the results for longer intervals did not always show an increase in the stroke speed. Such unexpected behaviors have not been reported before, but are important for a better understanding of GUI operations, because the dependent variables (MT and V) could not be accurately modeled by existing work. Our proposed IDSC model is simple, yet more accurate than previous work and forms a strong foundation for future models.

"
The Ethnobot: Gathering Ethnographies in the Age of IoT,https://dl.acm.org/authorize?N657315,"
Computational systems and objects are becoming increasingly closely integrated with our daily activities. Ubiquitous and pervasive computing first identified the emerging challenges of studying technology used on-the-move and in widely varied contexts. With IoT, previously sporadic experiences are interconnected across time and space in numerous and complex ways. This increasing complexity has multiplied the challenges facing those who study human experience to inform design. This paper describes the results of a study that used a chatbot or 'Ethnobot' to gather ethnographic data, and considers the opportunities and challenges in collecting this data in the absence of a human ethnographer. This study involved 13 participants gathering information about their experiences at the Royal Highland Show. We demonstrate the effectiveness of the Ethnobot in this setting, discuss the benefits and drawbacks of chatbots as a tool for ethnographic data collection, and conclude with recommendations for the design of chatbots for this purpose. The IoT has increased the number of activities that are digitally mediated. In this new and complex landscape where many lateral connections between users and devices occur, understanding the simultaneous activities of large numbers of people has become increasingly important. This is essential to both understanding the existing terrain into which new technologies will be deployed, and for testing out complex prototype IoT systems. This study has demonstrated that a simply implemented chatbot can provide useful data to begin to map out detailed events and actions in a new context. Despite the small number of participants in this study, the simultaneous gathering of data from three or four participants by one ethnographer points to the possibility of scaling up participant numbers, ultimately enabling data capture from crowds. Creating a chatbot that is more sophisticated and responsive is not just a case of implementing natural language programming, but will also require careful design. Ethnographers will be best placed to consider the design of questions and general functionality of the chatbot, with a view to the capturing the specific data they wish to gather and how other study methods may complement this. When considering the deployment of chatbot for ethnographic data collection we recommend attention to the following: 1) A chatbot should be sophisticated enough to have sensitivity to the participants’ current situation and activities, so that it can act as a guide or an assistant in the areas that it is exploring 2) Prompting subjects will achieve the best results 3) Flexibility is necessary so that the subjects always have the option to input free-text and report on activities of their choosing Whilst chatbots will not be a replacement for human ethnographers, it is clear that the role of the Ethnobot as a remote and ever-present ethnographic tool has a widereaching value for HCI research.
"
An Experimental Study of Cryptocurrency Market Dynamics,https://dl.acm.org/authorize?N657316,"
As cryptocurrencies gain popularity and credibility, marketplaces for cryptocurrencies are growing in importance. Understanding the dynamics of these markets can help to assess how viable the cryptocurrnency ecosystem is and how design choices affect market behavior. One existential threat to cryptocurrencies is dramatic fluctuations in traders' willingness to buy or sell. Using a novel experimental methodology, we conducted an online experiment to study how susceptible traders in these markets are to peer influence from trading behavior. We created bots that executed over one hundred thousand trades costing less than a penny each in 217 cryptocurrencies over the course of six months. We find that individual ""buy"" actions led to short-term increases in subsequent buy-side activity hundreds of times the size of our interventions. From a design perspective, we note that the design choices of the exchange we study may have promoted this and other peer influence effects, which highlights the potential social and economic impact of HCI in the design of digital institutions. Institutional design is a major area of study in the social sciences. The HCI community has an opportunity to contribute to this conversation as many farflung institutions—from banks to marriage in Second Life—migrate to digital spaces. The methodologies for studying digital systems; the awareness of interface effects; and the keen eyes for bias, ethics, and inclusion in the HCI community could add unique perspectives to the design of digital institutions. Markets are an example of an enormously important institution that is becoming increasingly digitized, and market irrationality may be a problem in markets that design-thinking could help address. Our specific application to cryptocurrencies is timely and urgent as new platforms are growing and potentially encouraging users to adopt risky trading strategies. Our work provides an example of how peers in an online system can audit the system dynamics through experimentation with typical behavior. Bots that randomly execute actions of normal users could provide a way to understand peer influence and other phenomena in a variety of online systems. These bots allow us to understand the causal impact of individual actions that can be taken by users in these systems. In studying the dynamics of cryptocurrency markets with this technique, we show that even trades worth just fractions of a penny can influence the nature of other much larger trades in the cryptocurrency markets we study. We observe an approximately two percentage point increase in buying activity after our buy interventions, and a cumulative monetary effect of 500 times the size of our interventions. While an increase of two percentage points might seem small for an individual action, in a large marketplace this amount is non-trivial. For example, at the time of writing Apple stock on the NASDAQ exchange had an average daily trading volume of 30 million USD, 2% of which would amount to hundreds of thousands of dollars over the course of a day on that stock alone. Designers of online markets should be aware of how minor changes in their systems that affect individual and collective behavior could have major social and economic impact.
"
Paragon: An Online Gallery for Enhancing Design Feedback with Visual Examples,https://dl.acm.org/authorize?N657317,"
Examples provide a source of inspiration for creating designs, but can they help improve the feedback process? Supplementing design feedback with examples could help recipients see issues clearly, identify concrete steps for improvement, and integrate novel ideas. Two online studies investigated how to support novices providing feedback on visual poster designs in an online context. Study One found that feedback providers select poster examples that complement their feedback and align with a provided rubric. Study Two shows that feedback providers give more specific, actionable, and novel input when using an example-centric approach, as opposed to text alone. To support this, we designed Paragon, an interface to efficiently browse examples using metadata. Finally, we discuss implications for collecting examples from the Web and structuring the design feedback process. In this paper, we explored the efficacy, role, and benefit of using examples in an online design feedback process. Two randomized Web experiments showed that online novice participants were capable of using examples and preferred to do so in providing feedback. Feedback supplemented with examples were considered more specific, actionable, and novel by experts and they also tended to be longer than text-only feedback. This required more time to complete, however this additional workload did not seem detrimental, as measured by subjective scores of compensation fairness. Finally, we have built an online gallery interface with filters using crowdsourced metadata to aid the process of finding useful design examples for feedback providers. While some participants responded favorably, others expressed frustration, surfacing the challenges in striking a balance between the additional cognitive load and the usefulness of such a mechanism.
"
"Personality Depends on The Medium: Differences in Self-Perception on Snapchat, Facebook and Offline",https://dl.acm.org/authorize?N657318,"
We investigate self-perception in social media through the lens of personality theory. Two mixed-methods studies involving 148 participants examine if people self-report different personality traits in social media compared with their offline traits. We first compare offline and Facebook traits, finding that on Facebook people are less Neurotic, Open and Agreeable. A second study compared offline, Facebook and Snapchat traits, replicating and extending our initial results. Again Facebook personality was less Neurotic and less Open than offline. In contrast, Snapchat personality was more Extravert than both offline and Facebook and more Open than Facebook. Interviews indicate how personality differences arise from social media affordances. Anxiety about audience judgments leads people to curate posts to appear less Neurotic on social media, but the transience of Snapchat promotes greater Extraversion than offline and Facebook. We discuss theory and design implications. Two studies show novel findings documenting large, reliable personality differences in offline and online personality, and between social media platforms. On Facebook people perceive their personality as less Neurotic and Open, which seems to arise from a desire to avoid controversy. Snapchat, while also less Neurotic is used in an Open hypersocial manner, with some seeing themselves as more Extravert than offline. These differences seem to arise from affordances of audience and ephemerality.
"
"""We Don't Do That Here"": How Collaborative Editing with Mentors Improves Engagement in Social Q&A Communities",https://dl.acm.org/authorize?N657319,"
Online question-and-answer (Q&A) communities like Stack Overflow have norms that are not obvious to novice users. Novices create and post programming questions without feedback, and the community enforces site norms through public downvoting and commenting. This can leave novices discouraged from further participation. We deployed a month long, just-in-time mentorship program to Stack Overflow in which we redirected novices in the process of asking a question to an on-site Help Room. There, novices received feedback on their question drafts from experienced Stack Overflow mentors. We present examples and discussion of various question improvements including: question context, code formatting, and wording that adheres to on-site cultural norms. We find that mentored questions are substantially improved over non-mentored questions, with average scores increasing by 50%. We provide design implications that challenge how socio-technical communities onboard novices across domains. In this study, we applied theory related to learning and communities of practice to a social Q&A site, by using methods related to mutual engagement and formative feedback. We created Help Rooms in order to provide timely and formative feedback to novices about their questions before they post them. We also used those Help Rooms to study the utility of collaborative question drafts. To understand the effectiveness of our technique and the types of interactions it facilitated, we performed a one-month live study on Stack Overflow. Our findings suggest that the quality of mentored novice questions is significantly different than that of questions that were not mentored. Specifically, we found that mentors provided feedback that improved the question quality by: annotating each question with important information, including crucial context details, explaining attempted solutions, and adopting a tone that meets community standards. As a result, the average scores increased over 50%, and novices were extremely satisfied with their mentorship experience. Further, we discuss how this study can expand to other communities through user insight before building and taking advantage of the flexibility of human mentors. In summary, our mentorship program improved the onboarding experience for novices and enabled mentors to improve their feedback skills. By involving users in making their own community more empathetic and supportive, we pave the way for a more engaged future generation of novices.
"
Using High Frequency Accelerometer and Mouse to Compensate for End-to-end Latency in Indirect Interaction,https://dl.acm.org/authorize?N657310,"
End-to-end latency corresponds to the temporal difference between a user input and the corresponding output from a system. It has been shown to degrade user performance in both direct and indirect interaction. If it can be reduced to some extend, latency can also be compensated through software compensation by trying to predict the future position of the cursor based on previous positions, velocities and accelerations. In this paper, we propose a hybrid hardware and software prediction technique specifically designed for partially compensating end-to-end latency in indirect pointing. We combine a computer mouse with a high frequency accelerometer to predict the future location of the pointer using Euler based equations. Our prediction method results in more accurate prediction than previously introduced prediction algorithms for direct touch. A controlled experiment also revealed that it can improve target acquisition time in pointing tasks. We introduced TurboMouse, a hybrid hardware and software latency compensation technique specifically designed for indirect interaction. TurboMouse combines the velocity measured by the mouse and the acceleration reported by an accelerometer embedded in the mouse to predict cursor’s position using Euler based equations. Our first experiment showed that TurboMouse appears as a good trade-off between latency reduction and introduction of disturbing side-effects that might be noticed by the user. Our second experiment showed that TurboMouse increases user performance in pointing task, especially in an environment with relatively high latency. Future work will be focused on attempting to further minimize the side-effects of TurboMouse for higher levels of latency compensation.

"
Modeling Perceived Screen Resolution Based on Position and Orientation of Wrist-Worn Devices,https://dl.acm.org/authorize?N657311,"
This paper presents a model allowing inferences of perceivable screen content in relation to position and orientation of mobile or wearable devices with respect to their user. The model is based on findings from vision science and allows prediction of a value of effective resolution that can be perceived by a user. It considers distance and angle between the device and the eyes of the observer as well as the resulting retinal eccentricity when the device is not directly focused but observed in the periphery. To validate our model, we conducted a study with 12 participants. Based on our results, we outline implications for the design of mobile applications that are able to adapt themselves to facilitate information throughput and usability. We presented and validated a model that allows inference of perceivable screen content on wrist-worn devices given their position in relation to an observer’s eyes. The model is built on insights from vision science and considers properties of the human visual system to predict what information is visible to an observer. As a first step to provide useful tools for user interface designers, we implemented a tool to visualize how a given type of screen content is perceived depending on the distance and orientation of the smartwatch display w.r.t. to the observer. We thereby distinguish whether the device is actively observed, i.e. the user is directly looking at the display, or whether it is located in the peripheral field of view. Given the output of our model, user interface designers can adapt the visual appearance of their applications in a way that assures that users are able to perceive the desired screen content.

"
Investigating Perceptual Congruence between Data and Display Dimensions in Sonification,https://dl.acm.org/authorize?N657312,"
The relationships between sounds and their perceived meaning and connotations are complex, making auditory perception an important factor to consider when designing sonification systems. Listeners often have a mental model of how a data variable should sound during sonification and this model is not considered in most data:sound mappings. This can lead to mappings that are difficult to use and can cause confusion. To investigate this issue, we conducted a magnitude estimation experiment to map how roughness, noise and pitch relate to the perceived magnitude of stress, error and danger. These parameters were chosen due to previous findings which suggest perceptual congruency between these auditory sensations and conceptual variables. Results from this experiment show that polarity and scaling preference are dependent on the data:sound mapping. This work provides polarity and scaling values that may be directly utilised by sonification designers to improve auditory displays in areas such as accessible and mobile computing, process-monitoring and biofeedback. We investigated the effects of acoustic parameter choice on a number of data:sound mappings. We presented a study in which magnitude estimation was used to map how the perceived magnitude of a data variable changed based on a change in an acoustic parameter. Polarities and scales were derived for each mapping. We found that the acoustic parameter used to convey a data variable had a significant effect on the listener’s perception of the magnitude of that data variable. This suggests that designing a data:sound mapping which is congruent with the listener’s mental model of how they expect the data value to sound in a sonification system is key to successful mappings and sonifications.

"
Acceptability and Acceptance of Autonomous Mobility on Demand: The Impact of an Immersive Experience,https://dl.acm.org/authorize?N657313,"
Autonomous vehicles have the potential to fundamentally change existing transportation systems. Beyond legal concerns, these societal evolutions will critically depend on user acceptance. As an emerging mode of public transportation [7], Autonomous mobility on demand (AMoD) is of particular interest in this context. The aim of the present study is to identify the main components of acceptability (before first use) and acceptance (after first use) of AMoD, following a user experience (UX) framework. To address this goal, we conducted three workshops (N=14) involving open discussions and a ride in an experimental autonomous shuttle. Using a mixed-methods approach, we measured pre-immersion acceptability before immersing the participants in an on-demand transport scenario, and eventually measured post-immersion acceptance of AMoD. Results show that participants were reassured about safety concerns, however they perceived the AMoD experience as ineffective. Our findings highlight key factors to be taken into account when designing AMoD experiences. In the present study, we placed 14 participants in an immersive AMoD experience in order to compare the acceptability (before first use) and acceptance (after first use) of an AMoD system. We used a mixed methods approach, complementing acceptance questionnaires with a psychological needs-driven approach using UX cards. Thereby, we were able to understand underlying factors which influence acceptability and acceptance. Our results show that that participants were reassured regarding safety concerns they had expressed. Nevertheless, the AMoD experienced during this study was perceived as not sufficiently effective. Consequently, perceived usefulness and performance expectancy both decreased significantly. These results shed light on factors which are essential for an optimal AMoD experience. Our study also revealed points of improvement, thereby providing leads to AMoD designers and researchers striving to develop a positive AMoD experience. We expect the results of this study to contribute to the development of user-centered AMoD and to inspire future studies in the context of new forms of mobility.

"
BioFidget: Biofeedback for Respiration Training Using an Augmented Fidget Spinner,https://dl.acm.org/authorize?N657314,"
This paper presents BioFidget, a biofeedback system that integrates physiological sensing and display into a smart fidget spinner for respiration training. We present a simple yet novel hardware design that transforms a fidget spinner into 1) a nonintrusive heart rate variability (HRV) sensor, 2) an electromechanical respiration sensor, and 3) an information display. The combination of these features enables users to engage in respiration training through designed tangible and embodied interactions, without requiring them to wear additional physiological sensors. The results of this empirical user study prove that the respiration training method reduces stress, and the proposed system meets the requirements of sensing validity and engagement with 32 participants in a practical setting. The fidget spinner is a popular toy that went viral in 2017. Although it is fun to play with, the general perception is that a fidget spinner is a useless machine which has a function but no direct purpose. Marketers sometimes claim that fidget spinners are a “treatment for people with attentiondeficit/hyperactivity disorder, autism, or anxiety,” and “a tool for focusing and relaxing.” However, there is no peerreviewed scientific evidence showing that fidget spinners are effective treatments for these conditions so far [29]. BioFidget integrates biofeedback, biosensing, and respiration training mechanisms into the form of a fidget spinner. The details of the physical, physiological, and visual designs have been disclosed. The results of technical and preliminary user testing also show that the proposed system and method provide valid and playful experiences that turn a popular toy into a useful stress management tool.

"
Gender-Inclusive Design: Sense of Belonging and Bias in Web Interfaces,https://dl.acm.org/authorize?N657325,"
We interact with dozens of web interfaces on a daily basis, making inclusive web design practices more important than ever. This paper investigates the impacts of web interface design on ambient belonging, or the sense of belonging to a community or culture. Our experiment deployed two content-identical webpages for an introductory computer science course, differing only in aesthetic features such that one was perceived as masculine while the other was gender-neutral. Our results confirm that young women exposed to the masculine page are negatively affected, reporting significantly less ambient belonging, interest in the course and in studying computer science broadly. They also experience significantly more concern about others' perception of their gender relative to young women exposed to the neutral page, while no similar effect is seen in young men. These results suggest that gender biases can be triggered by web design, highlighting the need for inclusive user interface design for the web. Much like there are well-understood principles from psychology governing user engagement or behavior in design, we argue for developing a comprehensive understanding of the psychology of inclusivity and bias in web interface design. This work begins to develop such a framework by examining ambient belonging in the context of gender bias. Through a controlled experiment exposing participants to masculine and gender-neutral interfaces, we demonstrate that gender-biased design choices can significantly negatively impact women users of those web interfaces, whereas gender-neutral design was perceived positively by individuals of all genders. This highlights the potential consequences of non-inclusive design, and the importance of developing a systemic understanding of bias for inclusivity in web design.
"
Too Close and Crowded: Understanding Stress on Mobile Instant Messengers based on Proxemics,https://dl.acm.org/authorize?N657326,"
Nowadays, mobile instant messaging (MIM) is a necessity for our private and public lives, but it has also been the cause of stress. In South Korea, MIM stress has become a serious social problem. To understand this stress, we conducted four focus groups with 20 participants under MIM stress. We initially discovered that MIM stress relates to how people perceive the territory in MIM. We then applied proxemics-the theory of human use of space-to the thematic analysis as the rationale. The data revealed two main themes: too close and too crowded. The participants were stressed due to design features that let strangers or crowds into their MIM applications and forced them to interact and share their status with them. Based on this finding, we propose a set of implications for designing anti-stress MIM applications. Today, MIM is one of the most popular communication tools. During the initial growth of MIM, closeness and the large number of people connected through MIM were competitive characteristics and were the reason why our participants used KakaoTalk as their primary MIM; it had the largest number of registered users in South Korea. Closeness and a large number of friends initially provided convenience. However, it now generates stress, and it has even become a social problem. Therefore, it is time to rethink MIM design with the aim of relieving stress while maintaining its early advantages. To begin this rethinking, we discovered, from the perspective of proxemics, two main themes: too close and too crowded. We found that the participants experienced stress with MIM that is similar to the stress people experience in physical space. We presented three sub-themes for each main theme and highlighted the related design features to understand how current MIM design features affect MIM stress. This study also provided design implications for friends, chat rooms, and messages, which are key elements of today’s MIM. In this study, we applied proxemics to understand MIM stress. However, we think that proxemics can explain many more possibilities about our use of digital space. We wanted to know the specific differences the design features of MIM make in terms of human spatial perception and which design principles make those differences; however, our study did not include these aspects. This study will contribute to future HCI work that finds “the hidden dimension” in digital space.
"
Socioeconomic Inequalities in the Non use of Facebook,https://dl.acm.org/authorize?N657327,"
Use and non-use of technology can occur in a variety of forms. This paper analyzes data from a probabilistic sample of 1000 US households to identify predictors for four different types of use and non-use of the social media site Facebook. The results make three important contributions. First, they demonstrate that many demographic and socioeconomic predictors of social media use and non-use identified in prior studies hold with a larger, more diverse sample. Second, they show how going beyond a binary distinction between use and non-use reveals inequalities in social media use and non-use not identified in prior work. Third, they contribute to ongoing discussions about the representativeness of social media data by showing which populations are, and are not, represented in samples drawn from social media. This paper provides three unique contributions. First, it moves beyond a dichotomous distinction between use and non-use to consider other types of relationships with social media. Second, the results show how this finer-grained approach reveals socioeconomic inequalities not identified in previous work. Third, it provides specific details about the types of populations we are, and are not, studying when we analyze data from social media.
"
Navigation Systems for Motorcyclists: Exploring Wearable Tactile Feedback for Route Guidance in the Real World,https://dl.acm.org/authorize?N657328,"
Current navigation systems for motor cyclists use visual or auditory cues for guidance. However, this poses a challenge to the motorcyclists since their visual and auditory channels are already occupied with controlling the motorbike, paying attention to other road users, and planing the next turn. In this work, we explore how tactile feedback can be used to guide motorcyclists. We present MOVING (MOtorbike VIbrational Navigation Guidance), a smart kidney belt that presents navigation cues through 12 vibration motors. In addition, we report on the design process of this wearable and on an evaluation with 16 participants in a real world riding setting. We show that MOVING outperforms off-the-shelf navigation systems in terms of turn errors and distraction. In this work, we report on the design and evaluation of a tactile navigation kidney belt for motorcyclists called MOVING. We explored the systemic motorcycle vibrations on different parts of the body to infer an optimal on-body location for the placements of the vibration motors. The MOVING system consists of 12 vibration motors integrated into a kidney belt in order to provide tactile navigation cues to the rider. In a real world evaluation, we explored how such a system performs compared to an off-the-shelf (visual) navigation system. Compared to other evaluations of wearable systems, we performed this evaluation in a real world environment providing a realistic setting. We showed that in this setting tactile cues outperform visual ones. In particular, motorcyclists using MOVING made less turn errors and perceive the system as less distracting compared to an off-the-shelf navigation system.

"
CodeTalk: Improving Programming Environment Accessibility for Visually Impaired Developers,https://dl.acm.org/authorize?N657329,"
In recent times, programming environments like Visual Studio are widely used to enhance programmer productivity. However, inadequate accessibility prevents Visually Impaired (VI) developers from taking full advantage of these environments. In this paper, we focus on the accessibility challenges faced by the VI developers in using Graphical User Interface (GUI) based programming environments. Based on a survey of VI developers and based on two of the authors' personal experiences, we categorize the accessibility difficulties into Discoverability, Glanceability, Navigability, and Alertability. We propose solutions to some of these challenges and implement these in CodeTalk, a plugin for Visual Studio. We show how CodeTalk improves developer experience and share promising early feedback from VI developers who used our plugin. We grouped the numerous accessibility challenges faced by VI developers in using GUI based programming environments into four categories, namely, discoverability, glanceability, navigability and alertability. We presented CodeTalk, a plugin for Visual Studio that enables VI developers to overcome some of these challenges. Participants in the exploratory user study have given very positive feedback on the utility and potential of CodeTalk to improve accessibility. We also presented several possible research directions that emerge from this work.

"
Morphees+: Studying Everyday Reconfigurable Objects for the Design and Taxonomy of Reconfigurable UIs,https://dl.acm.org/authorize?N657320,"
Users interact with many reconfigurable objects in daily life. These objects embed reconfigurations and shape- changing features that users are familiar with. For this reason, everyday reconfigurable objects have informed the design and taxonomy of shape changing UI. However, they have never been explored systematically. In this paper, we present a data set of 82 everyday reconfigurable objects that we collected in a workshop. We discuss how they can inspire the design of reconfigurable interfaces. We particularly focus on taxonomies of reconfigurable interfaces. Taxonomies have been suggested to help design and communication among researchers, however despite their extensive use, taxonomies are rarely evaluated. This paper analyses two established taxonomies - Rasmussen's and Roudaut's - using daily reconfigurable objects. We show relationships between the taxonomies and area for improvements. We propose Morphees+, a refined taxonomy based on Roudaut's Shape Resolution Taxonomy. Ad-hoc considering of existing reconfigurable objects has enabled researchers to use their affordance as well as quickly evaluate interactions with low-cost prototypes. This paper presents the first systematic study of reconfigurable everyday objects. We present a collection and analysis of 82 reconfigurable everyday objects to inform the design of reconfigurable interfaces. We revealed the similarity between two representative shape-changing interface taxonomies and areas for improvements, such as their subjective comprehension. We refined Roudaut’s taxonomy by adding the Modularity and Size, and adjusting the other features. By looking at the materials of the objects, we provided a better understanding of how to implement the reconfiguration features. We hope this work generates new research directions by revisiting existing objects and broadening the research area of reconfigurable interfaces.

"
The Theory-Practice Gap as Generative Metaphor,https://dl.acm.org/authorize?N657321,"
The theory-practice gap is a well-known concept in HCI research. It provides a way of describing a space that allegedly exists between the theory and practice of the field, and it has inspired many researchers to propose ways to ""bridge the gap."" In this paper, we propose a novel interpretation of the gap as a generative metaphor that frames problems and guides researchers towards possible solutions. We examine how the metaphor has emerged in HCI discourse, and what its limitations might be. Our examination raises concerns about treating the gap as given or obvious, which could reflect researchers' tendencies to adopt a problem-solving perspective. We discuss the value of considering problem setting in relation to the theory-practice gap, and then explore Derrida's strategy of ""reversal"" as a possible way to develop new metaphors to capture the relationship between theory and practice. The theory-practice gap is a well-known concept in HCI research [7,8,9,20—22, 26]. It changes the way scholars see the relationship between theory and practice, and it has motivated a diverse set of research projects geared towards solving the problem of how HCI theory can better support practitioners [3,6,22, 26]. In this paper, we propose interpreting the theory-practice gap as a metaphor. We use Donald Schön’s concept of generative metaphor [32] as an analytical framework to examine the theory-practice gap. We analyzed (1) different ways researchers have interpreted the gap, (2) reasons why researchers prioritize some problematic features of the theory-practice relationship and ignore others, (3) reasons why researchers propose certain solutions to the gap problem, and (4) the limitations of the problem-solving perspective as it pertains to the theory-practice gap. Has the gap changed? Has it grown wider or narrower? Or has the time come to explore new metaphors in more depth? These are potentially interesting and important questions to consider in the service of forging stronger connections between theory and practice. Our examination of the gap as a generative metaphor led us to three clear paths forward, which we believe are relevant and actionable for HCI researchers and practitioners. First, we argue for assessing existing proposed bridges, such as bridging concepts and the like, in order to determine whether and how they bring theory and practice closer together. Second, relatedly, researchers and practitioners could build a library of successful bridges, such as affordances, and treat each as cases to be examined and shared. This will go a long way in showing to both groups what makes some theoretical constructs more valuable and useful to practitioners than others. Third, inspired by the continuum metaphor, we believe it is crucial to frame practice as a kind of theorizing. Dorst [12], for instance, has argued that the core of design thinking is in its reliance on abductive reasoning, which, given a desired outcome, allows the designer to arrive at the “best explanation.” Such explanations by practitioners can, then, inform the thinking by researchers who can, in turn, develop them into theoretical constructs. Consequently, a productive cycle of mutual support and synergy can emerge through the adoption of a new metaphor. Derrida’s strategy of reversal and its application to the theory-practice relationship inspired us to propose and explore a different metaphor — that is, the metaphor of “continuum.” The new metaphor does not provide magical solutions to all problems, and the gap cannot be wished away, but it does allow us to think differently about the theory-practice relationship, addressing some issues and surpassing others.
"
Playing to Wait: A Taxonomy of Idle Games,https://dl.acm.org/authorize?N657322,"
Idle games are a recent minimalist gaming phenomenon in which the game is left running with little player interaction. We deepen understanding of idle games and their characteristics by developing a taxonomy and identifying game features. This paper examines 66 idle games using a grounded theory approach to analyze play, game mechanics, rewards, interactivity, progress rate, and user interface. To establish a clearly bounded definition of idle games, we analyzed 10 non-idle games with the same approach. We discuss how idle games move players from playing to planning, how they question dominant assumptions about gameplay, and their unusual use of resources such as player attention and computer cycles. Our work illuminates opportunities for the design of idle games, suggests design implications, and provides a framework for researchers to clearly articulate questions about this genre. In this paper, we undertook a grounded theory study of idle games. We developed a taxonomy to point out several of the defining characteristics of these games. Further, we discuss design implications for idle games, how they affect gameplay, game mechanics and interfaces that support playing to wait, and opportunities and challenges presented by this genre. Our taxonomy contribution aims to support designers and the games community to make sense of the idle games phenomenon and helps to understand how it may be leveraged. Designers can use the taxonomy as a guide to understand the game mechanics, types of gameplay, and design implications of each category of idle games, whether they are creating idle games or incorporating idle modes into games from other genres. Researchers can use our framework to construct studies around game design and planning mechanics; our taxonomy also provides a common language for researchers and game designers to collaborate. Finally, idle games may inspire new ways of thinking about what activities are valuable during play, how play should be organized, and what resources play demands, including both human resources, such as sustained attention, and environmental resources, such as power consumption.
"
Empowering Families Facing English Literacy Challenges to Jointly Engage in Computer Programming,https://dl.acm.org/authorize?N657323,"
Research suggests that parental engagement through Joint Media Engagement (JME) is an important factor in children's learning for coding and programming. Unfortunately, parents with limited technology background may have difficulty supporting their children's access to programming. English-language learning (ELL) families from marginalized communities face particular challenges in understanding and supporting programming, as code is primarily authored using English text. We present BlockStudio, a programming tool for empowering ELL families to jointly engage in introductory coding, using an environment embodying two design principles, text-free and visually concrete. We share a case study involving three community centers serving immigrant and refugee populations. Our findings show ELL families can jointly engage in programming without text, via co-creation and flexible roles, and can create a range of artifacts, indicating understanding of aspects of programming within this environment. We conclude with implications for coding together in ELL families and design ideas for text-free programming research. In this paper, we have presented two design principles (textfree, visually concrete), a system implementing them, and a case study showing JME among ELL families learning to code using this system. Our discussion shows varied possibilities for extending this new understanding of how coding environments may empower and support such underserved populations in learning to code together. We encourage the community to investigate making family-based programs more accessible to ELL families, building text-free curriculum to teach coding at scale, supporting text-free program comprehension techniques, and finally, finding ways to support children as they transition from one coding environment to the next. Our study focused on ELL families, but they are not alone in facing difficulties with English text. Neurodiverse people (e.g., those with dyslexia) experience challenges dealing with written text. The average reading ability of deaf children graduating high school is roughly at the third to fourth grade level [38]. Thus, the removal of text from coding could be useful beyond ELL populations. At the same time, it is not evident whether these same principles would be successful in empowering neurodiverse people to engage in coding, with or without family-based JME, providing another impactful avenue for future work. With these efforts, we can achieve more inclusive and diverse learning communities, and ultimately a more computing literate world.

"
Digital Exhibit Labels in Museums: Promoting Visitor Engagement with Cultural Artifacts,https://dl.acm.org/authorize?N657324,"
How can we use interactive displays in museums to help visitors appreciate authentic objects and artifacts that they can't otherwise touch or manipulate? This paper shares results from a design-based research study on the use of interactive displays to help visitors learn about artifacts in an exhibit on the history and culture of China. To explore the potential afforded by these displays, we unobtrusively video recorded 834 museum visitor groups who stopped in front of one collection of objects. Drawing on cognitive models of curiosity, we tested three redesigns of this display, each focusing on a different strategy to spark visitor curiosity, interest, and engagement. To understand the relative effectiveness of these designs, we analyzed visitor interaction and conversation. Our results uncovered significant differences across the conditions suggesting implications for the use of such technology in museums.
"
Training Person-Specific Gaze Estimators from User Interactions with Multiple Devices,https://dl.acm.org/authorize?N657335,"
Learning-based gaze estimation has significant potential to enable attentive user interfaces and gaze-based interaction on the billions of camera-equipped handheld devices and ambient displays. While training accurate person- and device-independent gaze estimators remains challenging, person-specific training is feasible but requires tedious data collection for each target device. To address these limitations, we present the first method to train person-specific gaze estimators across multiple devices. At the core of our method is a single convolutional neural network with shared feature extraction layers and device-specific branches that we train from face images and corresponding on-screen gaze locations. Detailed evaluations on a new dataset of interactions with five common devices (mobile phone, tablet, laptop, desktop computer, smart TV) and three common applications (mobile game, text editing, media center) demonstrate the significant potential of cross-device training. We further explore training with gaze locations derived from natural interactions, such as mouse or touch input. In this work we proposed the first method for multi-device person-specific gaze estimation. Our method leverages devicespecific encoders/decoders to adapt to device differences and uses shared feature extraction layers to encode the relation between personal facial appearance and gaze directions in a single representation shared across multiple devices. Our experiments demonstrated that our multi-device CNN outperforms single-device baselines for five different target devices. Furthermore, it could still improve the single-device CNN if it was trained with a sufficient amount of device-specific data. We also found that our method was more robust to noisy data than the single-device CNN. With the growing availability of camera-equipped devices, our method provides a practical and highly promising solution to personal gaze learning, thus opening up numerous opportunities for gaze-based applications in HCI and affective/cognitive computing.
"
Designing for Diabetes Decision Support Systems with Fluid Contextual Reasoning,https://dl.acm.org/authorize?N657336,"
Type 1 diabetes is a potentially life-threatening chronic condition that requires frequent interactions with diverse data to inform treatment decisions. While mobile technologies such as blood glucose meters have long been an essential part of this process, designing interfaces that explicitly support decision-making remains challenging. Dual-process models are a common approach to understanding such cognitive tasks. However, evidence from the first of two studies we present suggests that in demanding and complex situations, some individuals approach disease management in distinctive ways that do not seem to fit well within existing models. This finding motivated, and helped frame our second study, a survey (n=192) to investigate these behaviors in more detail. On the basis of the resulting analysis, we posit Fluid Contextual Reasoning to explain how some people with diabetes respond to particular situations, and discuss how an extended framework might help inform the design of user interfaces for diabetes management. In this paper, we have shown evidence that many people with diabetes can make decisions with their complex data in a manner that does not easily conform with established cognitive models associated with habitual (Type 1 analogous), or sensemaking (Type 2 analogous) thinking. The former is largely reflexive, and therefore limited in its ability to exhibit variously: contextual consideration of complex multivariate data; hypothetical thinking as to potential events or outcomes; and awareness of highly variable situations. By contrast, the latter requires substantial cognitive effort, hence is ill suited to continual repeated application. While FCR is not necessarily a challenge to the fundamentals of two minds thinking, the behavior that we have evidenced does not readily match either. It appears conscious and engaged with complex scenarios, but there is not necessarily identification of a ‘gap’ situation, nor is there inherent discarding or formation of new models. UI designs based solely on a two minds theory have to choose between two extremes, thereby offering no clear guidelines for frequent engagement that retains critical and hypothetical thinking. The inclusion of FCR could allow for such a state. While earlier work [23] appears relevant, we are not aware of integration into current diabetes UI design. Such an extension could help to provide a useful framework to analyze increasingly adopted tools such as CGM’s, and help suggest new designs. FCR does not invalidate the other modes, and we observed no cases where participants displayed FCR without displaying the other modes as well. FCR could be viewed as a workaround, to conserve cognitive resources, using models built up through sensemaking and experience. Interestingly, it appears to be supported and fostered by emerging ubiquitous technologies, such as CGMs. FCR, while not as prevalent as habitual and sensemaking, was found to be common among all demographics within our survey population. We suggest that all three modes are important tools for diabetes self-management, and greater care should be taken that UIs for supporting self-management support these different modes within appropriate contexts. Finally, we recognize that there is a significant body of research supporting dual-process theories, and it would be premature to challenge the perceived wisdom. However, there are more modest ways of viewing FCR mode as a useful cognitive workaround or adaption that allows individuals to accomplish domain specific requirements within the limitations of dual-process thinking. We have presented evidence that users appear to have found a way to integrate essential aspect of two recognized cognitive modes in order to interact with complex and hard to calculate data streams. This appears to allow them to weigh and make predictions on likely outcomes, and use those insights to make vital decisions, all in a fluid and frequent manner. We propose that support of such mental processes through medical UIs merits further exploration. We suggest that possible relevance to other domains would also bear investigation.
"
"The Impact of Word, Multiple Word, and Sentence Input on Virtual Keyboard Decoding Performance",https://dl.acm.org/authorize?N657337,"
Entering text on non-desktop computing devices is often done via an onscreen virtual keyboard. Input on such keyboards normally consists of a sequence of noisy tap events that specify some amount of text, most commonly a single word. But is single word-at-a-time entry the best choice? This paper compares user performance and recognition accuracy of word-at-a-time, phrase-at-a-time, and sentence-at-a-time text entry on a smartwatch keyboard. We evaluate the impact of differing amounts of input in both text copy and free composition tasks. We found providing input of an entire sentence significantly improved entry rates from 26 wpm to 32 wpm while keeping character error rates below 4%. In offline experiments with more processing power and memory, sentence input was recognized with a much lower 2.0% error rate. Our findings suggest virtual keyboards can enhance performance by encouraging users to provide more input per recognition event. We explored how to improve text entry speed and recognition accuracy when using a virtual keyboard decoder. We focused on allowing users to change the quantity of observations provided to the decoder for each recognition event. Compared to a word-at-a-time input strategy, providing input of an entire sentence significantly improved entry rates from 26 wpm to 32 wpm. This was done while keeping the character error rate below 4%. In offline recognition experiments, we found that with more processing power and memory, sentence input could be recognized with a 2.0% error rate while wordat-a-time input had a higher 2.8% error rate. Our results suggest recognition-based touchscreen input methods can be designed to enhance performance by allowing users to modulate the amount of input per recognition event.

"
Designing the Future of Personal Fashion,https://dl.acm.org/authorize?N657338,"
Advances in computer vision and machine learning are changing the way people dress and buy clothes. Given the vast space of fashion problems, where can data-driven technologies provide the most value? To understand consumer pain points and opportunities for technological interventions, this paper presents the results from two independent need-finding studies that explore the gold-standard of personalized shopping: interacting with a personal stylist. Through interviews with five personal stylists, we study the range of problems they address and their in-person processes for working with clients. In a separate study, we investigate how styling experiences map to online settings by building and releasing a chatbot that connects users to one-on-one sessions with a stylist, acquiring more than 70 organic users in three weeks. These conversations reveal that in-person and online styling sessions share similar goals, but online sessions often involve smaller problems that can be resolved more quickly. Based on these explorations, we propose future highly personalized, online interactions that address consumer trust and uncertainty, and discuss opportunities for automation.
"
To Distort or Not to Distort: Distance Cartograms in the Wild,https://dl.acm.org/authorize?N657339,"
Distance Cartograms (DC) distort geographical features so that the measured distance between a single location and any other location on a map indicates absolute travel time. Although studies show that users can efficiently assess travel time with DC, distortion applied in DC may confuse users, and its usefulness ""in the wild"" is unknown. To understand how real world users perceive DC's benefits and drawbacks, we devise techniques that improve DC's presentation (preserving topological relationships among map features while aiming at retaining shapes) and scalability (presenting accurate live travel time). We developed a DC-enabled system with these techniques, and deployed it to 20 participants for 4 weeks. During this period, participants spent, on average, more than 50% of their time with DC as opposed to a standard map. Participants felt DC to be intuitive and useful for assessing travel time. They indicated intent in adopting DC in their real-life scenarios. In this work, we devised novel techniques that enable development of a scalable system that presents DC. Through the field study, we found benefits of using DC can outweigh its drawbacks in the wild, and DC can be adopted by real world users in various spatial exploration use cases. We also identified some features we presented in our system can be perceived as unintuitive. We anticipate that our work sets up the possibility for a deeper understanding of identifying more use cases of DC for different users, and developing DC’s distortion model may open new research opportunities.
"
Towards a Multisensory Augmented Reality Map for Blind and Low Vision People: a Participatory Design Approach,https://dl.acm.org/authorize?N657330,"
Current low-tech Orientation & Mobility (O&M) tools for visually impaired people, e.g. tactile maps, possess limitations. Interactive accessible maps have been developed to overcome these. However, most of them are limited to exploration of existing maps, and have remained in laboratories. Using a participatory design approach, we have worked closely with 15 visually impaired students and 3 O&M instructors over 6 months. We iteratively designed and developed an augmented reality map destined at use in O&M classes in special education centers. This prototype combines projection, audio output and use of tactile tokens, and thus allows both map exploration and construction by low vision and blind people. Our user study demonstrated that all students were able to successfully use the prototype, and showed a high user satisfaction. A second phase with 22 international special education teachers allowed us to gain more qualitative insights. This work shows that augmented reality has potential for improving the access to education for visually impaired people. In this project, we designed a multisensory map for low vision and blind people. This prototype was based on a spatial augmented reality toolkit, originally designed for sighted people which we combined with audio feedback and existing tactile tools. Our work has two major contributions. First, we improved the state of the art on accessible interactive maps for visually impaired students by designing a tool that enables both map exploration and map construction (whereas most prior prototypes are limited to exploration of existing maps). Second, we rigorously applied a participatory design approach in close collaboration with students and professionals of a local special education center. This provides strong guarantees of usefulness and accessibility of our prototype. We believe that ultimately this work will contribute to improving the autonomy of visually impaired students.

"
Scaling Classroom IT Skill Tutoring: A Case Study from India,https://dl.acm.org/authorize?N657331,"
India is home to the largest under-25 demographic profile in the world, but lacks a job-ready educational system. It requires a wide-spread, skill-oriented educational model, equipping youth to thrive in highly dynamic job markets. As a response to the huge demand for technical education, a large private skill-tutoring ecosystem has sprung up in In-dia but remains geographically limited. This paper, drawn from a three-month ethnographic research conducted in Ameerpet (arguably India's largest IT skilling hub), probes the pedagogic style and characteristics of tutoring, and of-fers reasons why learners prefer to enroll into a physical model of classroom teaching over online courses. We make design suggestions for online learning platforms to attract students who are marginalized in the more formal and com-petitive education system, and opt for Ameerpet-like skill-hubs. Our primary offering is to suggest a shift in perspec-tive of online education platforms to include job readiness and accompanying changes in course content and delivery. Through a rich immersive ethnographic study, we attempted to offer learnings that sustain a large, physically located technical skill tutoring hub in Hyderabad, India. Our intent is to tease out implications that suggest a student preference for physical classroom situations to online MOOC-like learning resources to acquire job-oriented IT skill-tutoring; and knead these implications to offer design suggestions for online learning platforms that aim to target millions of students in India. We make design suggestions for online learning platforms to attract students who are marginalized in the more formal and competitive education system, and opt for Ameerpet-like skill-hubs. Our primary offering is to suggest a shift in perspective of online education platforms, to include job readiness and accompanying changes in course content and delivery, and point out directions for further work.
"
Regulating Feelings During Interpersonal Conflicts by Changing Voice Self-perception,https://dl.acm.org/authorize?N657332,"
Emotions play a major role in how interpersonal conflicts unfold. Although several strategies and technological approaches have been proposed for emotion regulation, they often require conscious attention and effort. This often limits their efficacy in practice. In this paper, we propose a different approach inspired by self-perception theory: noticing that people are often reacting to the perception of their own behavior, we artificially change their perceptions to influence their emotions. We conducted two studies to evaluate the potential of this approach by automatically and subtly altering how people perceive their own voice. In one study, participants that received voice feedback with a calmer tone during relationship conflicts felt less anxious. In the other study, participants who listened to their own voices with a lower pitch during contentious debates felt more powerful. We discuss the implications of our findings and the opportunities for designing automatic and less perceptible emotion regulation systems. In this paper, we presented a subtle approach for regulating emotions during interpersonal conflicts, which consists in changing how people perceive their own voice. By leveraging theories and findings from emotion regulation, conflict management, nonverbal signals and self-perception, we conducted two studies focusing on interpersonal conflicts, in which participants received voice feedback through headphones with a specific emotional tone. In the first study, romantic couples had conversations about conflicts via Skype, and we found that individuals who perceived their voice with a calmer tone felt less anxious and stressed. In the second study, participants who got involved in contentious debates felt more powerful when they heard their voices with a lower pitch. In both studies, participants were able to focus on their conversations without drifting their attention away to any emotion regulation technology, showing that the intervention does not require attention or effort to be effective. These findings offer promising opportunities for the design of technologies for emotion regulation.
"
Flotation Simulation in a Cable-driven Virtual Environment -- A Study with Parasailing,https://dl.acm.org/authorize?N657333,"
This paper presents flotation simulation in a cable-driven virtual environment. For this, a virtual parasailing system was developed, where the visual stimulus was provided through a VR headset and the physical stimulus was given by wires. In order to prevent the user from moving out of the limited workspace of the cable-driven system, the visual acceleration was washout-filtered to produce the physical acceleration. In the parasailing trajectory, we focused on the stages of vertical acceleration/deceleration and conducted an experiment to identify how much gain can be applied to the visual acceleration, which makes the user feel the natural self-motion when integrated with physical stimulus. Then, the results were tested using several types of full-course virtual parasailing. The results showed that fairly large differences between visual and physical stimuli would be accepted and different gains could be assigned depending on the user's altitudes. We presented virtual parasailing, where the visual stimulus is provided through a VR headset and the physical stimulus is given by a cable-driven system. In order to prevent the user from moving out of the limited workspace of the cabledriven system, the visual acceleration is washout-filtered to produce the physical acceleration. Then, an experiment was conducted to identify how much gain can be applied to the visual acceleration, which makes the user feel the natural self-motion when integrated with the physical stimulus. The results were tested using several types of full-course virtual parasailing. The results showed that fairly large differences between visual and physical stimuli would be accepted and different gains could be assigned depending on the user’s altitudes. Our research results can be applied to various instances of flotation simulation with jumping and free-fall motions. Good candidates include skydiving and bungee jumping, where the dominant motions are made along the vertical direction. Note that our cable-driven system can freely move the manned harness in any direction within the workspace. Therefore, free-flying extreme sports such as paragliding and wingsuit flying could also be simulated with appropriate extensions of acceleration/deceleration controls for surge and sway motions. Further investigations will be made for such extensions. Our experiment also has limitations. A notable one is that no haptic stimulus was provided for the user’s feet. In the real-world parasailing, people first stand on the boat deck and then take off. In our experiment, however, the user started taking-off from the neutral position, i.e., from the state of being suspended in air. Consequently, no haptic feedback was provided on the feet. It is well known that haptic cues have a huge impact on identifying visual gains [36]. If appropriate haptic feedback were provided for both taking-off and landing in our experiments, substantially different results would be obtained. Integrating the haptic cue into the cable-driven system is an area of potential future work.

"
Identity Work as Deliberation: AAPI Political Discourse in the 2016 US Presidential Election,https://dl.acm.org/authorize?N657334,"
Asian Americans and Pacific Islanders (AAPIs) are perceived as the ""model minority"" with a monolithic identity, in contrast to other marginalized racial groups in the United States. In reality, they are composed of different ethnicities, socio-economic backgrounds, and political ideologies. AAPIs share their political views online, engaging in the public sphere through a collaborative process we coin, ""identity work as deliberation."" Using the 2016 US Presidential Election as a case study, we retrieved 4,406 Reddit comments posted between October 2016 to December 2016. We examine how users engage in an online community through a deliberation lens to understand the extent to which Reddit supports identity work as a deliberative process. Under the collective AAPI umbrella, we find that ethnic identifications complicate the types of discussion possible within r/asianamerican. We discuss how the expression of identity, and thereby solidarity, in a politicized online setting may lead to a social movement. We conceptualized Reddit as a public space where people come together to engage in collaborative identity work as a public sphere through a process we call, “identity work as deliberation.” We situate our study during the 2016 US Presidential Election to capture a wide range of deliberation through which we can actively observe identity work and found that redditors engage in a multitude of strategies for maintaining values of inclusion, civility, and rationality. We analyze and discuss how the expression of identity, and thereby solidarity, in an online setting may shape collective action and lead to a social movement.
"
D-SWIME: A Design Space for Smartwatch Interaction Techniques Supporting Mobility and Encumbrance,https://dl.acm.org/authorize?N657345,"
Smartwatches enable rapid access to information anytime and anywhere. However, current smartwatch content navigation techniques, for panning and zooming, were directly adopted from those used on smartphones. These techniques are cumbersome when performed on small smartwatch screens and have not been evaluated for their support in mobility and encumbrance contexts (when the user's hands are busy). We studied the effect of mobility and encumbrance on common content navigation techniques and found a significant decrease in performance as the pace of mobility increases or when the user was encumbered with busy hands. Based on these initial findings, we proposed a design space which would improve efficiency when navigation techniques, such as panning and zooming, are employed in mobility contexts. Our results reveal that our design space can effectively be used to create novel interaction techniques that improve smartwatch content navigation in mobility and encumbrance contexts. Smartwatches enable access to information on-the-go, anytime and anywhere. This can lead to scenarios in which users are mobile and/or encumbered. However, current navigation techniques for zooming and panning have been directly transposed from smartphone environments to smartwatch environments. Our work investigates the impact of mobility and encumbrance on smartwatch interaction techniques. The first experiment revealed that standard interaction techniques are not resistant to different mobility contexts (standing, walking, and running) nor different encumbrance contexts (both hands available, non- dominant hand busy, and both hands busy). We hence proposed a design space dedicated to the design of smartwatch interaction techniques supporting mobility and encumbrance. Our design space defines a structure based on the concept of on-touch efforts (while the fingers are in contact with the touch screen), and pre-touch efforts (while the fingers are preparing the next touch event). For the second experiment, we designed and evaluated two novel zooming techniques and two novel panning techniques in unexplored and theoretically promising areas revealed by our design space. We also proposed an adaptive version of each technique. Results showed that: (i) our new techniques outperformed standard ones in all mobility and encumbrance contexts; and, (ii) reducing touch-efforts can reduce or cancel the negative impact of mobility and encumbrance. Qualitative results also showed a clear preference for navigation techniques reducing touch-efforts. In this work, we evaluated one version of adaptive behavior for each technique. We based the scaling and panning rates on informal pilots. We plan to further study the effect of varying zooming and panning scales on each technique to better understand their influence on touch efforts. Another extension of this work can be the exploration of the relationship between touch efforts, learning efforts (motor), and memory efforts (cognitive). Such exploration could extend our 2-axes design space into a four dimensional one. Due to the precision required for navigation tasks, eye-free interaction remains an open challenge. Our work is a step toward the goal of optimal smartwatch interaction techniques by first reducing the motor requirements. Future work can consider other factors, such as feedback, to improve eye-free input.
"
A Visual Interaction Framework for Dimensionality Reduction Based Data Exploration,https://dl.acm.org/authorize?N657346,"
Dimensionality reduction is a common method for analyzing and visualizing high-dimensional data. However, reasoning dynamically about the results of a dimensionality reduction is difficult. Dimensionality-reduction algorithms use complex optimizations to reduce the number of dimensions of a dataset, but these new dimensions often lack a clear relation to the initial data dimensions, thus making them difficult to interpret. Here we propose a visual interaction framework to improve dimensionality-reduction based exploratory data analysis. We introduce two interaction techniques, forward projection and backward projection, for dynamically reasoning about dimensionally reduced data. We also contribute two visualization techniques, prolines and feasibility maps, to facilitate the effective use of the proposed interactions. We apply our framework to PCA and autoencoder-based dimensionality reductions. Through data-exploration examples, we demonstrate how our visual interactions can improve the use of dimensionality reduction in exploratory data analysis. We propose a new visual interaction framework that lets users dynamically change the input and output of a dimensionality reduction (DR) and observe the effects of these changes. We achieve this framework through two new interactions, forward projection and backward projection, along with two new visualization techniques, prolines and feasibility map, that facilitate the effective use of the interactions. We apply our framework to principal component analysis (PCA) and autoencoder-based DRs and give examples demonstrating how our visual interactions can improve DR-based data exploration. We show that the framework interactions applied to PCA and autoencoders provide a desirable balance between speed and accuracy to sustain interactivity, scaling gracefully with increasing data size and dimensionality. Finally, we argue that our visual interaction framework can apply to black-box machine learning models at large and discuss how our framework subsumes recent approaches in visualizing deep neural network models. Exploratory data analysis is an iterative process in which analysts essentially run mental experiments on data, asking questions and (re)forming and evaluating hypotheses. Tukey and Wilk [57] were among the first to observe the similarities between data analysis and doing experiments. Of the eleven similarities between the two that they listed, one in particular is relevant here:“interaction, feedback, trial and error are all essential; convenience is dramatically helpful.” In fact, data can be severely underutilized (e.g., dead [24, 60]) without what-if analysis. However, to perform data analysis as if we were running data experiments, dynamic visual interactions that bidirectionally bind data and its visual representations must be among our tools. Our work here is a contribution to performing visual analysis in a way similar to running experiments.

"
Complex Mediation in the Formation of Political Opinions,https://dl.acm.org/authorize?N657347,"
The Internet plays an important role in the formation of political opinions by supporting citizens in discovering diverse political information and opinions. However, the echo chamber effect has become of increasing concern, referring to the tendency for people to encounter opinions and information similar to their own online. It remains poorly understood how ordinary citizens use the Internet in the formation of political opinions. To answer this question, we conducted an interview study with 32 Chinese citizens. We found that participants used complex strategies to coordinate personal networks and technologies in specific ways to better understand political events. To analyze this phenomenon, we draw on Bødker and Andersen's model of complex mediation which describes how multiple mediators including people and artifacts work together to mediate an activity. We discuss how complex mediation supported participants in informing their political opinions. We derive design implications for supporting people to form political opinions. In this paper, we reported a qualitative study of how Chinese citizens used personal social networks along with digital technologies to develop political opinions. We identified complex strategies, ranging from relying on one single medium to coordinating personal networks and technologies in a sophisticated way. We highlighted how the specific study context gave rise to the use of complex mediational strategies. Through the case study, we demonstrated how complex mediation constituted a bottomup effort to manage the echo chamber effect. We discussed design implications in light of the socio-political and economic dimensions of the media environment.
"
NavigaTone: Seamlessly Embedding Navigation Cues in Mobile Music Listening,https://dl.acm.org/authorize?N657348,"
As humans, we have the natural capability of localizing the origin of sounds. Spatial audio rendering leverages this skill by applying special filters to recorded audio to create the impression that a sound emanates from a certain position in the physical space. A main application for spatial audio on mobile devices is to provide non-visual navigation cues. Current systems require users to either listen to artificial beacon sounds, or the entire audio source (e.g., a song) is repositioned in space, which impacts the listening experience. We present NavigaTone, a system that takes advantage of multi-track recordings and provides directional cues by moving a single track in the auditory space. While minimizing the impact of the navigation component on the listening experience, a user study showed that participants could localize sources as good as with stereo panning while the listening experience was rated to be closer to common music listening. In this note we presented NavigaTone, a new approach to integrate navigation cues into everyday mobile music listening. Instead of blocking the auditory channel for the single purpose of presenting an auditory beacon at the target location, we take advantage of multitrack recordings to reduce the impact of the navigation component on the listening experience. In combination with spatial audio rendering, we are able to indicate the direction of the navigation target by moving, e.g., the voice of the singer around the user’s head. In a lab study with 16 users, the results of this new approach were on par with the much simpler stereo-panning approach, but found our spatial display to be more natural and less cognitively demanding. Although increased cognitive load under realistic circumstances might influence the perception, we believe that our approach can provide navigational cues in very different scenarios from pedestrian navigation to navigation in virtual worlds. As we were mostly interested in the ability to localize sources, we performed the lab experiment using a short loop of a vocal track that comes with the Klang:app we used. To be able to work reliably with a multitude of songs, NavigaTone needs to ensure that the orientation cue is audible at the waypoints, i.e., the voice of the singer should be present at an intersection where the user needs to perform a left turn. If we look at the capabilities of modern DJ software that allow us to create remixes on the fly, we can think of incorporating the navigation function even deeper into the playback mechanism. An intelligent algorithm could generate a remix of the original track adapted to the navigation task, with the samples used for localization shifted slightly from their original timing to make sure they are present when needed.

"
Considering Agency and Data Granularity in the Design of Visualization Tools,https://dl.acm.org/authorize?N657349,"
Previous research has identified trade-offs when it comes to designing visualization tools. While constructive ""bottom-up' tools promote a hands-on, user-driven design process that enables a deep understanding and control of the visual mapping, automated tools are more efficient and allow people to rapidly explore complex alternative designs, often at the cost of transparency. We investigate how to design visualization tools that support a user-driven, transparent design process while enabling efficiency and automation, through a series of design workshops that looked at how both visualization experts and novices approach this problem. Participants produced a variety of solutions that range from example-based approaches expanding constructive visualization to solutions in which the visualization tool infers solutions on behalf of the designer, e.g., based on data attributes. On a higher level, these findings highlight agency and granularity as dimensions that can guide the design of visualization tools in this space. We have explored the design space of visualization tools that could combine the benefits of constructive, “bottom-up” visualization tools, namely transparency and active involvement in the visualization process, with those of “top-down” approaches, such as rapid visualization creation and applicability to large datasets. We analyzed solutions proposed by ourselves and a group of participants with a range of skill sets and levels of expertise in visualization. From this data we derived a catalog of solutions, a set of three design strategies (automated iteration, abstraction, and automated choices) and propose a conceptualization of the design space of visualization tools based on agency and granularity. Finally, based on our analysis we provide a critical discussion of the existing landscape of tools, which we hope will help designers and the community consider better options in the design of visualization tools.

"
El Paquete Semanal: The Week's Internet in Havana,https://dl.acm.org/authorize?N657340,"
We contribute a case study of El Paquete Semanal or ""The Weekly Package"" -- the pervasive, offline internet in Cuba. We conducted a qualitative inquiry of El Paquete through extensive fieldwork---interviews and observations---in Havana, Cuba. Our findings highlight the human infrastructure that supports this offline internet, rendered visible through the lens of articulation work. By offering an in-depth perspective into these workings of El Paquete, we aim to challenge established notions of what an (or the) internet ""should"" look like in more and less ""developed"" contexts. We highlight how El Paquete is a non-standardized and non-neutral internet, but still human-centered. We also offer an enriched understanding of how an entirely offline internet can provide expansive information access to support leisure and livelihood, additionally serving as a locally relevant platform that affords local participation. We presented a qualitative inquiry of the El Paquete information-sharing ecosystem in Havana, Cuba, and how it acts as an internet for the majority of Cubans. Our research contributes to scholarship in HCI by emphasizing the key human infrastructural elements that support the sustenance and growth of an expansive information network. For the field of CSCW, we strengthen prior research on the human infrastructure lens through our in-depth engagement with the articulation work that EP’s operation relies on. These are the small and large everyday tasks that are not always visible in information networks but successfully make the EP more human-centered. Finally, for the field of ICTD, we extend a rich body of research on media-sharing practices and offline information networks with our case study of EP, as we highlight how its actors rely on it for leisure and livelihood, both generating and consuming locally relevant content.
"
Voice Interfaces in Everyday Life,https://dl.acm.org/authorize?N657341,"
Voice User Interfaces (VUIs) are becoming ubiquitously available, being embedded both into everyday mobility via smartphones, and into the life of the home via 'assistant' devices. Yet, exactly how users of such devices practically thread that use into their everyday social interactions remains underexplored. By collecting and studying audio data from month-long deployments of the Amazon Echo in participants' homes-informed by ethnomethodology and conversation analysis-our study documents the methodical practices of VUI users, and how that use is accomplished in the complex social life of the home. Data we present shows how the device is made accountable to and embedded into conversational settings like family dinners where various simultaneous activities are being achieved. We discuss how the VUI is finely coordinated with the sequential organisation of talk. Finally, we locate implications for the accountability of VUI interaction, request and response design, and raise conceptual challenges to the notion of designing 'conversational' interfaces. The analysis presented here explicates how VUI use is routinely accounted for and embedded in talk-ininteraction. By drawing on fragments from our corpus of recordings of Amazon Echo use collected from multiple homes, our findings reveal how the use of the Echo is made ‘at home’, as situated actions, and becomes embedded in the life of the home rather than that of a discrete singular isolatable event. Our data reveals that the incipience of interaction with a VUI is achieved through its readyavailability, yet users may still methodically account for a request given the social context within which the use is done. We also unpacked the use of a VUI as sequentially organised in and through talk in the home. Ultimately, we identified two collaborative activities in using a VUI: addressing the device in turns-at-talk, and dealing with responses from the device. Finally, we turned to transferring our findings from that of matters of interaction to conceptual discussion points, to inform and shape future research and design on the use of VUIs.
"
Mental Health Support and its Relationship to Linguistic Accommodation in Online Communities,https://dl.acm.org/authorize?N657342,"
Many online communities cater to the critical and unmet needs of individuals challenged with mental illnesses. Generally, communities engender characteristic linguistic practices, known as norms. Conformance to these norms, or linguistic accommodation, encourages social approval and acceptance. This paper investigates whether linguistic accommodation impacts a specific social feedback: the support received by an individual in an online mental health community. We first quantitatively derive two measures for each post in these communities: 1) the linguistic accommodation it exhibits, and 2) the level of support it receives. Thereafter, we build a statistical framework to examine the relationship between these measures. Although the extent to which accommodation is associated with support varies, we find a positive link between the two, consistent across 55 Reddit communities serving various psychological needs. We discuss how our work surfaces a tension in the functioning of these sensitive communities, and present design implications for improving their support provisioning mechanisms. In this paper, we presented a comprehensive study examining the relationship between linguistic accommodation and social support in online mental health communities. Employing a large dataset of 55 Reddit communities that focus on a variety of mental health topics, we first quantitatively derived measures for two kinds of social support, emotional and informational, as received by posts shared in these communities. Then we measured linguistic accommodation exhibited in them, based on a psycholinguistic measure. Our results showed that there is a significant positive association between linguistic accommodation and both the types of support, consistent across the communities we studied. Based on these findings, we note a tension between the vitality of conformance to a community’s norms and the goals of support providers. Our work bears implications for the design tools that can help improve online support provisioning mechanisms.
"
Value-Suppressing Uncertainty Palettes,https://dl.acm.org/authorize?N657343,"
Understanding uncertainty is critical for many analytical tasks. One common approach is to encode data values and uncertainty values independently, using two visual variables. These resulting bivariate maps can be difficult to interpret, and interference between visual channels can reduce the discriminability of marks. To address this issue, we contribute Value-Suppressing Uncertainty Palettes (VSUPs). VSUPs allocate larger ranges of a visual channel to data when uncertainty is low, and smaller ranges when uncertainty is high. This non-uniform budgeting of the visual channels makes more economical use of the limited visual encoding space when uncertainty is low, and encourages more cautious decision-making when uncertainty is high. We demonstrate several examples of VSUPs, and present a crowdsourced evaluation showing that, compared to traditional bivariate maps, VSUPs encourage people to more heavily weight uncertainty information in decision-making tasks. Uncertainty, data quality, or confidence values are often considered separately from the data itself, relegated to tooltips or visually distant supplemental charts. We contend, in contrast, that uncertainty information ought to be directly integrated within a shared chart. This integration introduces additional complexity in the design and presentation of data. ValueSuppressing Uncertainty Palettes represent one strategy for dealing with this complexity, by assigning mark properties in a way that supports the disambiguation of values in data where uncertainty is low, but suppresses these judgments when uncertainty is high. This decision of how to allocate visual variables promotes patterns of decision-making that make responsible use of uncertainty information, discouraging comparison of values in unreliable regions of the data, and promoting comparison in regions of high certainty.
"
Baang: A Viral Speech-based Social Platform for Under-Connected Populations,https://dl.acm.org/authorize?N657344,"
Speech is more natural than text for a large part of the world including hard-to-reach populations (low-literate, poor, tech-novice, visually-impaired, marginalized) and oral cultures. Voice-based services over simple mobile phones are effective means to provide orality-driven social connectivity to such populations. We present Baang, a versatile and inclusive voice-based social platform that allows audio content creation and sharing among its open community of users. Within 8 months, Baang spread virally to 10,721 users (69% of them blind) who participated in 269,468 calls and shared their thoughts via 44,178 audio-posts, 343,542 votes, 124,389 audio-comments and 94,864 shares. We show that the ability to vote, comment and share leads to viral spread, deeper engagement, longer retention and emergence of true dialog among participants. Beyond connectivity, Baang provides its users with a voice and a social identity as well as means to share information and get community support. We presented Baang, a versatile, flexible, and inclusive voicebased social platform for hard-to-reach and oral populations. Analysis of gathered data shows that Baang creates a vibrant community of users from diverse socio-economic and linguistic backgrounds including 69% blinds, 10% females and mostly low-educated, unemployed, young men from all over Pakistan. Baang’s open community included people from remote areas and linguistic minorities. Social network features like content sharing and voice comments led to viral and enthusiastic uptake of the service, high user engagement and retention, and true dialog among the community. Browsing and scoring mechanisms of Baang ensure majority-driven quality assurance but not at the risk of drowning the voice of minorities. Baang provides a window into the collective values of a community as they raise their voice against disability abuse, female harassment, foul language, hatred, terrorism and unite for their rights and in support of the oppressed. We show that voice-based social platforms can provide under-connected and tech-naive individuals with a voice and social identity. Next, we plan to annotate the speech content for various interesting features like prosody, accent, sentiment. This would enable localization of linguistic resources for Pakistan. We also plan to analyze Baang’s social network dynamics.
"
Haptic Links: Bimanual Haptics for Virtual Reality Using Variable Stiffness Actuation,https://dl.acm.org/authorize?N657455,"
We present Haptic Links, electro-mechanically actuated physical connections capable of rendering variable stiffness between two commodity handheld virtual reality (VR) controllers. When attached, Haptic Links can dynamically alter the forces perceived between the user's hands to support the haptic rendering of a variety of two-handed objects and interactions. They can rigidly lock controllers in an arbitrary configuration, constrain specific degrees of freedom or directions of motion, and dynamically set stiffness along a continuous range. We demonstrate and compare three prototype Haptic Links: Chain, Layer-Hinge, and Ratchet-Hinge. We then describe interaction techniques and scenarios leveraging the capabilities of each. Our user evaluation results confirm that users can perceive many two-handed objects or interactions as more realistic with Haptic Links than with typical unlinked VR controllers. Haptic Links demonstrate the potential to improve the haptic rendering of two-handed objects and interactions in VR using inter-controller variable stiffness feedback. The multiple implementations of Haptic Links yield different capabilities and advantages for object rendering. Our evaluation shows that Haptic Links can improve the perceived realism of two-handed objects without significantly detracting from the rendering of normal interactions requiring disjoint controllers. Finally, the interaction techniques we introduce leverage Haptic Links to provide more compelling haptic experiences in VR. Virtual reality has become increasingly immersive, leaving us with a growing need for authentic haptic interactions. Haptic Links offer designers of VR experiences a wide range of new haptic tools that work seamlessly with the handheld controllers of commodity VR systems. While our prototypes represent just a starting point in the design of future Haptic Links, we find their early success encouraging for the exploration of a new class of devices which can rapidly augment existing controllers to provide a customized haptic experience.

"
Breeze: Sharing Biofeedback through Wearable Technologies,https://dl.acm.org/authorize?N657456,"
Digitally presenting physiological signals as biofeedback to users raises awareness of both body and mind. This paper describes the effectiveness of conveying a physiological signal often overlooked for communication: breathing. We present the design and development of digital breathing patterns and their evaluation along three output modalities: visual, audio, and haptic. We also present Breeze, a wearable pendant placed around the neck that measures breathing and sends biofeedback in real-time. We evaluated how the breathing patterns were interpreted in a fixed environment and gathered qualitative data on the wearable device's design. We found that participants intentionally modified their own breathing to match the biofeedback, as a technique for understanding the underlying emotion. Our results describe how the features of the breathing patterns and the feedback modalities influenced participants' perception. We include guidelines and suggested use cases, such as Breeze being used by loved ones to increase connectedness and empathy. We described Breeze, a wearable device to communicate breathing biofeedback. Breeze functions bidirectionally, by collecting data with physiological sensors and providing ambient biofeedback. To assess what information people can infer from the various feedback modalities that Breeze provides (visual, audio, and haptic), we conducted a laboratory study where people rated the perceived emotions from a set of generated breathing patterns. To our knowledge our work is the first to describe, quantitatively (breathing measurement) and qualitatively (semistructured interviews), how people naturally mimic a foreign breathing pattern in order to understand it. We described a simple yet effective methodology for extracting interactions between breathing and perceived emotions, opening the use of breathing as a form of biofeedback. Such findings may reinforce the bond that shared biofeedback can provide in remote communication. We envision that making physiological signals more visible could promote empathy and improve connectedness. Our next step is to deploy this technology outside the laboratory in a long-term longitudinal study. We look forward to seeing how people will use Breeze in their everyday lives to communicate with their loved ones.

"
eystrokes,https://dl.acm.org/authorize?N657457,"
We report on typing behaviour and performance of 168,000 volunteers in an online study. The large dataset allows detailed statistical analyses of keystroking patterns, linking them to typing performance. Besides reporting distributions and confirming some earlier findings, we report two new findings. First, letter pairs that are typed by different hands or fingers are more predictive of typing speed than, for example, letter repetitions. Second, rollover-typing, wherein the next key is pressed before the previous one is released, is sur- prisingly prevalent. Notwithstanding considerable variation in typing patterns, unsupervised clustering using normalised inter-key intervals reveals that most users can be divided into eight groups of typists that differ in performance, accuracy, hand and finger usage, and rollover. The code and dataset are released for scientific use. The findings have high-level implications in several areas of HCI research. Modelling assumptions behind the design of text entry methods must be updated, since they have been based on a view of typing styles as rather uniform. Also, training should reconsider individuals’ ways of pressing keys. Training procedures have been based mostly on the assumption of touch typing and ignored individual-specific ways of typing. Exercises could explicitly train in rollover and be personalised for typists’ deficits in line with the cluster they belong to. We found that untrained typists can be as fast as trained typists, but trained typists in general are faster than untrained ones. It is possible that individualised training could help non-touch typists boost their performance. This implies that designers should be sensitive to the different ways people type – that is, design sensing pipelines and text entry techniques that enable rollover also on multitouch surfaces.
"
Selection-based Text Entry in Virtual Reality,https://dl.acm.org/authorize?N657458,"
In recent years, Virtual Reality (VR) and 3D User Interfaces (3DUI) have seen a drastic increase in popularity, especially in terms of consumer-ready hardware and software. While the technology for input as well as output devices is market ready, only a few solutions for text input exist, and empirical knowledge about performance and user preferences is lacking. In this paper, we study text entry in VR by selecting characters on a virtual keyboard. We discuss the design space for assessing selection-based text entry in VR. Then, we implement six methods that span different parts of the design space and evaluate their performance and user preferences. Our results show that pointing using tracked hand-held controllers outperforms all other methods. Other methods such as head pointing can be viable alternatives depending on available resources. We summarize our findings by formulating guidelines for choosing optimal virtual keyboard text entry methods in VR. In this paper, we have studied text entry in VR using a virtual keyboard and discussed the design space including criteria for assessing VR text entry methods. We have introduced six candidates that span different parts of the design space and evaluate their text entry performance and user preference. Although the general conclusion is to choose Controller Pointing for text entry in VR, its usage is dependent on certain criteria and limitations (e.g. tracked hand-held controllers). In addition, isomorphic keyboard interaction, as in the Freehand method, performed badly, even though it had promising user experience results. To sum up, and putting our findings together with related work and our design space, in this paper we present an example decision support tool in the form of a flowchart, so that the results can be easily used by future VR designers and researchers. Text entry is an essential part of human computer interaction and there is still much research needed. Design annotation (e.g. for 3D artists or architects), filename entry or parameter setting, and communication between users are just a few applications for text entry in VR. Future VR systems (e.g. diaries, shops or social networks) may be designed to enable the user to stay in VR for longer times and therefore longer text entry needs to be feasible, too. Finally, the qualifying techniques need to be evaluated in the context of interactive immersive virtual environments.

"
Effects of Viewing Multiple Viewpoint Videos on Metacognition of Collaborative Experiences,https://dl.acm.org/authorize?N657459,"
This paper discusses the effects of multiple viewpoint videos for metacognition of experiences. We present a system for recording multiple users' collaborative experiences by wearable and environmental sensors, and another system for viewing multiple viewpoint videos automatically identified and extracted to associate to individual users. We designed an experiment to compare the metacognition of one's own experience between those based on memory and those supported by video viewing. The experimental results show that metacognitive descriptions related to one's own mind, such as feelings and preferences, are possible regardless whether a person is viewing videos, but such episodic descriptions as the content of someone's utterance and what s/he felt associated with it are strongly promoted by video viewing. We conducted another experiment where the same participants did identical metacognitive description tasks about half a year after the previous experiment. Through the experiments, we found the first-person view video is mostly used for confirming the episodic facts immediately after the experience, whereas after half a year, even one's own experience is often felt like the experiences of others therefore the videos capturing themselves from the conversation partners and environment become important for thinking back to the situations where they were placed. This paper presented a system that abundantly records multiple viewpoint videos and automatically extracts scenes associated to individual user’s experiences and discussed the effects of viewing videos for reflecting and verbalizing experiences. To obtain concrete data for discussion, we conducted a metacognition experiment and compared the amount and the content of the metacognitive descriptions obtained using our system with ones that did not use it. Our experimental results showed that our system is effective for reflecting on episodic memories related to the behaviors of oneself and others. We conclude that our system simplified recalling a scene’s situations where memories are gradually fading. Many participants immersed themselves in the reexperience of their own recorded experiences with our system six months after the original experience. The necessity of such research is increasing in the modern society where continuous lifelog has become possible and the importance of understanding and talking about our experiences using videos of ourselves and others increase. The research results are thought to serve as design guidelines for the support system for memory assistance of elderly people and reflections of learners in the education field, for example. Since an important characteristic of our system is the recording of identical scenes from multiple viewpoints, we should confirm the correlation between videos that the user actually watched and the metacognitive descriptions derived from them. We could not do such discussions since such additional equipment as an eye tracker is necessary to identify which video provided by the video viewer the user is watching. However, it is meaningful that participant comments argued that there was no significant advantage of multiple viewpoint videos immediately after the experience, but they claimed that they are useful for re-experiencing old experiences. Detailed discussions are future work. Discussion on how our system that supports recording/viewing videos (from multiple viewpoints) affects metacognition must be done based on whether it contributes to the development of the skills and the abilities of the perception. Future work will investigate an experience-capturing system that can be used in various situations in daily life and evaluate it through long-term use.

"
"I Lead, You Help but Only with Enough Details: Understanding User Experience of Co-Creation with Artificial Intelligence",https://dl.acm.org/authorize?N657450,"
Recent advances in artificial intelligence (AI) have increased the opportunities for users to interact with the technology. Now, users can even collaborate with AI in creative activities such as art. To understand the user experience in this new user--AI collaboration, we designed a prototype, DuetDraw, an AI interface that allows users and the AI agent to draw pictures collaboratively. We conducted a user study employing both quantitative and qualitative methods. Thirty participants performed a series of drawing tasks with the think-aloud method, followed by post-hoc surveys and interviews. Our findings are as follows: (1) Users were significantly more content with DuetDraw when the tool gave detailed instructions. (2) While users always wanted to lead the task, they also wanted the AI to explain its intentions but only when the users wanted it to do so. (3) Although users rated the AI relatively low in predictability, controllability, and comprehensibility, they enjoyed their interactions with it during the task. Based on these findings, we discuss implications for user interfaces where users can collaborate with AI in creative works. This study examined the user experience of a user–AI collaboration interface for creative work, especially focusing on its communication and initiative issues. We designed a prototype, DuetDraw, in which AI and users can draw pictures cooperatively, and conducted a user study using both quantitative and qualitative approaches. The results of the study revealed that during collaboration, users (1) are more content when AI provides detailed explanations but only when they want it to do so, (2) want to take the initiative at every moment of the process, and (3) have a fun and new user experience through interaction with AI. Finally, based on these findings, we suggested design implications for user–AI collaboration interfaces for creative work. We hope that this work will serve as a step toward a richer and more inclusive understanding of interfaces in which users and AI collaborate in creative works.

"
Supporting Collaborative Health Tracking in the Hospital: Patients' Perspectives,https://dl.acm.org/authorize?N657451,"
The hospital setting creates a high-stakes environment where patients' lives depend on accurate tracking of health data. Despite recent work emphasizing the importance of patients' engagement in their own health care, less is known about how patients track their health and care in the hospital. Through interviews and design probes, we investigated hospitalized patients' tracking activity and analyzed our results using the stage-based personal informatics model. We used this model to understand how to support the tracking needs of hospitalized patients at each stage. In this paper, we discuss hospitalized patients' needs for collaboratively tracking their health with their care team. We suggest future extensions of the stage-based model to accommodate collaborative tracking situations, such as hospitals, where data is collected, analyzed, and acted on by multiple people. Our findings uncover new directions for HCI research and highlight ways to support patients in tracking their care and improving patient safety. Patients in the hospital want to keep track of their health, but lack the digital tools to collect, track, and reflect on all the data that is important to them. In this study, we investigated inpatient needs for tracking their health, and found that inpatients envision collaboratively tracking their health and care plan with their clinical team. We found their tracking process follows the stage-based model, but that collaborative tracking introduces new barriers to and requirements for successful tracking. We provided insights about designing collaborative tracking systems to help hospitalized patients manage their health and care, reflections on how collaborative tracking extends the stagebased model of personal informatics, and suggestions for new research directions. With better tools for collaborative tracking, patients in the hospital will be able to keep track of their health and care, increasing the potential for improved health outcomes and medical error prevention.
"
Investigating the Impact of Gender on Rank in Resume Search Engines,https://dl.acm.org/authorize?N657452,"
In this work we investigate gender-based inequalities in the context of resume search engines, which are tools that allow recruiters to proactively search for candidates based on keywords and filters. If these ranking algorithms take demographic features into account (directly or indirectly), they may produce rankings that disadvantage some candidates. We collect search results from Indeed, Monster, and CareerBuilder based on 35 job titles in 20 U. S. cities, resulting in data on 855K job candidates. Using statistical tests, we examine whether these search engines produce rankings that exhibit two types of indirect discrimination: individual and group unfairness. Furthermore, we use controlled experiments to show that these websites do not use inferred gender of candidates as explicit features in their ranking algorithms.
"
Cognitive Load Estimation in the Wild,https://dl.acm.org/authorize?N657453,"
Cognitive load has been shown, over hundreds of validated studies, to be an important variable for understanding human performance. However, establishing practical, non-contact approaches for automated estimation of cognitive load under real-world conditions is far from a solved problem. Toward the goal of designing such a system, we propose two novel vision-based methods for cognitive load estimation, and evaluate them on a large-scale dataset collected under real-world driving conditions. Cognitive load is defined by which of 3 levels of a validated reference task the observed subject was performing. On this 3-class problem, our best proposed method of using 3D convolutional neural networks achieves 86.1% accuracy at predicting task-induced cognitive load in a sample of 92 subjects from video alone. This work uses the driving context as a training and evaluation dataset, but the trained network is not constrained to the driving environment as it requires no calibration and makes no assumptions about the subject's visual appearance, activity, head pose, scale, and perspective. Cognitive load estimation in the wild is an important and challenging problem. We propose two computer vision based approaches for addressing this problem. The first approach uses HMM models. The second approach uses a 3D-CNN model. Both are based on temporal dynamics of the eye over a period of 6 seconds as captured by 90 visible light video frames. The HMM method tracks explicitly-extracted pupil positions over time, while the 3D-CNN method operates endto-end on the raw grayscale eye region image sequences. On a dataset of 92 subjects, the HMM approach achieves 77.7% average accuracy and the 3D-CNN approach achieves 86.1%.
"
The Effect of Offset Correction and Cursor on Mid-Air Pointing in Real and Virtual Environments,https://dl.acm.org/authorize?N657454,"
Pointing at remote objects to direct others' attention is a fundamental human ability. Previous work explored methods for remote pointing to select targets. Absolute pointing techniques that cast a ray from the user to a target are affected by humans' limited pointing accuracy. Recent work suggests that accuracy can be improved by compensating systematic offsets between targets a user aims at and rays cast from the user to the target. In this paper, we investigate mid-air pointing in the real world and virtual reality. Through a pointing study, we model the offsets to improve pointing accuracy and show that being in a virtual environment affects how users point at targets. In the second study, we validate the developed model and analyze the effect of compensating systematic offsets. We show that the provided model can significantly improve pointing accuracy when no cursor is provided. We further show that a cursor improves pointing accuracy but also increases the selection time. In this paper, we built mid-air pointing offset compensation models for real and virtual environments based on pointing gestures of 20 participants. We built models for four different ray casting techniques and used cross-validation (CV) to show that we achieve the smallest remaining offset when using eye-finger ray cast (EFRC). In a second study, we further investigated EFRC in a selection task. We confirm findings of previous work that using a cursor improves mid-air pointing precision. We show that the accuracy of mid-air pointing without a cursor can be improved through correction models for both real and virtual environments by 13.1%. Further, we show that using a cursor a correction model can reduces the remaining pointing error by 4.5%. As the pointing accuracy may be affected by the HMD we envision as next step a study using HMDs with a variety of FoVs to understand the impact of a limited FoV. In the presented paper we investigated real-world (RW) and virtual reality (VR) which are representing the edges of the Milgram continuum [44], in the next steps, we will also investigate pointing in augmented reality (AR) and mixed reality.

"
"CLAW: A Multifunctional Handheld Haptic Controller for Grasping, Touching, and Triggering in Virtual Reality",https://dl.acm.org/authorize?N657465,"
CLAW is a handheld virtual reality controller that augments the typical controller functionality with force feedback and actuated movement to the index finger. Our controller enables three distinct interactions (grasping virtual object, touching virtual surfaces, and triggering) and changes its corresponding haptic rendering by sensing the differences in the user's grasp. A servo motor coupled with a force sensor renders controllable forces to the index finger during grasping and touching. Using position tracking, a voice coil actuator at the index fingertip generates vibrations for various textures synchronized with finger movement. CLAW also supports a haptic force feedback in the trigger mode when the user holds a gun. We describe the design considerations for CLAW and evaluate its performance through two user studies. The first study obtained qualitative user feedback on the naturalness, effectiveness, and comfort when using the device. The second study investigated the ease of the transition between grasping and touching when using our device. In this paper, we present the design of a novel haptic controller, named CLAW, that augments a typical VR handheld controller functionality with force feedback and actuated movement to the index finger. The primary design principle of our controller is to provide a multi-purpose controller that contains both the expected functionality of a VR controller (i.e., buttons, 6DOF movement control, thumb joysticks, trigger) and enables a variety of force and tactile renderings for the most commonly expected hand interactions: grasping, touching, and triggering. We also contribute a way to switch between haptic modes, by sensing the differences in the user’s grasp as well as the situational context of the virtual scene. Our user evaluations show that CLAW is highly effective in a variety of interactions with participants reporting high scores on realism, control, and manipulation ability, as well as low error rates when switching haptic modes. We hope that our CLAW controller inspires a new generation of VR handheld controllers that provide higher fidelity haptics (beyond vibro-tactile) on a single compact multi-purpose device.

"
Ripple Thermostat: Affecting the Emotional Experience through Interactive Force Feedback and Shape Change,https://dl.acm.org/authorize?N657466,"
Force feedback and shape change are modalities with a growing application potential beyond the more traditional GUIs. We present two studies that explored the effect of these modalities on the emotional experience when interacting with an intelligent thermostat. The first study compared visual feedback, force feedback, and a combination of force feedback and shape change. Results indicate that force feedback correlates to experienced dominance during interaction, while shape change mainly affects experienced arousal. The second study explored how force feedback and shape change could communicate affective meaning during interaction with the thermostat through a co-design study. Participants designed the thermostat behavior for three scenarios supporting energy savings. Results suggest that despite their abstractness, force feedback and shape change convey affective meaning during the user-system dialogue. The findings contribute to the design of intelligible and intuitive feedback. In this paper, we presented two studies that explored the effect of haptic force feedback and shape change on the emotional experience. Based on our findings, we suggest that haptic force feedback has a unique potential to coordinate control in a human-computer dialogue. Shape change seems to be an eligible modality to provide affective meaning to the behavior of system that could make behavior of everyday intelligent objects more intuitive and intelligible.

"
A Qualitative Exploration of Perceptions of Algorithmic Fairness,https://dl.acm.org/authorize?N657467,"
Algorithmic systems increasingly shape information people are exposed to as well as influence decisions about employment, finances, and other opportunities. In some cases, algorithmic systems may be more or less favorable to certain groups or individuals, sparking substantial discussion of algorithmic fairness in public policy circles, academia, and the press. We broaden this discussion by exploring how members of potentially affected communities feel about algorithmic fairness. We conducted workshops and interviews with 44 participants from several populations traditionally marginalized by categories of race or class in the United States. While the concept of algorithmic fairness was largely unfamiliar, learning about algorithmic (un)fairness elicited negative feelings that connect to current national discussions about racial injustice and economic inequality. In addition to their concerns about potential harms to themselves and society, participants also indicated that algorithmic fairness (or lack thereof) could substantially affect their trust in a company or product. One way to make social change is to bolster pragmatic arguments for corporations to do good, by demonstrating that societally positive actions are also good business practice. Consider for example how Green to Gold effectively argued that sustainable business practices not only benefit the environment but can yield significant financial profit [32]. In this paper, we presented a novel exploration of how traditionally marginalized populations perceive algorithmic fairness. While our findings can inform a range of stakeholders, we highlight the insight that company handling of algorithmic fairness interacts significantly with user trust. We hope this insight may provide additional motivation for companies across the technology sector to actively pursue algorithmic fairness. Future work could fruitfully explore these findings with a broader population, noting that Plane et al.’s study offers evidence that at least some of these issues may resonate widely [67]. We also suggest further exploring concrete actions that companies can take regarding algorithmic fairness, such as making specific improvements to product experiences, to build and maintain user trust. Finally, we suggest further research on how stakeholders across the ecosystem can work collectively to leverage their different perspectives and skills to pursue algorithmic fairness.
"
The Benefits and Challenges of Video Calling for Emergency Situations,https://dl.acm.org/authorize?N657468,"
In the coming years, emergency calling services in North America will begin to incorporate new modalities for reporting emergencies, including video-based calling. The challenge is that we know little of how video calling systems should be designed and what benefits or challenges video calling might bring. We conducted observations and contextual interviews within three emergency response call centres to investigate these points. We focused on the work practices of call takers and dispatchers. Results show that video calls could provide valuable contextual information about a situation and help to overcome call taker challenges with information ambiguity, location, deceit, and communication issues. Yet video calls have the potential to introduce issues around control, information overload, and privacy if systems are not designed well. These results point to the need to think about emergency video calling along a continuum of visual modalities ranging from audio calls accompanied with images or video clips to one-way video streams to two-way video streams where camera control and camera work need to be carefully designed. We believe that this work sets the scene for a rich research agenda. Of course, this study has its limitations. Currently many 9-1-1 centres in Canada use the same card set for answering calls and assessing information. Thus, despite the fact that we studied call centres in only one province of Canada, it is likely that our findings around 9-1-1 work practices would be similar to other call centres in Canada. Given that we have not studied emergency call centres in other countries, it is unknown if our results generalize to them. However, our results on call taking procedures were similar to studies reported in other Western countries, including the United States and the UK [4,35,45,56]. This suggests that work practices may be similar, though reactions to video calling and individual preferences may differ across the world. Our work was specifically scoped to focus on the experiences of call takers and dispatchers, given the depth needed to understand this perspective. Yet this does mean that we do not have data from actual callers that might show what their specific needs would be for video calls and what privacy concerns they might have. This suggests future studies aimed at a broad spectrum of callers, with varying backgrounds and experiences. Future research should also explore the interactional exchanges between callers and call takers in more detail. A deeper comparison to remote troubleshooting work would also hold value. What we see as the key benefit of our work is the opening up of the design space around future emergency calls and call handling. While organizations continue to move to new solutions for emergency calls in Canada and others countries, it is not the case that there is a narrow set of possibilities for such technologies. In fact, the design space is rich with possibilities and potential problems that will need to be addressed. We have looked at one specific angle and there are many more, including the specific needs of callers and first responders who receive the information dispatched by call centres. There is also much work to be done on exploring the design of video calling solutions and testing out the ideas that may stem from our research.
"
Whiskers: Exploring the Use of Ultrasonic Haptic Cues on the Face,https://dl.acm.org/authorize?N657469,"
Haptic cues are a valuable feedback mechanism for smart glasses. Prior work has shown how they can support navigation, deliver notifications and cue targets. However, a focus on actuation technologies such as mechanical tactors or fans has restricted the scope of research to a small number of cues presented at fixed locations. To move beyond this limitation, we explore perception of in-air ultrasonic haptic cues on the face. We present two studies examining the fundamental properties of localization, duration and movement perception on three facial sites suitable for use with glasses: the cheek, the center of the forehead, and above the eyebrow. The center of the forehead led to optimal performance with a localization error of 3.77mm and accurate duration (80%) and movement perception (87%). We apply these findings in a study delivering eight different ultrasonic notifications and report mean recognition rates of up to 92.4% (peak: 98.6%). We close with design recommendations for ultrasonic haptic cues on the face. In summary, this paper proposed whiskers – high fidelity ultrasonic haptic cues to the face. It explores their feasibility across a wide range of cue parameters and in a practical notification task. The data demonstrates the fundamental viability of ultrasonic haptic cues on the face. This can both inform designers about how to best create such cues and also provide developers with practical targets for next generation wearable hardware that can produce them. In this way, this paper hopes to facilitate the integration of high fidelity ultrasonic non-contact haptic displays into next generation HMDs.
"
Let's Play!: Digital and Analog Play between Preschoolers and Parents,https://dl.acm.org/authorize?N657460,"
Play is an enjoyable and developmentally useful part of early childhood, and parent-child play is a highly productive mechanism by which children learn to participate in the world. We conducted an observational lab study to examine how 15 parent-child pairs (children age 4-6) respond to and play with tablet apps as compared to analog toys. We found that parents and children were less likely to engage with each other or to respond to each other's bids for attention during play sessions with tab-lets versus play sessions with toys. We also observed that specific design features of tablet devices and children's apps-such as one-sided interfaces, game paradigms that demand continual attention, and lack of support for parallel interaction-are the primary mechanism shaping these differences. We provide guidance suggesting how children's apps might be re-designed to preserve the ad-vantages of digital play experiences while also evolving to build in the advantages of traditional toys. We presented the first study examining detailed observations of parents’ and children’s shared reactions to a variety of traditional toys and tablet apps. While children engaged eagerly with both kinds of stimuli, we observed that opportunities remain to make their favorite apps and games more inclusive of play partners and more conducive to shared parent-child experiences. We discussed suggestions on how children’s tablet apps might be re-designed to preserve the advantages of digital play experiences while also adopting the benefits of traditional toys.
"
Combating Attrition in Digital Self-Improvement Programs using Avatar Customization,https://dl.acm.org/authorize?N657461,"
Digital self-improvement programs (e.g., interventions, training programs, self-help apps) are widely accessible, but can not employ the same degree of external regulation as programs delivered in controlled environments. As a result, they suffer from high attrition -- even the best programs won't work if people don't use them. We propose that volitional engagement -- facilitated through avatar customization -- can help combat attrition. We asked 250 participants to engage daily for 3 weeks in a one-minute breathing exercise for anxiety reduction, using either a generic avatar or one that they customized. Customizing an avatar resulted in significantly less attrition and more sustained engagement as measured through login counts. The problem of attrition affects self-improvement programs across a range of do-mains; we provide a subtle, versatile, and broadly-applicable solution. Digital programs delivered at scale have the potential to increase access to support for people interested in selfimprovement in contexts as diverse as behaviour change, lifelong learning, and mental wellness. However, even the best-designed programs won’t work if people don’t use them. Research in a variety of contexts has shown that digital programs that were effective in an RCT are subject to high attrition when delivered in-the-wild, i.e., at scale. We present a general-purpose interaction technique for combating attrition – avatar customization – and show that it significantly improves retention over three weeks of a daily stress-relief intervention (a breathing exercise) over being assigned a generic avatar. Given the broad accessibility and wide reach of programs delivered at scale, increasing adherence through interaction design has the potential to benefit a great number of people working at self-improvement.
"
"Use the Force Picker, Luke: Space-Efficient Value Input on Force-Sensitive Mobile Touchscreens",https://dl.acm.org/authorize?N657462,"
Picking values from long ordered lists, such as when setting a date or time, is a common task on smartphones. However, the system pickers and tables used for this require significant screen space for spinning and dragging, covering other information or pushing it off-screen. The Force Picker reduces this footprint by letting users increase and decrease values over a wide range using force touch for rate-based control. However, changing input direction this way is difficult. We propose three techniques to address this. With our best candidate, Thumb-Roll, the Force Picker lets untrained users achieve similar accuracy as a standard picker, albeit less quickly. Shrinking it to a single table row, 20% of the iOS picker height, slightly affects completion time, but not accuracy. Intriguingly, after 70 minutes of training, users were significantly faster with this minimized Thumb-Roll Picker compared to the standard picker, at the same accuracy and only 6% of the gesture footprint. We close with application examples. In this paper, we presented a way to reduce the screen real estate for picking values from long, ordered ranges on smartphones. While existing controls, like pickers or tables, require a significant amount of gesture footprint for dragging and spinning, we exploited the force sensors on modern smartphones to reduce the gesture footprint to the size of the thumb. We conducted three experiments to find a force-based technique that allows for bidirectional control of such a picker, while still fitting inside a standard table row. Our first experiment identified Thumb-Roll, a technique that changes direction upon gently rolling the thumb to the left or right before applying force, as the fastest and most accurate technique to control a standard-sized picker by force input. Although users were slower, accuracy was almost identical, and Thumb-Roll reduced the gesture footprint by 88%. We then showed that this allows the Force Picker to be shrunk down to a minimum size, without affecting accuracy, although it slightly slowed down untrained subjects. Our final study showed that trained participants can actually be significantly faster using the minimized Thumb-Roll Force Picker than with a standard spin & drag picker, without significant loss of accuracy. With Thumb-Roll, these users needed only 6% of the gesture space required for dragging and spinning, and the minimized Force Picker takes up only 20% of the height of the iOS 10 system picker. We provided application examples for value input on smartphones that benefit from the much smaller footprint and in-place touch interaction of the minimized Thumb-Roll Force Picker. We hope that our findings inspire other researchers and practitioners to further improve our key daily interactions with our smartphones through their force-sensing capabilities.

"
Live Sketch: Video-driven Dynamic Deformation of Static Drawings,https://dl.acm.org/authorize?N657463,"
Creating sketch animations using traditional tools requires special artistic skills, and is tedious even for trained professionals. To lower the barrier for creating sketch animations, we propose a new system, emphLive Sketch,&lt;/i&gt; which allows novice users to interactively bring static drawings to life by applying deformation-based animation effects that are extracted from video examples. Dynamic deformation is first extracted as a sparse set of moving control points from videos and then transferred to a static drawing. Our system addresses a few major technical challenges, such as motion extraction from video, video-to-sketch alignment, and many-to-one motion-driven sketch animation. While each of the sub-problems could be difficult to solve fully automatically, we present reliable solutions by combining new computational algorithms with intuitive user interactions. Our pilot study shows that our system allows both users with or without animation skills to easily add dynamic deformation to static drawings. We have presented a new interactive system for animating sketch drawings using video examples. The key idea is to extract object motion presented in the video using sparse point tracking, and transfer it to the sketch image using controlled mesh deformation. For motion extraction, we propose a new tracking method that is robust against occlusion and ambiguity, and further combines it with easy user controls for reliable tracking. For motion transfer, our system allows the user to fine control the animation using a set of interactive tools. We conducted a pilot study to show that given static sketches and the corresponding videos, non-professional users can turn them into vivid animations using the amount of time and efforts similar to that regard by the professional users. We believe our current work has only scratched the surface of an exciting opportunity. As pointed out earlier, our system still has problems in handling smooth image regions such as deforming clouds, or stochastic motions such as ocean waves, whose motion is hard to extract using automatic or interactive ways. Expanding the system to handle more types of motions is desirable. The system currently can only transfer the raw motion extracted from videos, it would be interesting to use existing motion stylization or exaggeration approaches, such as the Animation Filter [45], to produce more lively motion trajectories by applying a simple filter. More stylizing animations may also be made by interactively applying Motion Amplifiers [28], Elemental Dynamics [50] to our animation results. In the future we would like to address the first subproblem as discussed in the Introduction section by building a large set of videos with different motion properties and developing a retrieval method to find proper video examples given an arbitrary sketch as input.

"
ECGLens: Interactive Visual Exploration of Large Scale ECG Data for Arrhythmia Detection,https://dl.acm.org/authorize?N657464,"
The Electrocardiogram (ECG) is commonly used to detect arrhythmias. Traditionally, a single ECG observation is used for diagnosis, making it difficult to detect irregular arrhythmias. Recent technology developments, however, have made it cost-effective to collect large amounts of raw ECG data over time. This promises to improve diagnosis accuracy, but the large data volume presents new challenges for cardiologists. This paper introduces ECGLens, an interactive system for arrhythmia detection and analysis using large-scale ECG data. Our system integrates an automatic heartbeat classification algorithm based on convolutional neural network, an outlier detection algorithm, and a set of rich interaction techniques. We also introduce A-glyph, a novel glyph designed to improve the readability and comparison of ECG signals. We report results from a comprehensive user study showing that A-glyph improves the efficiency in arrhythmia detection, and demonstrate the effectiveness of ECGLens in arrhythmia detection through two expert interviews. We have presented an interactive data exploration system, ECGLens, that enables cardiologists to visually identify and analyze arrhythmia in large-scale ECG records. The system design supports the exploration process by integrating a heartbeat classification algorithm, an outlier detection algorithm to our system, as well as an interactive workflow for arrhythmia detection. The design includes the novel A-glyph representation for ECG data which we show outperforms baseline designs in identifying heartbeats that exhibit arrhythmia. Moreover, our evaluation shows that the overall system successfully supported arrhythmia identification within large-scale ECG datasets. In the future, we plan on addressing the limitations of our current implementation and deploying our system to local hospitals so as to improve our system through more users’ feedback. Moreover, we intend to improve our classification algorithm by feeding clinician-provided corrections back to a model re-training process in real-time.

"
"""Protection on that Erection?"": Discourses of Accountability & Compromising Participation in Digital Sexual Health",https://dl.acm.org/authorize?N657475,"
This paper analyses sexual health workers' 'talk' around their introduction of a digital platform to enhance a regionally managed condom distribution scheme for young people. In examining the discursive resources workers used in framing the sexual health service, their service users and digital technology, we argue that problematic ideologies around young people and sexuality were exercised and reproduced. Workers positioned themselves as the gatekeepers of young people's sexual health, who were in turn constructed as 'mischievous' and 'misguided', with technology having a corruptive role over what was considered to be 'healthy' and 'normal' sexual relationships. We suggest our findings indicate severe challenges in developing community-commissioned platforms alongside service providers, and questions how plausible user participation can be in attempting to conduct collaborative, participatory and engaged work in this context. This paper has examined the discursive resources sexual health workers used in framing a digital application to support their young people’s condom distribution scheme. We argue that, in framing users as potential deviants, we were unable to retain values of user-centred design, participation, and community approaches to sexual health in conducting applied, collaborative research. Our research highlights a number of tensions around how sexual health services are positioned by sexual health workers, and how, subsequently, the role of digital technology was problematised. Nevertheless, we suggest there are opportunities to challenge traditional approaches of sexual health through introducing digital elements into service provision. Although workers’ constructions of technology were at times questionable, digital approaches held discursive impact for our participants, particularly when they were seen to be supporting the agenda of running an efficient service. Mindful of how these services are situated within the broader socio-economic circumstances, these topdown approaches pose considerable challenges to usercentred participation. A discursive approach to analysis facilitated a nuanced consideration around the complexities of this setting, and we suggest that DP holds great opportunity in analysing applied HCI work, particularly when use cases indicate contradiction, problematic assumptions, or power dynamics.
"
Fast & Furious: Detecting Stress with a Car Steering Wheel,https://dl.acm.org/authorize?N657476,"
Stress affects the lives of millions of people every day. In-situ sensing could enable just-in-time stress management interventions. We present the first work to detect stress using the movements of a car's existing steering wheel. We extend prior work on PC peripherals and demonstrate that stress, expressed through muscle tension in the limbs, can be measured through the way we drive a car. We collected data in a driving simulator under controlled circumstances to vary the levels of induced stress, within subjects. We analyze angular displacement data to estimate coefficients related to muscle tension using an inverse filtering technique. We prove that the damped frequency of a mass spring damper model representing the arm is significantly higher during stress. Stress can be detected with only a few turns during driving. We validate these measures against a known stressor and calibrate our sensor against known stress measurements. In this paper, we have introduced a simple yet effective way to measure mental stress using only the steering wheel of an automobile. We have shown the efficacy of using a simple mass spring damper (MSD) model to detect the stress affecting the muscles of the arm. We calibrated our sensing algorithm against well known stress measurements such as self-reports, heart rate variability (RMSSD), and electrodermal activity (EDA). To validate our model, we have compared the damping frequency of the MSD system with well known math and music stressors. Using this model, we have found that it is possible to detect viable signals of stress with only a few turns, and that our sensor can capture longer term effects of stress expressed through muscle tension. This is the first work of this type, opening up new opportunities to use devices already embedded in a car as in-situ non-obtrusive stress sensors.

"
Norms Matter: Contrasting Social Support Around Behavior Change in Online Weight Loss Communities,https://dl.acm.org/authorize?N657477,"
Online health communities (OHCs) provide support across conditions; for weight loss, OHCs offer support to foster positive behavior change. However, weight loss behaviors can also be subverted on OHCs to promote disordered eating practices. Using comments as proxies for support, we use computational linguistic methods to juxtapose similarities and differences in two Reddit weight loss communities, r/proED and r/loseit. We employ language modeling and find that word use in both communities is largely similar. Then, by building a word embedding model, specifically a deep neural network on comment words, we contrast the context of word use and find differences that imply different behavior change goals in these OHCs. Finally, these content and context norms predict whether a comment comes from r/proED or r/loseit. We show that norms matter in understanding how different OHCs provision support to promote behavior change and discuss the implications for design and moderation of OHCs. In this paper, we proposed a computational approach to understand norms of social support around behavior change for two weight loss communities, r/loseit and r/proED. We analyzed the comments in the communities using language models, and found that the tokens they use were surprisingly similar. Then, we explored the context of use of these tokens in the comments of the two communities with word embedding models, and observed that the context of word use implied substantially different support practices. Finally, we developed and evaluated logistic regression classifiers to identity the community a comment comes from, thereby distinguishing between healthy and subversive support behaviors. Overall, we found that norms of support in these two communities facilitated healthy as well as subversive behavior change around weight loss. Our work suggests strategies and solutions with our methods and insights toward improving online health communities.
"
“A Stalker's Paradise”: How Intimate Partner Abusers Exploit Technology,https://dl.acm.org/authorize?N657478,"
This paper describes a qualitative study with 89 participants that details how abusers in intimate partner violence (IPV) contexts exploit technologies to intimidate, threaten, monitor, impersonate, harass, or otherwise harm their victims. We show that, at their core, many of the attacks in IPV contexts are technologically unsophisticated from the perspective of a security practitioner or researcher. For example, they are often carried out by a UI-bound adversary - an adversarial but authenticated user that interacts with a victim»s device or account via standard user interfaces - or by downloading and installing a ready-made application that enables spying on a victim. Nevertheless, we show how the sociotechnical and relational factors that characterize IPV make such attacks both extremely damaging to victims and challenging to counteract, in part because they undermine the predominant threat models under which systems have been designed. We discuss the nature of these new IPV threat models and outline opportunities for HCI research and design to mitigate these attacks. This paper discusses how intimate partner abusers exploit technologies to intimidate, threaten, monitor, impersonate, harass, or otherwise harm their victims. We show that many prevalent attacks in IPV may be easily carried out by average technology users because traditional threat models are often undermined by the IPV context. Our analysis suggests that one important threat model in this setting is characterized by UI-bound adversaries, authenticated but adversarial users that interact with a system via the regular UI. We provide constructive ideas for how to deal with UI-bound and other adversaries via IPV design reviews and better tools for detecting applications used to spy on victims. Taken together, our findings set the stage for future research and improvements to IPV safety.
"
